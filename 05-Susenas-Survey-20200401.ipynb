{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "DW Prospera - Susenas Survey 20200401",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ygautomo/02-Prospera-Datawarehouse/blob/master/05-Susenas-Survey-20200401.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZBBBH88sBv3",
        "colab_type": "text"
      },
      "source": [
        "# **01- Data Warehouse Prospera- National Social Economic Survey (Susenas)**\n",
        "## Data Warehouse Prospera Steps and Code\n",
        "### Status : Last Update 20200401"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9jcGOKSsBv6",
        "colab_type": "text"
      },
      "source": [
        "## **Python Environment Setup**\n",
        "We will be using a several different libraries throughout this steps. If you've successfully completed the [installation instructions](https://github.com/cs109/content/wiki/Installing-Python), all of the following statements should run."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tXrBSQzUDoQ",
        "colab_type": "text"
      },
      "source": [
        "### Mount Google Drive, Google Cloud Storage & AWS S3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3SrQRDtuTxz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "\n",
        "# drive.flush_and_unmount()\n",
        "drive.mount('/content/drive')\n",
        "# drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "!pwd\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhLW6Q5cEyu3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mount Google Cloud Storage\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "projectID = 'datawarehouse-001'\n",
        "bucketID = 'bucket-prospera-01'\n",
        "!gcloud config set project {projectID}\n",
        "!gsutil ls gs://{bucketID}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgwUyJ7W2ci5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mount Google Big Query\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Set BQ working directory 01\n",
        "dictBQDirectory = {\n",
        "  '05-susenas': 'datawarehouse-001:05_susenas', \n",
        "}\n",
        "\n",
        "projectID = 'datawarehouse-001'\n",
        "\n",
        "!gcloud config set project {projectID}\n",
        "!bq ls --project_id={projectID} {dictBQDirectory['05-susenas']}\n",
        "# !bq help"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyqoOQ03Nvgd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mount AWS S3\n",
        "\n",
        "# Add Credentials within, .boto files\n",
        "# [Credentials]\n",
        "# aws_access_key_id = AKIAJ23E6HNMRJM3UVPA\n",
        "# aws_secret_access_key = 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\n",
        "# [s 3]\n",
        "# use-sigv4=True\n",
        "# host=s3.us-east-2.amazonaws.com\n",
        "\n",
        "# !gsutil config\n",
        "# !gsutil version -l\n",
        "\n",
        "# update .boto files\n",
        "# !cd '/content/.config/legacy_credentials/data@prospera.or.id'\n",
        "# !ls\n",
        "# !pwd\n",
        "\n",
        "# !cat '/content/.config/legacy_credentials/data@prospera.or.id/.boto'\n",
        "# print ('\\n')\n",
        "\n",
        "# !gsutil cp '/content/drive/My Drive/Database/.boto' '/content/.config/legacy_credentials/data@prospera.or.id/.boto'\n",
        "\n",
        "bucketID = 'bucket-prospera-01'\n",
        "!gsutil ls s3://{bucketID}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYojFb3vUSbm",
        "colab_type": "text"
      },
      "source": [
        "### Setup Python Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmTtlwW1sBv8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Final Update 20200301\n",
        "# System-specific parameters and functions module provides the version number of the Python interpreter\n",
        "import sys\n",
        "print(\"Python version:        %6.6s(need at least 3.5.0)\" % sys.version)\n",
        "\n",
        "# IPython provides a rich architecture for interactive computing\n",
        "import IPython\n",
        "print(\"IPython version:      %6.6s (need at least 6.0.0)\" % IPython.__version__)     # (need at least 1.0.0)\n",
        "\n",
        "# Mathematical functions module provides access to the mathematical functions defined by the C standard\n",
        "# import math\n",
        "\n",
        "# Matplotlib is a Python 2D plotting library which produces publication quality figures in a variety of hardcopy formats \n",
        "# and interactive environments across platforms.\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "print(\"Mapltolib version:    %6.6s (need at least 3.0.0)\" % matplotlib.__version__)   # (need at least 1.2.1)\n",
        "\n",
        "# NumPy is the fundamental package for scientific computing with Python\n",
        "import numpy as np\n",
        "print(\"Numpy version:        %6.6s (need at least 1.15.0)\" % np.__version__)         # (need at least 1.7.1)\n",
        "\n",
        "# Pandas is a library providing high-performance, easy-to-use data structures and data analysis tools for Python\n",
        "import pandas as pd\n",
        "print(\"Pandas version:       %6.6s (need at least 0.20.0)\" % pd.__version__)         # (need at least 0.11.0)\n",
        "\n",
        "# Generate pseudo-random numbers. This module implements pseudo-random number generators for various distributions.\n",
        "# import random\n",
        "\n",
        "# Scikit-Learn a Machine Learning library in Python. Simple and efficient tools for data mining and data analysis\n",
        "import sklearn as sk\n",
        "print(\"Scikit-Learn version: %6.6s (need at least 0.15.0)\" % sk.__version__)         # (need at least 0.5.0)\n",
        "\n",
        "# Seaborn is a Python data visualization library based on matplotlib\n",
        "import seaborn as sns\n",
        "print(\"Seaborn version:      %6.6s (need at least 0.5.0)\" % sns.__version__)         # (need at least 0.5.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewifNF5csBwC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Customized python environment Setup\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# from matplotlib import pyplot as plt\n",
        "pd.set_option(\"display.precision\", 2)\n",
        "\n",
        "# Module for higher-order functions\n",
        "from functools import reduce\n",
        "\n",
        "# Unix style pathname pattern expansion\n",
        "import glob\n",
        "\n",
        "# Miscellaneous operating system interfaces\n",
        "import os\n",
        "\n",
        "# Pure python package for reading/writing dBase, FoxPro, and Visual FoxPro .dbf files (including memos)\n",
        "!pip install dbf\n",
        "import dbf\n",
        "\n",
        "# Convert DBF files to CSV, DataFrames, HDF5 tables, and SQL tables. Python3 compatible.\n",
        "!pip install simpledbf\n",
        "from simpledbf import Dbf5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBxS6bCAsBwF",
        "colab_type": "text"
      },
      "source": [
        "# **Machine Learning Pipeline:**\n",
        "![alt text](https://drive.google.com/uc?id=1zUK9aLiPk1zReXV19RMUQjqe3BrcvbyM)\n",
        "\n",
        "# **Step 01 - Project Goals & Problems**\n",
        "* Develop Datawarehouse for Prospera, which data is taken from Egnyte nad transform the data into Google BigQuery as Datawarehouse Platform.\n",
        "\n",
        "# **Step 02 - Data Retrieval**\n",
        "Data retrieval: This is mainly data collection, extraction, and acquisition from various data sources and data stores.\n",
        "\n",
        "Data retrieval process: \n",
        "1. Take raw data from Egnyte\n",
        "2. Standardize file name (linux file system)\n",
        "3. Convert into csv files\n",
        "4. Check and review data\n",
        "5. Convert data type if neccessary\n",
        "6. Merge data if necessary\n",
        "7. Create data description and save into json\n",
        "8. Put raw data into Google Cloud Storage\n",
        "9. Upload and transform the data into Google BigQuery\n",
        "\n",
        "Final Update 20200401"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPRDCgHSA7Pq",
        "colab_type": "text"
      },
      "source": [
        "## Step 0201 Set Working Directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-oCvtIW5L2A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set working directory 01\n",
        "dictDirectory = {\n",
        "  'susenas-2000': '/content/drive/My Drive/Database/susenas/susenas-2000',\n",
        "  'susenas-2000-data': '/content/drive/My Drive/Database/susenas/susenas-2000/data',\n",
        "  'susenas-2001': '/content/drive/My Drive/Database/susenas/susenas-2001',\n",
        "  'susenas-2001-data': '/content/drive/My Drive/Database/susenas/susenas-2001/data',\n",
        "  'susenas-2002': '/content/drive/My Drive/Database/susenas/susenas-2002',\n",
        "  'susenas-2002-data': '/content/drive/My Drive/Database/susenas/susenas-2002/data',\n",
        "  'susenas-2003': '/content/drive/My Drive/Database/susenas/susenas-2003',\n",
        "  'susenas-2003-data': '/content/drive/My Drive/Database/susenas/susenas-2003/data',\n",
        "  'susenas-2004': '/content/drive/My Drive/Database/susenas/susenas-2004',\n",
        "  'susenas-2004-data': '/content/drive/My Drive/Database/susenas/susenas-2004/data',\n",
        "  'susenas-2005': '/content/drive/My Drive/Database/susenas/susenas-2005',\n",
        "  'susenas-2005-data': '/content/drive/My Drive/Database/susenas/susenas-2005/data',\n",
        "  'susenas-2006': '/content/drive/My Drive/Database/susenas/susenas-2006',\n",
        "  'susenas-2006-data': '/content/drive/My Drive/Database/susenas/susenas-2006/data',\n",
        "  'susenas-2007': '/content/drive/My Drive/Database/susenas/susenas-2007',\n",
        "  'susenas-2007-data': '/content/drive/My Drive/Database/susenas/susenas-2007/data',\n",
        "  'susenas-2008': '/content/drive/My Drive/Database/susenas/susenas-2008',\n",
        "  'susenas-2008-data': '/content/drive/My Drive/Database/susenas/susenas-2008/data',\n",
        "  'susenas-2009': '/content/drive/My Drive/Database/susenas/susenas-2009',\n",
        "  'susenas-2009-data': '/content/drive/My Drive/Database/susenas/susenas-2009/data',\n",
        "  'susenas-2010': '/content/drive/My Drive/Database/susenas/susenas-2010',\n",
        "  'susenas-2010-data': '/content/drive/My Drive/Database/susenas/susenas-2010/data',\n",
        "  'susenas-2011': '/content/drive/My Drive/Database/susenas/susenas-2011',\n",
        "  'susenas-2011-data': '/content/drive/My Drive/Database/susenas/susenas-2011/data',\n",
        "  'susenas-2012': '/content/drive/My Drive/Database/susenas/susenas-2012',\n",
        "  'susenas-2012-data': '/content/drive/My Drive/Database/susenas/susenas-2012/data',\n",
        "  'susenas-2013': '/content/drive/My Drive/Database/susenas/susenas-2013',\n",
        "  'susenas-2013-data': '/content/drive/My Drive/Database/susenas/susenas-2013/data',\n",
        "  'susenas-2014': '/content/drive/My Drive/Database/susenas/susenas-2014',\n",
        "  'susenas-2014-data': '/content/drive/My Drive/Database/susenas/susenas-2014/data',\n",
        "  'susenas-2015': '/content/drive/My Drive/Database/susenas/susenas-2015',\n",
        "  'susenas-2015-data': '/content/drive/My Drive/Database/susenas/susenas-2015/data',\n",
        "  'susenas-2016': '/content/drive/My Drive/Database/susenas/susenas-2016',\n",
        "  'susenas-2016-data': '/content/drive/My Drive/Database/susenas/susenas-2016/data',\n",
        "  'susenas-2017': '/content/drive/My Drive/Database/susenas/susenas-2017',\n",
        "  'susenas-2017-data': '/content/drive/My Drive/Database/susenas/susenas-2017/data',\n",
        "  'susenas-2018': '/content/drive/My Drive/Database/susenas/susenas-2018',\n",
        "  'susenas-2018-data': '/content/drive/My Drive/Database/susenas/susenas-2018/data',\n",
        "  'susenas-2019': '/content/drive/My Drive/Database/susenas/susenas-2019',\n",
        "  'susenas-2019-data': '/content/drive/My Drive/Database/susenas/susenas-2019/data'\n",
        "}\n",
        "\n",
        "workingDirectory = dictDirectory['susenas-2001-data']\n",
        "print (workingDirectory)\n",
        "# cd '/content/drive/My Drive/Database/susenas/susenas-2000'\n",
        "os.chdir(workingDirectory)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gV6ZzZ_j4vAX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set working directory 02\n",
        "pwd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCAdTJi-57N4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set working directory 03\n",
        "ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xbxq6SOqa86u",
        "colab_type": "text"
      },
      "source": [
        "### List File within working Directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4v-wmgalSZC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "# Set working directory 01\n",
        "workingDirectory = dictDirectory['susenas-2002']\n",
        "os.chdir(workingDirectory)\n",
        "path = !pwd\n",
        "print (path)\n",
        "\n",
        "# List file on working directory 02\n",
        "# listFile = [f for f in glob.glob(\"*.csv\")]\n",
        "# listFile = [f for f in glob.glob(\"*.dta\")]\n",
        "listFile = [f for f in glob.glob(\"*.*\")]\n",
        "\n",
        "# Prints all files within directory 03\n",
        "loop = 0\n",
        "for e in listFile:\n",
        "  print (e)\n",
        "  loop += 1\n",
        "\n",
        "print (loop)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRkyKGMOSpz8",
        "colab_type": "text"
      },
      "source": [
        "### Google Cloud Storage Command"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qX_wXaNtEaJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set Google Cloud Storage working directory 01\n",
        "dictGCSDirectory = {\n",
        "  'susenas-2000': 'bucket-prospera-01/01-rawdata/01-bps/05-susenas/susenas-2000',\n",
        "  'susenas-2000-data': 'bucket-prospera-01/01-rawdata/01-bps/05-susenas/susenas-2000/data',\n",
        "  'susenas-2001': 'bucket-prospera-01/01-rawdata/01-bps/05-susenas/susenas-2001',\n",
        "  'susenas-2001-data': 'bucket-prospera-01/01-rawdata/01-bps/05-susenas/susenas-2001/data',\n",
        "  'susenas-2002': 'bucket-prospera-01/01-rawdata/01-bps/05-susenas/susenas-2002',\n",
        "  'susenas-2002-data': 'bucket-prospera-01/01-rawdata/01-bps/05-susenas/susenas-2002/data',\n",
        "  'susenas-2003': 'bucket-prospera-01/01-rawdata/01-bps/05-susenas/susenas-2003',\n",
        "  'susenas-2003-data': 'bucket-prospera-01/01-rawdata/01-bps/05-susenas/susenas-2003/data',\n",
        "  'susenas-2004': 'bucket-prospera-01/01-rawdata/01-bps/05-susenas/susenas-2004',\n",
        "  'susenas-2004-data': 'bucket-prospera-01/01-rawdata/01-bps/05-susenas/susenas-2004/data',\n",
        "  'susenas-2005': 'bucket-prospera-01/01-rawdata/01-bps/05-susenas/susenas-2005',\n",
        "  'susenas-2005-data': 'bucket-prospera-01/01-rawdata/01-bps/05-susenas/susenas-2005/data',\n",
        "  'susenas-2006': 'bucket-prospera-01/01-rawdata/01-bps/05-susenas/susenas-2006',\n",
        "  'susenas-2006-data': 'bucket-prospera-01/01-rawdata/01-bps/05-susenas/susenas-2006/data',\n",
        "  'susenas-2007': 'bucket-prospera-01/01-rawdata/01-bps/05-susenas/susenas-2007',\n",
        "  'susenas-2007-data': 'bucket-prospera-01/01-rawdata/01-bps/05-susenas/susenas-2007/data',\n",
        "  'susenas-2008': 'bucket-prospera-01/01-rawdata/01-bps/05-susenas/susenas-2008',\n",
        "  'susenas-2008-data': 'bucket-prospera-01/01-rawdata/01-bps/05-susenas/susenas-2008/data',\n",
        "  'susenas-2009': 'bucket-prospera-01/01-rawdata/01-bps/05-susenas/susenas-2009',\n",
        "  'susenas-2009-data': 'bucket-prospera-01/01-rawdata/01-bps/05-susenas/susenas-2009/data',\n",
        "  'susenas-2010': 'bucket-prospera-01/01-rawdata/01-bps/05-susenas/susenas-2010',\n",
        "  'susenas-2010-data': 'bucket-prospera-01/01-rawdata/01-bps/05-susenas/susenas-2010/data',\n",
        "  'susenas-2011': 'bucket-prospera-01/01-rawdata/01-bps/05-susenas/susenas-2011',\n",
        "  'susenas-2011-data': 'bucket-prospera-01/01-rawdata/01-bps/05-susenas/susenas-2011/data',\n",
        "  'susenas-2012': 'bucket-prospera-01/01-rawdata/01-bps/05-susenas/susenas-2012',\n",
        "  'susenas-2012-data': 'bucket-prospera-01/01-rawdata/01-bps/05-susenas/susenas-2012/data',\n",
        "  'susenas-2013': 'bucket-prospera-01/01-rawdata/01-bps/05-susenas/susenas-2013',\n",
        "  'susenas-2013-data': 'bucket-prospera-01/01-rawdata/01-bps/05-susenas/susenas-2013/data',\n",
        "  'susenas-2014': 'bucket-prospera-01/01-rawdata/01-bps/05-susenas/susenas-2014',\n",
        "  'susenas-2014-data': 'bucket-prospera-01/01-rawdata/01-bps/05-susenas/susenas-2014/data',\n",
        "  'susenas-2015': 'bucket-prospera-01/01-rawdata/01-bps/05-susenas/susenas-2015',\n",
        "  'susenas-2015-data': 'bucket-prospera-01/01-rawdata/01-bps/05-susenas/susenas-2015/data',\n",
        "  'susenas-2016': 'bucket-prospera-01/01-rawdata/01-bps/05-susenas/susenas-2016',\n",
        "  'susenas-2016-data': 'bucket-prospera-01/01-rawdata/01-bps/05-susenas/susenas-2016/data',\n",
        "  'susenas-2017': 'bucket-prospera-01/01-rawdata/01-bps/05-susenas/susenas-2017',\n",
        "  'susenas-2017-data': 'bucket-prospera-01/01-rawdata/01-bps/05-susenas/susenas-2017/data',\n",
        "  'susenas-2018': 'bucket-prospera-01/01-rawdata/01-bps/05-susenas/susenas-2018',\n",
        "  'susenas-2018-data': 'bucket-prospera-01/01-rawdata/01-bps/05-susenas/susenas-2018/data',\n",
        "  'susenas-2019': 'bucket-prospera-01/01-rawdata/01-bps/05-susenas/susenas-2019',\n",
        "  'susenas-2019-data': 'bucket-prospera-01/01-rawdata/01-bps/05-susenas/susenas-2019/data'\n",
        "}\n",
        "\n",
        "# List file on working directory on Google Cloud Storage\n",
        "bucketName = 'bucket-prospera-01'\n",
        "bucketDirectory = dictGCSDirectory['susenas-2000']\n",
        "\n",
        "# !gcloud config set project {projectID}\n",
        "# !gsutil ls gs://{bucketName}/01-rawdata/01-bps/04-sensus-ekonomi/se-2016/se-2016-umb-nonkeuangan\n",
        "# !gsutil ls gs://{bucketDirectory}\n",
        "# !gsutil ls\n",
        "\n",
        "\n",
        "# List file on working directory on AWS S3\n",
        "bucketName = 'bucket-prospera-01'\n",
        "bucketDirectory = dictGCSDirectory['susenas-2000']\n",
        "# !gsutil ls s3://{bucketDirectory}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4M7PlLhCJC5L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copy data from Google Drive into Google Cloud Storage\n",
        "bucketName = 'bucket-prospera-01'\n",
        "workingDirectory = dictDirectory['susenas-2000']\n",
        "bucketDirectory = dictGCSDirectory['susenas-2000']\n",
        "\n",
        "# Delete data using gsutil cp command\n",
        "# !gsutil rm gs://{bucketName}/01-rawdata/01-bps/04-sensus-ekonomi/se-2016/se-2016-umb-jk/**\n",
        "\n",
        "# Copy data using gsutil cp command\n",
        "# !gsutil -m cp -r /content/drive/My\\ Drive/Database/se-2016-umb-keuangan/data/* gs://{bucketName}/01-rawdata/01-bps/04-sensus-ekonomi/se-2016/se-2016-umb-jk/data\n",
        "\n",
        "# !gsutil cp /content/drive/My\\ Drive/Database/se-2016-listing/data/se2016-listing-merge.csv gs://{bucketName}/01-rawdata/01-bps/04-sensus-ekonomi/se-2016/se-2016-listing/data\n",
        "# !gsutil cp /content/drive/My\\ Drive/Database/se-2016-listing/data/se2016-listing-33-convert.csv gs://{bucketName}/01-rawdata/01-bps/04-sensus-ekonomi/se-2016/se-2016-listing/data\n",
        "# !gsutil -m cp -r /content/drive/My\\ Drive/Data/* gs://bucket-prospera-01/01-rawdata/01-bps/04-sensus-ekonomi/se-2016/se-2016-listing/data\n",
        "\n",
        "# Copy data using gsutil rsync command exclude directories\n",
        "# !gsutil rsync -d /content/drive/My\\ Drive/Database/se-2016-umb-nonkeuangan/ gs://{bucketName}/01-rawdata/01-bps/04-sensus-ekonomi/se-2016/se-2016-umb-nonkeuangan/\n",
        "# !gsutil rsync -d /content/drive/My\\ Drive/Database/se-2016-umb-produksi/ gs://{bucketDirectory}\n",
        "\n",
        "# Copy data using gsutil rsync command include directories\n",
        "# !gsutil rsync -r /content/drive/My\\ Drive/Database/se-2016-listing/ gs://{bucketName}/01-rawdata/01-bps/04-sensus-ekonomi/se-2016/se-2016-listing/\n",
        "!gsutil rsync -d -r /content/drive/My\\ Drive/Database/susenas/susenas-2000/ gs://{bucketDirectory}\n",
        "\n",
        "\n",
        "# Copy data using gsutil rsync command into AWS S3\n",
        "# gsutil rsync -d -r gs://my-gs-bucket s3://my-s3-bucket"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAFdTPM7nXzG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copy data from Google Cloud Storage into AWS S3\n",
        "bucketName = 'bucket-prospera-01'\n",
        "bucketDirectory = dictGCSDirectory['se-2016-direktori']\n",
        "\n",
        "# !gsutil ls s3://{bucketDirectory}\n",
        "\n",
        "# Delete data using gsutil cp command\n",
        "# !gsutil rm gs://{bucketName}/01-rawdata/01-bps/04-sensus-ekonomi/se-2016/se-2016-umb-jk/**\n",
        "\n",
        "# Copy data using gsutil cp command\n",
        "# !gsutil -m cp -r /content/drive/My\\ Drive/Database/se-2016-umb-keuangan/data/* gs://{bucketName}/01-rawdata/01-bps/04-sensus-ekonomi/se-2016/se-2016-umb-jk/data\n",
        "\n",
        "# !gsutil cp /content/drive/My\\ Drive/Database/se-2016-listing/data/se2016-listing-merge.csv gs://{bucketName}/01-rawdata/01-bps/04-sensus-ekonomi/se-2016/se-2016-listing/data\n",
        "# !gsutil cp /content/drive/My\\ Drive/Database/se-2016-listing/data/se2016-listing-33-convert.csv gs://{bucketName}/01-rawdata/01-bps/04-sensus-ekonomi/se-2016/se-2016-listing/data\n",
        "# !gsutil -m cp -r /content/drive/My\\ Drive/Data/* gs://bucket-prospera-01/01-rawdata/01-bps/04-sensus-ekonomi/se-2016/se-2016-listing/data\n",
        "\n",
        "# Copy data using gsutil rsync command exclude directories\n",
        "# !gsutil rsync -d /content/drive/My\\ Drive/Database/se-2016-umb-nonkeuangan/ gs://{bucketName}/01-rawdata/01-bps/04-sensus-ekonomi/se-2016/se-2016-umb-nonkeuangan/\n",
        "# !gsutil rsync -d /content/drive/My\\ Drive/Database/se-2016-umb-produksi/ gs://{bucketDirectory}\n",
        "\n",
        "# Copy data using gsutil rsync command include directories\n",
        "!gsutil rsync -d -r gs://{bucketDirectory} s3://{bucketDirectory}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2MrpDWrCXeN",
        "colab_type": "text"
      },
      "source": [
        "### Google Big Query Command"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sbtq21f9cg8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set working directory on Google Big Query 01\n",
        "projectId = 'datawarehouse-001'\n",
        "directoryBQ = ['datawarehouse-001:04_sensus_ekonomi', 'datawarehouse-001:04_sensus_ekonomi']\n",
        "\n",
        "!gcloud config set project {projectID}\n",
        "# List file on Google Big Query working directory 02\n",
        "# !bq show datawarehouse-001:04_sensus_ekonomi.se_2016_listing_merge\n",
        "!bq ls  {directoryBQ[0]}\n",
        "#  !bq ls --max_results=1000 {directoryBQ[0]}\n",
        "# !bq rm --help"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DJ701ULOU8m",
        "colab_type": "text"
      },
      "source": [
        "Big Query Delete Table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QywdOf7L7QJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "# Big Query delete table se2016-listing\n",
        "listBQFile = [\n",
        "  'se_2016_listing_11', 'se_2016_listing_12', 'se_2016_listing_13', 'se_2016_listing_14', 'se_2016_listing_15', \n",
        "  'se_2016_listing_16', 'se_2016_listing_17', 'se_2016_listing_18', 'se_2016_listing_19', 'se_2016_listing_21', \n",
        "  'se_2016_listing_31', 'se_2016_listing_32', 'se_2016_listing_33', 'se_2016_listing_34', 'se_2016_listing_35', \n",
        "  'se_2016_listing_36', 'se_2016_listing_51', 'se_2016_listing_52', 'se_2016_listing_53', 'se_2016_listing_61', \n",
        "  'se_2016_listing_62', 'se_2016_listing_63', 'se_2016_listing_64', 'se_2016_listing_65', 'se_2016_listing_71', \n",
        "  'se_2016_listing_72', 'se_2016_listing_73', 'se_2016_listing_74', 'se_2016_listing_75', 'se_2016_listing_76', \n",
        "  'se_2016_listing_81', 'se_2016_listing_82', 'se_2016_listing_91', 'se_2016_listing_94', 'se_2016_listing_merge' \n",
        "]\n",
        "\n",
        "# Big Query delete table se2016-direktori\n",
        "listBQFile = [\n",
        "]\n",
        "\n",
        "# Big Query delete table se2016-umk\n",
        "listBQFile = [\n",
        "]\n",
        "\n",
        "# Big Query delete table se2016-umb-jk\n",
        "listBQFile = [\n",
        "]\n",
        "\n",
        "# Big Query delete table se2016-umb-jnk\n",
        "listBQFile = [\n",
        "]\n",
        "\n",
        "# Big Query delete table se2016-umb-sp\n",
        "listBQFile = [\n",
        "]\n",
        "\n",
        "# Set working directory on Google Big Query 01\n",
        "projectId = 'datawarehouse-001'\n",
        "directoryBQ = ['datawarehouse-001:05_susenas']\n",
        "\n",
        "# List file on Google Big Query working directory 02\n",
        "# !bq ls --max_results=1000 {directoryBQ[0]}\n",
        "\n",
        "# !bq rm --help\n",
        "\n",
        "loop = 0\n",
        "for e in listBQFile:\n",
        "  bqFileName = directoryBQ[0] + \".\" + e\n",
        "  !bq rm -f -t {bqFileName}\n",
        "  # print (\"delete\", e)\n",
        "  print (\"delete\", bqFileName)\n",
        "  loop += 1\n",
        "\n",
        "print (loop)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4bk_H_tPCaN",
        "colab_type": "text"
      },
      "source": [
        "Big Query Create Table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T__9kwZ-5c7Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Upload file on working directory to Google Big Query\n",
        "\n",
        "# Set Working Diretory 01\n",
        "workingDirectory = dictDirectory['se-2016-direktori']\n",
        "\n",
        "!bq load \\\n",
        "    --source_format=CSV \\\n",
        "    --skip_leading_rows=1 \\\n",
        "    datawarehouse-001:04_sensus_ekonomi.se_2016_umb_jk_02_21 \\\n",
        "    gs://bucket-prospera-01/01-rawdata/01-bps/04-sensus-ekonomi/se-2016/se-2016-umb-jk/data/data-01/se2016-umb-jk-01-21.csv \\\n",
        "    ./se2016-umb-jka-layout.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8yDeMxHPN_D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "# Big Query create table susenas\n",
        "listBQFile = [\n",
        "  'susenas00_ki', 'susenas00-ki.csv', 'susenas00-ki-layout-prospera.json', 'susenas00_kr', 'susenas00-kr.csv', 'susenas00-kr-layout-prospera.json', 'susenas00_kna', 'susenas00-kna.csv', 'susenas00-kna-layout-prospera.json'\n",
        "]\n",
        "\n",
        "# Set working directory 01\n",
        "workingDirectory = dictDirectory['susenas-2000']\n",
        "os.chdir(workingDirectory)\n",
        "path = !pwd\n",
        "\n",
        "pathData = 'gs://bucket-prospera-01/01-rawdata/01-bps/05-susenas/susenas-2000/data/'\n",
        "fileLayout = ''\n",
        "\n",
        "loop = 0\n",
        "for i in range (0, len(listBQFile), 3):\n",
        "  pathSource = pathData + listBQFile[i+1]\n",
        "  fileLayout = listBQFile[i+2]\n",
        "  pathDestination = 'datawarehouse-001:05_susenas.' + listBQFile[i]\n",
        "\n",
        "  # Upload file on working directory to Google Big Query\n",
        "  !bq load \\\n",
        "    --source_format=CSV \\\n",
        "    --skip_leading_rows=1 \\\n",
        "    --replace=True \\\n",
        "    {pathDestination} \\\n",
        "    {pathSource} \\\n",
        "    ./{fileLayout}\n",
        "\n",
        "  print (pathDestination, pathSource, fileLayout)\n",
        "  loop += 1\n",
        "\n",
        "print (loop)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCsx8TVEB4Ln",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "# Big Query create table\n",
        "listBQFile = [\n",
        "  'se2016_umb_jk_01_11', 'se2016-umb-jk-01-11.csv', \n",
        "  'se2016_umb_jk_01_12', 'se2016-umb-jk-01-12.csv', \n",
        "  'se2016_umb_jk_01_13', 'se2016-umb-jk-01-13.csv', \n",
        "  'se2016_umb_jk_01_14', 'se2016-umb-jk-01-14.csv', \n",
        "  'se2016_umb_jk_01_15', 'se2016-umb-jk-01-15.csv', \n",
        "  'se2016_umb_jk_01_16', 'se2016-umb-jk-01-16.csv', \n",
        "  'se2016_umb_jk_01_17', 'se2016-umb-jk-01-17.csv', \n",
        "  'se2016_umb_jk_01_18', 'se2016-umb-jk-01-18.csv', \n",
        "  'se2016_umb_jk_01_19', 'se2016-umb-jk-01-19.csv', \n",
        "  'se2016_umb_jk_01_21', 'se2016-umb-jk-01-21.csv', \n",
        "  'se2016_umb_jk_01_31', 'se2016-umb-jk-01-31.csv', \n",
        "  'se2016_umb_jk_01_32', 'se2016-umb-jk-01-32.csv', \n",
        "  'se2016_umb_jk_01_33', 'se2016-umb-jk-01-33.csv', \n",
        "  'se2016_umb_jk_01_34', 'se2016-umb-jk-01-34.csv', \n",
        "  'se2016_umb_jk_01_35', 'se2016-umb-jk-01-35.csv', \n",
        "  'se2016_umb_jk_01_36', 'se2016-umb-jk-01-36.csv', \n",
        "  'se2016_umb_jk_01_51', 'se2016-umb-jk-01-51.csv', \n",
        "  'se2016_umb_jk_01_52', 'se2016-umb-jk-01-52.csv', \n",
        "  'se2016_umb_jk_01_53', 'se2016-umb-jk-01-53.csv', \n",
        "  'se2016_umb_jk_01_61', 'se2016-umb-jk-01-61.csv', \n",
        "  'se2016_umb_jk_01_62', 'se2016-umb-jk-01-62.csv', \n",
        "  'se2016_umb_jk_01_63', 'se2016-umb-jk-01-63.csv', \n",
        "  'se2016_umb_jk_01_64', 'se2016-umb-jk-01-64.csv', \n",
        "  'se2016_umb_jk_01_65', 'se2016-umb-jk-01-65.csv', \n",
        "  'se2016_umb_jk_01_71', 'se2016-umb-jk-01-71.csv', \n",
        "  'se2016_umb_jk_01_72', 'se2016-umb-jk-01-72.csv', \n",
        "  'se2016_umb_jk_01_73', 'se2016-umb-jk-01-73.csv', \n",
        "  'se2016_umb_jk_01_74', 'se2016-umb-jk-01-74.csv', \n",
        "  'se2016_umb_jk_01_75', 'se2016-umb-jk-01-75.csv', \n",
        "  'se2016_umb_jk_01_76', 'se2016-umb-jk-01-76.csv', \n",
        "  'se2016_umb_jk_01_81', 'se2016-umb-jk-01-81.csv', \n",
        "  'se2016_umb_jk_01_82', 'se2016-umb-jk-01-82.csv', \n",
        "  'se2016_umb_jk_01_91', 'se2016-umb-jk-01-91.csv', \n",
        "  'se2016_umb_jk_01_94', 'se2016-umb-jk-01-94.csv',\n",
        "  'se2016_umb_jk_02_11', 'se2016-umb-jk-02-11.csv', \n",
        "  'se2016_umb_jk_02_12', 'se2016-umb-jk-02-12.csv', \n",
        "  'se2016_umb_jk_02_13', 'se2016-umb-jk-02-13.csv', \n",
        "  'se2016_umb_jk_02_14', 'se2016-umb-jk-02-14.csv', \n",
        "  'se2016_umb_jk_02_15', 'se2016-umb-jk-02-15.csv', \n",
        "  'se2016_umb_jk_02_16', 'se2016-umb-jk-02-16.csv', \n",
        "  'se2016_umb_jk_02_17', 'se2016-umb-jk-02-17.csv', \n",
        "  'se2016_umb_jk_02_18', 'se2016-umb-jk-02-18.csv', \n",
        "  'se2016_umb_jk_02_19', 'se2016-umb-jk-02-19.csv', \n",
        "  'se2016_umb_jk_02_21', 'se2016-umb-jk-02-21.csv', \n",
        "  'se2016_umb_jk_02_31', 'se2016-umb-jk-02-31.csv', \n",
        "  'se2016_umb_jk_02_32', 'se2016-umb-jk-02-32.csv', \n",
        "  'se2016_umb_jk_02_33', 'se2016-umb-jk-02-33.csv', \n",
        "  'se2016_umb_jk_02_34', 'se2016-umb-jk-02-34.csv', \n",
        "  'se2016_umb_jk_02_35', 'se2016-umb-jk-02-35.csv', \n",
        "  'se2016_umb_jk_02_36', 'se2016-umb-jk-02-36.csv', \n",
        "  'se2016_umb_jk_02_51', 'se2016-umb-jk-02-51.csv', \n",
        "  'se2016_umb_jk_02_52', 'se2016-umb-jk-02-52.csv', \n",
        "  'se2016_umb_jk_02_53', 'se2016-umb-jk-02-53.csv', \n",
        "  'se2016_umb_jk_02_61', 'se2016-umb-jk-02-61.csv', \n",
        "  'se2016_umb_jk_02_62', 'se2016-umb-jk-02-62.csv', \n",
        "  'se2016_umb_jk_02_63', 'se2016-umb-jk-02-63.csv', \n",
        "  'se2016_umb_jk_02_64', 'se2016-umb-jk-02-64.csv', \n",
        "  'se2016_umb_jk_02_65', 'se2016-umb-jk-02-65.csv', \n",
        "  'se2016_umb_jk_02_71', 'se2016-umb-jk-02-71.csv', \n",
        "  'se2016_umb_jk_02_72', 'se2016-umb-jk-02-72.csv', \n",
        "  'se2016_umb_jk_02_73', 'se2016-umb-jk-02-73.csv', \n",
        "  'se2016_umb_jk_02_74', 'se2016-umb-jk-02-74.csv', \n",
        "  'se2016_umb_jk_02_75', 'se2016-umb-jk-02-75.csv', \n",
        "  'se2016_umb_jk_02_76', 'se2016-umb-jk-02-76.csv', \n",
        "  'se2016_umb_jk_02_81', 'se2016-umb-jk-02-81.csv', \n",
        "  'se2016_umb_jk_02_82', 'se2016-umb-jk-02-82.csv', \n",
        "  'se2016_umb_jk_02_91', 'se2016-umb-jk-02-91.csv', \n",
        "  'se2016_umb_jk_02_94', 'se2016-umb-jk-02-94.csv',\n",
        "  'se2016_umb_jk_03_11', 'se2016-umb-jk-03-11.csv', \n",
        "  'se2016_umb_jk_03_12', 'se2016-umb-jk-03-12.csv', \n",
        "  'se2016_umb_jk_03_13', 'se2016-umb-jk-03-13.csv', \n",
        "  'se2016_umb_jk_03_14', 'se2016-umb-jk-03-14.csv', \n",
        "  'se2016_umb_jk_03_15', 'se2016-umb-jk-03-15.csv', \n",
        "  'se2016_umb_jk_03_16', 'se2016-umb-jk-03-16.csv', \n",
        "  'se2016_umb_jk_03_17', 'se2016-umb-jk-03-17.csv', \n",
        "  'se2016_umb_jk_03_18', 'se2016-umb-jk-03-18.csv', \n",
        "  'se2016_umb_jk_03_19', 'se2016-umb-jk-03-19.csv', \n",
        "  'se2016_umb_jk_03_21', 'se2016-umb-jk-03-21.csv', \n",
        "  'se2016_umb_jk_03_31', 'se2016-umb-jk-03-31.csv', \n",
        "  'se2016_umb_jk_03_32', 'se2016-umb-jk-03-32.csv', \n",
        "  'se2016_umb_jk_03_33', 'se2016-umb-jk-03-33.csv', \n",
        "  'se2016_umb_jk_03_34', 'se2016-umb-jk-03-34.csv', \n",
        "  'se2016_umb_jk_03_35', 'se2016-umb-jk-03-35.csv', \n",
        "  'se2016_umb_jk_03_36', 'se2016-umb-jk-03-36.csv', \n",
        "  'se2016_umb_jk_03_51', 'se2016-umb-jk-03-51.csv', \n",
        "  'se2016_umb_jk_03_52', 'se2016-umb-jk-03-52.csv', \n",
        "  'se2016_umb_jk_03_53', 'se2016-umb-jk-03-53.csv', \n",
        "  'se2016_umb_jk_03_61', 'se2016-umb-jk-03-61.csv', \n",
        "  'se2016_umb_jk_03_62', 'se2016-umb-jk-03-62.csv', \n",
        "  'se2016_umb_jk_03_63', 'se2016-umb-jk-03-63.csv', \n",
        "  'se2016_umb_jk_03_64', 'se2016-umb-jk-03-64.csv', \n",
        "  'se2016_umb_jk_03_65', 'se2016-umb-jk-03-65.csv', \n",
        "  'se2016_umb_jk_03_71', 'se2016-umb-jk-03-71.csv', \n",
        "  'se2016_umb_jk_03_72', 'se2016-umb-jk-03-72.csv', \n",
        "  'se2016_umb_jk_03_73', 'se2016-umb-jk-03-73.csv', \n",
        "  'se2016_umb_jk_03_74', 'se2016-umb-jk-03-74.csv', \n",
        "  'se2016_umb_jk_03_75', 'se2016-umb-jk-03-75.csv', \n",
        "  'se2016_umb_jk_03_76', 'se2016-umb-jk-03-76.csv', \n",
        "  'se2016_umb_jk_03_81', 'se2016-umb-jk-03-81.csv', \n",
        "  'se2016_umb_jk_03_82', 'se2016-umb-jk-03-82.csv', \n",
        "  'se2016_umb_jk_03_91', 'se2016-umb-jk-03-91.csv', \n",
        "  'se2016_umb_jk_03_94', 'se2016-umb-jk-03-94.csv',\n",
        "  'se2016_umb_jk_11', 'se2016-umb-jk-11.csv', \n",
        "  'se2016_umb_jk_12', 'se2016-umb-jk-12.csv', \n",
        "  'se2016_umb_jk_13', 'se2016-umb-jk-13.csv', \n",
        "  'se2016_umb_jk_14', 'se2016-umb-jk-14.csv', \n",
        "  'se2016_umb_jk_15', 'se2016-umb-jk-15.csv', \n",
        "  'se2016_umb_jk_16', 'se2016-umb-jk-16.csv', \n",
        "  'se2016_umb_jk_17', 'se2016-umb-jk-17.csv', \n",
        "  'se2016_umb_jk_18', 'se2016-umb-jk-18.csv', \n",
        "  'se2016_umb_jk_19', 'se2016-umb-jk-19.csv', \n",
        "  'se2016_umb_jk_21', 'se2016-umb-jk-21.csv', \n",
        "  'se2016_umb_jk_31', 'se2016-umb-jk-31.csv', \n",
        "  'se2016_umb_jk_32', 'se2016-umb-jk-32.csv', \n",
        "  'se2016_umb_jk_33', 'se2016-umb-jk-33.csv', \n",
        "  'se2016_umb_jk_34', 'se2016-umb-jk-34.csv', \n",
        "  'se2016_umb_jk_35', 'se2016-umb-jk-35.csv', \n",
        "  'se2016_umb_jk_36', 'se2016-umb-jk-36.csv', \n",
        "  'se2016_umb_jk_51', 'se2016-umb-jk-51.csv', \n",
        "  'se2016_umb_jk_52', 'se2016-umb-jk-52.csv', \n",
        "  'se2016_umb_jk_53', 'se2016-umb-jk-53.csv', \n",
        "  'se2016_umb_jk_61', 'se2016-umb-jk-61.csv', \n",
        "  'se2016_umb_jk_62', 'se2016-umb-jk-62.csv', \n",
        "  'se2016_umb_jk_63', 'se2016-umb-jk-63.csv', \n",
        "  'se2016_umb_jk_64', 'se2016-umb-jk-64.csv', \n",
        "  'se2016_umb_jk_65', 'se2016-umb-jk-65.csv', \n",
        "  'se2016_umb_jk_71', 'se2016-umb-jk-71.csv', \n",
        "  'se2016_umb_jk_72', 'se2016-umb-jk-72.csv', \n",
        "  'se2016_umb_jk_73', 'se2016-umb-jk-73.csv', \n",
        "  'se2016_umb_jk_74', 'se2016-umb-jk-74.csv', \n",
        "  'se2016_umb_jk_75', 'se2016-umb-jk-75.csv', \n",
        "  'se2016_umb_jk_76', 'se2016-umb-jk-76.csv', \n",
        "  'se2016_umb_jk_81', 'se2016-umb-jk-81.csv', \n",
        "  'se2016_umb_jk_82', 'se2016-umb-jk-82.csv', \n",
        "  'se2016_umb_jk_91', 'se2016-umb-jk-91.csv', \n",
        "  'se2016_umb_jk_94', 'se2016-umb-jk-94.csv',   \n",
        "  'se2016_umb_jk_merge', 'se2016-umb-jk-merge.csv',   \n",
        "]\n",
        "\n",
        "# Set working directory 01\n",
        "os.chdir(workingDirectory)\n",
        "path = !pwd\n",
        "\n",
        "loop = 0\n",
        "for i in range (0, 274, 2):\n",
        "\n",
        "  if listBQFile[i] == 'se2016_umb_jk_01_11':\n",
        "    pathData = 'gs://bucket-prospera-01/01-rawdata/01-bps/04-sensus-ekonomi/se-2016/se-2016-umb-jk/data/data-01/'\n",
        "    fileLayout = 'se2016-umb-jka-layout.json'\n",
        "  elif listBQFile[i] == 'se2016_umb_jk_02_11':\n",
        "    pathData = 'gs://bucket-prospera-01/01-rawdata/01-bps/04-sensus-ekonomi/se-2016/se-2016-umb-jk/data/data-02/'\n",
        "    fileLayout = 'se2016-umb-jkb-layout.json'\n",
        "  elif listBQFile[i] == 'se2016_umb_jk_03_11':\n",
        "    pathData = 'gs://bucket-prospera-01/01-rawdata/01-bps/04-sensus-ekonomi/se-2016/se-2016-umb-jk/data/data-03/'\n",
        "    fileLayout = 'se2016-umb-jkc-layout.json'\n",
        "  elif listBQFile[i] == 'se2016_umb_jk_11':\n",
        "    pathData = 'gs://bucket-prospera-01/01-rawdata/01-bps/04-sensus-ekonomi/se-2016/se-2016-umb-jk/data/'\n",
        "    fileLayout = 'se2016-umb-jk-layout.json'\n",
        "\n",
        "  pathSource = pathData + listBQFile[i+1]\n",
        "  pathDestination = 'datawarehouse-001:04_sensus_ekonomi.' + listBQFile[i]\n",
        "\n",
        "  # Upload file on working directory to Google Big Query\n",
        "  !bq load \\\n",
        "      --source_format=CSV \\\n",
        "      --skip_leading_rows=1 \\\n",
        "      {pathDestination} \\\n",
        "      {pathSource} \\\n",
        "      ./{fileLayout}\n",
        "\n",
        "  print (listBQFile[i])\n",
        "  loop += 1\n",
        "\n",
        "print (loop)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1py-B7WEAnm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ha6JKL57cCs8",
        "colab_type": "text"
      },
      "source": [
        "## Step 0202 Standardize File Names"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuNnCdMDADXc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "dictFile = {\n",
        "  'susenas-2000': {'source': 'susenas-2000-', 'dest': 'susenas00-'},\n",
        "  'susenas-2001': {'source': 'susenas-2001-', 'dest': 'susenas01-'},\n",
        "  'susenas-2002': {'source': 'susenas-2002-', 'dest': 'susenas02-'},\n",
        "  'susenas-2003': {'source': 'se-2016-umk-', 'dest': 'susenas-umk-'},\n",
        "  'susenas-2004': {'source': '_data1_umk_v1', 'dest': 'susenas-umk-01-'},\n",
        "  'susenas-2005': {'source': '_data2_umk_v1', 'dest': 'susenas-umk-02-'},\n",
        "  'susenas-2006': {'source': 'se-2016-umb-jk', 'dest': 'susenas-umb-jk'},\n",
        "  'susenas-2007': {'source': '_data1_umb-jk_v1', 'dest': 'susenas-umb-jk-01-'},\n",
        "  'susenas-2008': {'source': '_data2_umb-jk_v1', 'dest': 'susenas-umb-jk-02-'},\n",
        "  'susenas-2009': {'source': '_data3_umb-jk_v1', 'dest': 'susenas-umb-jk-03-'},\n",
        "  'susenas-2010': {'source': 'se-2016-umb-jnk', 'dest': 'susenas-umb-jnk'},\n",
        "  'susenas-2011': {'source': '_data1_umb-jnk_v1', 'dest': 'susenas-umb-jnk-01-'},\n",
        "  'susenas-2012': {'source': '_data2_umb-jnk_v1', 'dest': 'susenas-umb-jnk-02-'},\n",
        "  'susenas-2013': {'source': '_data3_umb-jnk_v1', 'dest': 'susenas-umb-jnk-03-'},\n",
        "  'susenas-2014': {'source': 'se-2016-umb-sp', 'dest': 'susenas-umb-sp'},\n",
        "  'susenas-2015': {'source': '_data1_umb-sp_v1', 'dest': 'susenas-umb-sp-01-'},\n",
        "  'susenas-2016': {'source': '_data2_umb-sp_v1', 'dest': 'susenas-umb-sp-02-'},\n",
        "  'susenas-2017': {'source': '_data3_umb-sp_v1', 'dest': 'susenas-umb-sp-03-'},\n",
        "  'susenas-2018': {'source': '_data3_umb-sp_v1', 'dest': 'susenas-umb-sp-03-'}\n",
        "}\n",
        "\n",
        "# Set working directory 01\n",
        "workingDirectory = dictDirectory['susenas-2002']\n",
        "os.chdir(workingDirectory)\n",
        "path = !pwd\n",
        "\n",
        "# List dbf file within directory 02\n",
        "listFile = [f for f in glob.glob(\"*.*\")]\n",
        "fileSource = dictFile['susenas-2002']['source']\n",
        "fileDestination = dictFile['susenas-2002']['dest']\n",
        "\n",
        "# Rename dbf file within directory 03\n",
        "loop = 0\n",
        "for e in listFile:\n",
        "  pathSource = path[0] + '/' + e\n",
        "  pathDestination = path[0] + '/' + e\n",
        "  # pathDestination = path[0] + '/' + fileDestination + e\n",
        "  # print ('renaming ' + pathSource)\n",
        "  fname,ext = os.path.splitext(pathDestination)\n",
        "  fname = fname.replace(fileSource,fileDestination)\n",
        "\n",
        "  os.rename(pathSource, fname + ext)\n",
        "  print (e, fname)\n",
        "  loop += 1\n",
        "\n",
        "print (loop)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_cUm_tdcRdb",
        "colab_type": "text"
      },
      "source": [
        "## Step 0203A Convert File from stata into csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRd5IauKsBwG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "# listFile = [f for f in glob.glob(\"*.dta\")]\n",
        "listFile = ['ind96a.dta', 'ind96b.dta']\n",
        "\n",
        "path = !pwd\n",
        "loop = 0\n",
        "for e in listFile:\n",
        "  pathSource = path[0] + '/' + e\n",
        "  print ('converting ' + pathSource)\n",
        "  fname,ext = os.path.splitext(pathSource)\n",
        "  pathDestination = fname + '.csv'\n",
        "\n",
        "  # Convert file from stata into csv\n",
        "  dfStata = pd.io.stata.read_stata(pathSource)\n",
        "  dfStata.to_csv(pathDestination, encoding='utf-8', index=False)\n",
        "\n",
        "  # Read and import csv file dataset into pandas data frame, change paths if needed\n",
        "  dfBPSData = pd.read_csv(pathDestination)\n",
        "\n",
        "  print(\"dfBPSData.shape :\", dfBPSData.shape)\n",
        "  print(\"type(dfBPSData) :\", type(dfBPSData))\n",
        "\n",
        "print (loop)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnxjYhKyPR88",
        "colab_type": "text"
      },
      "source": [
        "## Step 0203B Convert File from dbf into csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boCXFJ8aPLbE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "# Set working directory 01\n",
        "os.chdir(dictDirectory['susenas-2001-data'])\n",
        "path = !pwd\n",
        "\n",
        "# List dbf file within directory 02\n",
        "# listFile = [f for f in glob.glob(\"*.dbf\")]\n",
        "# listFile = ['ssn01ki2.dbf', 'ssn01ki_bintang2.dbf','ssn01kr.dbf]\n",
        "listFile = ['ssn01kr.dbf']\n",
        "\n",
        "# Convert dbf file into csv 03\n",
        "loop = 0\n",
        "for e in listFile:\n",
        "  pathSource = path[0] + '/' + e\n",
        "  fname,ext = os.path.splitext(pathSource)\n",
        "  pathDestination = fname + '.csv'\n",
        "\n",
        "  # Convert file from dbf into csv (using Dbf5)\n",
        "  dbfFile = Dbf5(pathSource)\n",
        "  dfDbf = dbfFile.to_dataframe()\n",
        "  dfDbf.to_csv(pathDestination, encoding='utf-8', index=False)\n",
        "\n",
        "  # Convert file from dbf into csv (using dbf)\n",
        "  # with dbf.Table(pathSource) as table:\n",
        "  #  dbf.export(table, pathDestination)\n",
        "\n",
        "  # Read and import csv file dataset into pandas data frame, change paths if needed\n",
        "  # dfBPSData = pd.read_csv(pathDestination)\n",
        "\n",
        "  # print(\"dfBPSData.shape :\", dfBPSData.shape)\n",
        "  # print(\"type(dfBPSData) :\", type(dfBPSData))\n",
        "  print (\"converting\", e, \"to\", pathDestination)\n",
        "  loop += 1\n",
        "\n",
        "print (loop)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYWlW7Y0cdvZ",
        "colab_type": "text"
      },
      "source": [
        "## Step 0204 Check & Review Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJIWFRwY4Po9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read and import csv file dataset into pandas data frame, change paths if needed\n",
        "dictFileReview = {\n",
        "  'susenas00-ki': 'susenas00-ki.csv', 'susenas00-kr': 'susenas00-kr.csv', 'susenas00-kna': 'susenas00-kna.csv', 'susenas00-mod-ki': 'susenas00-mod-ki.csv', 'susenas00-mod-kr': 'susenas00-mod-kr.csv',\n",
        "  'susenas01-ki': 'susenas01-ki.csv', 'susenas01-kr': 'susenas01-kr.csv', 'susenas01-ind-km': 'susenas01-ind-km.csv', 'susenas01-rt-km': 'susenas01-rt-km.csv',\n",
        "  'susenas01-ssn01ki2': 'ssn01ki2.csv', 'susenas01-ssn01ki-bintang': 'ssn01ki_bintang2.csv', 'susenas01-ssn01kr': 'ssn01kr.csv'\n",
        "}\n",
        "\n",
        "# Set working directory 01\n",
        "os.chdir(dictDirectory['susenas-2001-data'])\n",
        "path = !pwd\n",
        "\n",
        "# Review dataset 02\n",
        "fname = dictFileReview['susenas01-ki']\n",
        "# fname = dictFileReview['susenas01-ssn01ki2']\n",
        "# fname = dictFileReview['susenas01-ssn01ki-bintang']\n",
        "# fname = dictFileReview['susenas01-ssn01kr']\n",
        "\n",
        "# fname = 'susenas00-ki.csv'\n",
        "\n",
        "pathSource = path[0] + '/' + fname\n",
        "# print (pathSource)\n",
        "dfBPSData = pd.read_csv(pathSource)\n",
        "\n",
        "print (fname)\n",
        "print (\"dfBPSData.shape  :\", dfBPSData.shape)\n",
        "# print (\"type(dfBPSData)  :\", type(dfBPSData))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJULAHsasrAb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "# Set working directory 01\n",
        "workingDirectory = dictDirectory['se-2016-listing-data']\n",
        "os.chdir(workingDirectory)\n",
        "path = !pwd\n",
        "\n",
        "# List csv file within directory 02\n",
        "listFile = [f for f in glob.glob(\"*.csv\")]\n",
        "\n",
        "# Prints all files within directory 03\n",
        "loop = 0\n",
        "for fname in listFile:\n",
        "  pathSource = path[0] + '/' + fname\n",
        "  # pathDestination = pathSource\n",
        "  dfBPSData = pd.read_csv(pathSource)\n",
        "\n",
        "  print (fname, \".shape  :\", dfBPSData.shape)\n",
        "  loop += 1\n",
        "\n",
        "print (loop)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqeuGrJj-RFA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Examine dataset, see data type\n",
        "print (pathSource)\n",
        "dfBPSData.info(verbose=True, null_counts=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUYzQXRCqpVf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Examine dataset, see data type\n",
        "print(dfBPSData.describe(percentiles=[], include='all').transpose().to_string())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4HNyNlp5783",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Examine dataset, see data values\n",
        "dfBPSData.head()\n",
        "dfBPSData.tail()\n",
        "# dfBPSData.sort_values(by=['psid'], ascending=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGloACERum6K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Examine dataset, see data values\n",
        "dfBPSData.head()\n",
        "# dfBPSData.tail()\n",
        "# dfBPSData.sort_values(by=['psid'], ascending=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nyd6i3s6Ykb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# basic info about columns in each dataset\n",
        "for name, df in dfs.items():\n",
        "    print(\"df: %s\\n\" %name)\n",
        "    print(\"df:\", name, \"type:\", type(df), \"\\n\")\n",
        "    print(\"shape: %d rows, %d cols\\n\" %df.shape)\n",
        "    \n",
        "    print(\"column info:\")\n",
        "    for col in df.columns:\n",
        "        print(\"* %s: %d nulls, %d nans, %d unique vals, most common: %s\" % (\n",
        "            col, \n",
        "            df[col].isnull().sum(),\n",
        "            df[col].isna().sum(),\n",
        "            df[col].nunique(),\n",
        "            df[col].value_counts().head(2).to_dict()\n",
        "        ))\n",
        "    print(\"\\n------\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q362ArQ_HXIn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Examine dataset\n",
        "# print(dfBPSData.describe(percentiles=[], include='all').transpose().to_string())\n",
        "print(dfBPSData.count().transpose().to_string())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pneHPWaGBjsR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.reset_option('display.show_dimensions')\n",
        "pd.set_option('display.show_dimensions', False)\n",
        "print (pd.options.display.max_rows, pd.options.display.show_dimensions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v78u6YBW86_c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Examine dataset, first 5 rows\n",
        "# dfBPSData['DDESA94'].isna().any()\n",
        "# dfBPSData.sort_values(by='psid')\n",
        "# dfBPSData.tail(10)\n",
        "dfBPSData.isna()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjXJiFPxgu-Y",
        "colab_type": "text"
      },
      "source": [
        "## Step 0205 Convert Data Type\n",
        "Convert data type float into integer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eH1j5M24iv4F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "# Read and import csv file dataset into pandas data frame, change paths if needed\n",
        "fname = 'se2016-listing-11.csv'\n",
        "\n",
        "path = !pwd\n",
        "# print (path)\n",
        "\n",
        "pathSource = path[0] + '/' + fname\n",
        "pathDestination = pathSource\n",
        "# print (pathSource)\n",
        "dfBPSData = pd.read_csv(pathSource)\n",
        "\n",
        "print (fname)\n",
        "print (\"dfBPSData.shape  :\", dfBPSData.shape)\n",
        "print (\"type(dfBPSData)  :\", type(dfBPSData))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DusZEXqGjAY7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Examine dataset, found isna & maximum values\n",
        "# dfBPSData.columns.isna().any()\n",
        "# dfBPSData['psid'].max()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48fPO0PYgsPF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Examine dataset, create data type dictionary\n",
        "dfBPSTypeSeries  = dict(dfBPSData.dtypes)\n",
        "print (dfBPSTypeSeries)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7Rk2oELhMXJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert data type float into integer\n",
        "for (key, values) in dfBPSTypeSeries.items():\n",
        "  if values=='float64':\n",
        "    print (key, values)\n",
        "    dfBPSData[key] = dfBPSData[key].astype('Int64')\n",
        "\n",
        "    # Special case on certain field\n",
        "    # if key!='D94_VNOB':\n",
        "      # dfBPSData[key] = dfBPSData[key].astype('Int64')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eFX_WNOyRRs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save data from convert data type operation\n",
        "print (pathDestination)\n",
        "dfBPSData.to_csv(pathDestination, encoding='utf-8', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hfng_tp1uJhE",
        "colab_type": "text"
      },
      "source": [
        "Convert data type float into integer (Loop)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aY5RjFlpuHJW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "# list csv file within directory\n",
        "listFile = [f for f in glob.glob(\"*.csv\")]\n",
        "\n",
        "# data type to convert\n",
        "dDataType = {\n",
        "  'provinsi':'object',\n",
        "  'nama_prov':'object',\n",
        "  'kabupaten':'object',\n",
        "  'nama_kab':'object',\n",
        "  'idperkab':'Int64',\n",
        "  'b1r11d':'object',\n",
        "  'b1r13':'object',\n",
        "  'b1r14a':'object',\n",
        "  'b1r14b':'object',\n",
        "  'kategori':'object',\n",
        "  'b1r15c':'object',\n",
        "  'b1r15d':'object',\n",
        "  'b1r16':'object',\n",
        "  'b1r19a':'Int64',\n",
        "  'b1r21':'object',\n",
        "  'b1r22a':'object',\n",
        "  'b1r22b':'object',\n",
        "  'kat_omset':'Int64',\n",
        "  'skalausaha':'object',\n",
        "  'penimbang':'object',\n",
        "  'renum':'Int64'\n",
        "    }\n",
        "\n",
        "path = !pwd\n",
        "print (path)\n",
        "loop = 0\n",
        "\n",
        "for e in listFile:\n",
        "  pathSource = path[0] + '/' + e\n",
        "  fname,ext = os.path.splitext(pathSource)\n",
        "  pathDestination = fname + \"-convert\" + ext\n",
        "  dfBPSData = pd.read_csv(pathSource)\n",
        "\n",
        "  # print (fname, \".shape  :\", dfBPSData.shape)\n",
        "  # print (pathSource)\n",
        "  print (pathDestination)\n",
        "  \n",
        "  # Examine dataset, create data type dictionary\n",
        "  dfBPSTypeSeries  = dict(dfBPSData.dtypes)\n",
        "\n",
        "  # Convert data type float into integer\n",
        "  for (key, values) in dfBPSTypeSeries.items():\n",
        "    dfBPSData[key] = dfBPSData[key].astype(dDataType[key.lower()])\n",
        "    # print (key, values, \"convert to\", dDataType[key.lower()])\n",
        "\n",
        "  # Save data from convert data type operation\n",
        "  # print (pathDestination)\n",
        "  dfBPSData.to_csv(pathDestination, encoding='utf-8', index=False)\n",
        "  loop += 1\n",
        "\n",
        "print (loop)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uX3tNt4zFM4O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save data into google cloud storage\n",
        "bucket_name = 'bucket-prospera-01'\n",
        "!gsutil cp /content/drive/My\\ Drive/Database/se-2016-listing/data/se2016-listing-33.csv gs://{bucket_name}/01-rawdata/01-bps/04-sensus-ekonomi/se-2016/se-2016-listing/data\n",
        "!gsutil cp /content/drive/My\\ Drive/Database/se-2016-listing/data/se-2016-listing-33-convert.csv gs://{bucket_name}/01-rawdata/01-bps/04-sensus-ekonomi/se-2016/se-2016-listing/data\n",
        "# !gsutil -m cp -r /content/drive/My\\ Drive/Data/* gs://bucket-prospera-01/01-rawdata/01-bps/04-sensus-ekonomi/se-2016/se-2016-listing/data\n",
        "/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAoEpo1PjNO4",
        "colab_type": "text"
      },
      "source": [
        "## Step 0206 Merge Dataset\n",
        "Merge Dataset if required"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ndyaSVwzo7o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "# Read and import csv file dataset into pandas data frame, change paths if needed\n",
        "fname  = 'ind95.csv' # Merge data files\n",
        "fnameA = 'ind95a.csv'\n",
        "fnameB = 'ind95b.csv'\n",
        "\n",
        "path = !pwd\n",
        "pathSourceA = path[0] + '/' + fnameA\n",
        "pathSourceB = path[0] + '/' + fnameB\n",
        "pathDestination = path[0] + '/' + fname\n",
        "print (pathSourceA)\n",
        "print (pathSourceB)\n",
        "dfBPSDataA = pd.read_csv(pathSourceA)\n",
        "dfBPSDataB = pd.read_csv(pathSourceB)\n",
        "\n",
        "print(\"dfBPSDataA.shape  :\", dfBPSDataA.shape)\n",
        "print(\"type(dfBPSDataA)  :\", type(dfBPSDataA))\n",
        "print(\"dfBPSDataB.shape  :\", dfBPSDataB.shape)\n",
        "print(\"type(dfBPSDataB)  :\", type(dfBPSDataB))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YAl2E_m2RjP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Rename joining keys\n",
        "dfBPSDataA.rename({'nomor': 'NOMOR_A'}, axis='columns', inplace=True)\n",
        "dfBPSDataB.rename({'nomor': 'NOMOR_B'}, axis='columns', inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksYR16Zzj7g_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Merge data files\n",
        "dfBPSData = dfBPSDataA.merge(dfBPSDataB, left_on='NOMOR_A', right_on='NOMOR_B')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhHx-SgAkXmB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dfBPSData[['NOMOR_A','NOMOR_B']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_NEl1xr_3mu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save data from convert data type operation\n",
        "print (pathDestination)\n",
        "dfBPSData.to_csv(pathDestination, encoding='utf-8', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bz91TM1WofAg",
        "colab_type": "text"
      },
      "source": [
        "### List Files within Working Directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhsjEz6jwMVW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "# Set working directory 01\n",
        "workingDirectory = dictDirectory['se-2016-direktori-data']\n",
        "os.chdir(workingDirectory)\n",
        "path = !pwd\n",
        "print (path)\n",
        "\n",
        "# List file on working directory 02\n",
        "listFile = [f for f in glob.glob(\"*.csv\")]\n",
        "# listFile = [f for f in glob.glob(\"*.dbf\")]\n",
        "# listFile = [f for f in glob.glob(\"*.*\")]\n",
        "\n",
        "# Prints all files within directory 03\n",
        "loop = 0\n",
        "for e in listFile:\n",
        "  print (e)\n",
        "  loop += 1\n",
        "\n",
        "print (loop)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iov9X9Hx9Skg",
        "colab_type": "text"
      },
      "source": [
        "### Merge File tableA + tableB -> tableMerge"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_gQnmQAodv0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "# Merge table se2016-umk\n",
        "listFile = [\n",
        "  'se2016-umk-11.csv', 'se2016-umk-01-11.csv', 'se2016-umk-02-11.csv', 'se2016-umk-12.csv', 'se2016-umk-01-12.csv', 'se2016-umk-02-12.csv', 'se2016-umk-13.csv', 'se2016-umk-01-13.csv', 'se2016-umk-02-13.csv', 'se2016-umk-14.csv', 'se2016-umk-01-14.csv', 'se2016-umk-02-14.csv', 'se2016-umk-15.csv', 'se2016-umk-01-15.csv', 'se2016-umk-02-15.csv', \n",
        "  'se2016-umk-16.csv', 'se2016-umk-01-16.csv', 'se2016-umk-02-16.csv', 'se2016-umk-17.csv', 'se2016-umk-01-17.csv', 'se2016-umk-02-17.csv', 'se2016-umk-18.csv', 'se2016-umk-01-18.csv', 'se2016-umk-02-18.csv', 'se2016-umk-19.csv', 'se2016-umk-01-19.csv', 'se2016-umk-02-19.csv', 'se2016-umk-21.csv', 'se2016-umk-01-21.csv', 'se2016-umk-02-21.csv', \n",
        "  'se2016-umk-31.csv', 'se2016-umk-01-31.csv', 'se2016-umk-02-31.csv', 'se2016-umk-32.csv', 'se2016-umk-01-32.csv', 'se2016-umk-02-32.csv', 'se2016-umk-33.csv', 'se2016-umk-01-33.csv', 'se2016-umk-02-33.csv', 'se2016-umk-34.csv', 'se2016-umk-01-34.csv', 'se2016-umk-02-34.csv', 'se2016-umk-35.csv', 'se2016-umk-01-35.csv', 'se2016-umk-02-35.csv', \n",
        "  'se2016-umk-36.csv', 'se2016-umk-01-36.csv', 'se2016-umk-02-36.csv', 'se2016-umk-51.csv', 'se2016-umk-01-51.csv', 'se2016-umk-02-51.csv', 'se2016-umk-52.csv', 'se2016-umk-01-52.csv', 'se2016-umk-02-52.csv', 'se2016-umk-53.csv', 'se2016-umk-01-53.csv', 'se2016-umk-02-53.csv', 'se2016-umk-61.csv', 'se2016-umk-01-61.csv', 'se2016-umk-02-61.csv', \n",
        "  'se2016-umk-62.csv', 'se2016-umk-01-62.csv', 'se2016-umk-02-62.csv', 'se2016-umk-63.csv', 'se2016-umk-01-63.csv', 'se2016-umk-02-63.csv', 'se2016-umk-64.csv', 'se2016-umk-01-64.csv', 'se2016-umk-02-64.csv', 'se2016-umk-65.csv', 'se2016-umk-01-65.csv', 'se2016-umk-02-65.csv', 'se2016-umk-71.csv', 'se2016-umk-01-71.csv', 'se2016-umk-02-71.csv', \n",
        "  'se2016-umk-72.csv', 'se2016-umk-01-72.csv', 'se2016-umk-02-72.csv', 'se2016-umk-73.csv', 'se2016-umk-01-73.csv', 'se2016-umk-02-73.csv', 'se2016-umk-74.csv', 'se2016-umk-01-74.csv', 'se2016-umk-02-74.csv', 'se2016-umk-75.csv', 'se2016-umk-01-75.csv', 'se2016-umk-02-75.csv', 'se2016-umk-76.csv', 'se2016-umk-01-76.csv', 'se2016-umk-02-76.csv', \n",
        "  'se2016-umk-81.csv', 'se2016-umk-01-81.csv', 'se2016-umk-02-81.csv', 'se2016-umk-82.csv', 'se2016-umk-01-82.csv', 'se2016-umk-02-82.csv', 'se2016-umk-91.csv', 'se2016-umk-01-91.csv', 'se2016-umk-02-91.csv', 'se2016-umk-94.csv', 'se2016-umk-01-94.csv', 'se2016-umk-02-94.csv'\n",
        "]\n",
        "\n",
        "# Set working directory 01\n",
        "workingDirectory = dictDirectory['se-2016-umk-data']\n",
        "os.chdir(workingDirectory)\n",
        "path = !pwd\n",
        "# print (path)\n",
        "\n",
        "loop = 0\n",
        "for i in range (0, len(listFile), 3):\n",
        "  fname = listFile[i]     # Merge data files\n",
        "  fnameA = listFile[i+1]\n",
        "  fnameB  = listFile[i+2]\n",
        "  pathSourceA = path[0] + '/data-01/' + fnameA\n",
        "  pathSourceB = path[0] + '/data-02/' + fnameB\n",
        "  pathDestination = path[0] + '/' + fname\n",
        "\n",
        "  # print (pathSourceA, pathSourceB, pathDestination)\n",
        "  dfBPSDataA = pd.read_csv(pathSourceA)\n",
        "  dfBPSDataB = pd.read_csv(pathSourceB)\n",
        "\n",
        "  # Rename joining keys\n",
        "  dfBPSDataA.rename({'IDPERUSAHA': 'PERUSAHAAN_ID', 'JENISKUESI': 'JENISKUESIONER_A', 'PROV': 'PROVINSI_IDA', 'SKALAUSAHA': 'SKALAUSAHA_A', 'WEIGHT': 'WEIGHT_A'}, axis='columns', inplace=True)\n",
        "  dfBPSDataB.rename({'IDPERUSAHA': 'PERUSAHAAN_ID', 'JENISKUESI': 'JENISKUESIONER_B', 'PROV': 'PROVINSI_IDB', 'SKALAUSAHA': 'SKALAUSAHA_B', 'WEIGHT': 'WEIGHT_B'}, axis='columns', inplace=True)\n",
        "\n",
        "  dfBPSData = [dfBPSDataA, dfBPSDataB]\n",
        "\n",
        "  # Merge data files\n",
        "  # dfBPSData = dfBPSDataA.merge(dfBPSDataB, left_on='IDPERUSAHAAN_A', right_on='IDPERUSAHAAN_B')\n",
        "  dfBPSDataMerge = reduce(lambda left,right: pd.merge(left,right,on='PERUSAHAAN_ID'), dfBPSData)\n",
        "  \n",
        "  # print (pathDestination, \"dfBPSDataA.shape:\", dfBPSDataA.shape, \"dfBPSDataB.shape:\", dfBPSDataB.shape)\n",
        "  # print (pathDestination, \"dfBPSDataA.shape:\", dfBPSDataA.shape, \"dfBPSDataB.shape:\", dfBPSDataB.shape, \"dfBPSDataMerge.shape:\", dfBPSData.shape)\n",
        "  # print (\"dfBPSDataA.shape:\", dfBPSDataA.shape[0], \"dfBPSDataB.shape:\", dfBPSDataB.shape[0], \"dfBPSDataMerge.shape:\", dfBPSData.shape[0])\n",
        "\n",
        "  # Save data from merge data type operation\n",
        "  print (pathDestination, dfBPSDataMerge.shape)\n",
        "  # dfBPSDataMerge.to_csv(pathDestination, encoding='utf-8', index=False)\n",
        "\n",
        "  loop += 1\n",
        "\n",
        "print (loop)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3Pezw3vtu9-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "# Merge all table\n",
        "# Set working directory 01\n",
        "workingDirectory = dictDirectory['se-2016-direktori-data']\n",
        "os.chdir(workingDirectory)\n",
        "path = !pwd\n",
        "# print (path)\n",
        "\n",
        "# List file on working directory 02\n",
        "listFile = [f for f in glob.glob(\"*.csv\")]\n",
        "\n",
        "pathDestination = path[0] + '/' + 'se2016-direktori-merge.csv'\n",
        "dfMerges = []\n",
        "totalRows = 0\n",
        "loop = 0\n",
        "for e in listFile:\n",
        "  pathSource = path[0] + '/' + e\n",
        "  # print ('merge ' + pathSource)\n",
        "\n",
        "  # Read and import csv file dataset into pandas data frame, change paths if needed\n",
        "  dfBPSData = pd.read_csv(pathSource)\n",
        "\n",
        "  dfMerges.append(dfBPSData)\n",
        "  print(\"dfBPSData.shape :\", e, dfBPSData.shape)\n",
        "  totalRows += dfBPSData.shape[0]\n",
        "  loop += 1\n",
        "\n",
        "print (loop)\n",
        "\n",
        "\n",
        "dfBPSDataMerge = pd.concat(dfMerges)\n",
        "print(\"dfBPSDataMerge.shape :\", dfBPSDataMerge.shape, totalRows)\n",
        "\n",
        "print (pathDestination)\n",
        "dfBPSDataMerge.to_csv(pathDestination, encoding='utf-8', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqhUYaU_9smX",
        "colab_type": "text"
      },
      "source": [
        "### Merge File tableA + tableB + tableC -> tableMerge"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lfsv8YVMZyPf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "# Merge table se2016-umb-jk\n",
        "listFile = [\n",
        "  'se2016-umb-jk-11.csv', 'se2016-umb-jk-01-11.csv', 'se2016-umb-jk-02-11.csv', 'se2016-umb-jk-03-11.csv', 'se2016-umb-jk-12.csv', 'se2016-umb-jk-01-12.csv', 'se2016-umb-jk-02-12.csv', 'se2016-umb-jk-03-12.csv', 'se2016-umb-jk-13.csv', 'se2016-umb-jk-01-13.csv', 'se2016-umb-jk-02-13.csv', 'se2016-umb-jk-03-13.csv', 'se2016-umb-jk-14.csv', 'se2016-umb-jk-01-14.csv', 'se2016-umb-jk-02-14.csv', 'se2016-umb-jk-03-14.csv', 'se2016-umb-jk-15.csv', 'se2016-umb-jk-01-15.csv', 'se2016-umb-jk-02-15.csv', 'se2016-umb-jk-03-15.csv', \n",
        "  'se2016-umb-jk-16.csv', 'se2016-umb-jk-01-16.csv', 'se2016-umb-jk-02-16.csv', 'se2016-umb-jk-03-16.csv', 'se2016-umb-jk-17.csv', 'se2016-umb-jk-01-17.csv', 'se2016-umb-jk-02-17.csv', 'se2016-umb-jk-03-17.csv', 'se2016-umb-jk-18.csv', 'se2016-umb-jk-01-18.csv', 'se2016-umb-jk-02-18.csv', 'se2016-umb-jk-03-18.csv', 'se2016-umb-jk-19.csv', 'se2016-umb-jk-01-19.csv', 'se2016-umb-jk-02-19.csv', 'se2016-umb-jk-03-19.csv', 'se2016-umb-jk-21.csv', 'se2016-umb-jk-01-21.csv', 'se2016-umb-jk-02-21.csv', 'se2016-umb-jk-03-21.csv', \n",
        "  'se2016-umb-jk-31.csv', 'se2016-umb-jk-01-31.csv', 'se2016-umb-jk-02-31.csv', 'se2016-umb-jk-03-31.csv', 'se2016-umb-jk-32.csv', 'se2016-umb-jk-01-32.csv', 'se2016-umb-jk-02-32.csv', 'se2016-umb-jk-03-32.csv', 'se2016-umb-jk-33.csv', 'se2016-umb-jk-01-33.csv', 'se2016-umb-jk-02-33.csv', 'se2016-umb-jk-03-33.csv', 'se2016-umb-jk-34.csv', 'se2016-umb-jk-01-34.csv', 'se2016-umb-jk-02-34.csv', 'se2016-umb-jk-03-34.csv', 'se2016-umb-jk-35.csv', 'se2016-umb-jk-01-35.csv', 'se2016-umb-jk-02-35.csv', 'se2016-umb-jk-03-35.csv', \n",
        "  'se2016-umb-jk-36.csv', 'se2016-umb-jk-01-36.csv', 'se2016-umb-jk-02-36.csv', 'se2016-umb-jk-03-36.csv', 'se2016-umb-jk-51.csv', 'se2016-umb-jk-01-51.csv', 'se2016-umb-jk-02-51.csv', 'se2016-umb-jk-03-51.csv', 'se2016-umb-jk-52.csv', 'se2016-umb-jk-01-52.csv', 'se2016-umb-jk-02-52.csv', 'se2016-umb-jk-03-52.csv', 'se2016-umb-jk-53.csv', 'se2016-umb-jk-01-53.csv', 'se2016-umb-jk-02-53.csv', 'se2016-umb-jk-03-53.csv', 'se2016-umb-jk-61.csv', 'se2016-umb-jk-01-61.csv', 'se2016-umb-jk-02-61.csv', 'se2016-umb-jk-03-61.csv', \n",
        "  'se2016-umb-jk-62.csv', 'se2016-umb-jk-01-62.csv', 'se2016-umb-jk-02-62.csv', 'se2016-umb-jk-03-62.csv', 'se2016-umb-jk-63.csv', 'se2016-umb-jk-01-63.csv', 'se2016-umb-jk-02-63.csv', 'se2016-umb-jk-03-63.csv', 'se2016-umb-jk-64.csv', 'se2016-umb-jk-01-64.csv', 'se2016-umb-jk-02-64.csv', 'se2016-umb-jk-03-64.csv', 'se2016-umb-jk-65.csv', 'se2016-umb-jk-01-65.csv', 'se2016-umb-jk-02-65.csv', 'se2016-umb-jk-03-65.csv', 'se2016-umb-jk-71.csv', 'se2016-umb-jk-01-71.csv', 'se2016-umb-jk-02-71.csv', 'se2016-umb-jk-03-71.csv', \n",
        "  'se2016-umb-jk-72.csv', 'se2016-umb-jk-01-72.csv', 'se2016-umb-jk-02-72.csv', 'se2016-umb-jk-03-72.csv', 'se2016-umb-jk-73.csv', 'se2016-umb-jk-01-73.csv', 'se2016-umb-jk-02-73.csv', 'se2016-umb-jk-03-73.csv', 'se2016-umb-jk-74.csv', 'se2016-umb-jk-01-74.csv', 'se2016-umb-jk-02-74.csv', 'se2016-umb-jk-03-74.csv', 'se2016-umb-jk-75.csv', 'se2016-umb-jk-01-75.csv', 'se2016-umb-jk-02-75.csv', 'se2016-umb-jk-03-75.csv', 'se2016-umb-jk-76.csv', 'se2016-umb-jk-01-76.csv', 'se2016-umb-jk-02-76.csv', 'se2016-umb-jk-03-76.csv', \n",
        "  'se2016-umb-jk-81.csv', 'se2016-umb-jk-01-81.csv', 'se2016-umb-jk-02-81.csv', 'se2016-umb-jk-03-81.csv', 'se2016-umb-jk-82.csv', 'se2016-umb-jk-01-82.csv', 'se2016-umb-jk-02-82.csv', 'se2016-umb-jk-03-82.csv', 'se2016-umb-jk-91.csv', 'se2016-umb-jk-01-91.csv', 'se2016-umb-jk-02-91.csv', 'se2016-umb-jk-03-91.csv', 'se2016-umb-jk-94.csv', 'se2016-umb-jk-01-94.csv', 'se2016-umb-jk-02-94.csv', 'se2016-umb-jk-03-94.csv'\n",
        "]\n",
        "\n",
        "# Merge table se2016-umb-jnk\n",
        "listFile = [\n",
        "  'se2016-umb-jnk-11.csv', 'se2016-umb-jnk-01-11.csv', 'se2016-umb-jnk-02-11.csv', 'se2016-umb-jnk-03-11.csv', 'se2016-umb-jnk-12.csv', 'se2016-umb-jnk-01-12.csv', 'se2016-umb-jnk-02-12.csv', 'se2016-umb-jnk-03-12.csv', 'se2016-umb-jnk-13.csv', 'se2016-umb-jnk-01-13.csv', 'se2016-umb-jnk-02-13.csv', 'se2016-umb-jnk-03-13.csv', 'se2016-umb-jnk-14.csv', 'se2016-umb-jnk-01-14.csv', 'se2016-umb-jnk-02-14.csv', 'se2016-umb-jnk-03-14.csv', 'se2016-umb-jnk-15.csv', 'se2016-umb-jnk-01-15.csv', 'se2016-umb-jnk-02-15.csv', 'se2016-umb-jnk-03-15.csv', \n",
        "  'se2016-umb-jnk-16.csv', 'se2016-umb-jnk-01-16.csv', 'se2016-umb-jnk-02-16.csv', 'se2016-umb-jnk-03-16.csv', 'se2016-umb-jnk-17.csv', 'se2016-umb-jnk-01-17.csv', 'se2016-umb-jnk-02-17.csv', 'se2016-umb-jnk-03-17.csv', 'se2016-umb-jnk-18.csv', 'se2016-umb-jnk-01-18.csv', 'se2016-umb-jnk-02-18.csv', 'se2016-umb-jnk-03-18.csv', 'se2016-umb-jnk-19.csv', 'se2016-umb-jnk-01-19.csv', 'se2016-umb-jnk-02-19.csv', 'se2016-umb-jnk-03-19.csv', 'se2016-umb-jnk-21.csv', 'se2016-umb-jnk-01-21.csv', 'se2016-umb-jnk-02-21.csv', 'se2016-umb-jnk-03-21.csv', \n",
        "  'se2016-umb-jnk-31.csv', 'se2016-umb-jnk-01-31.csv', 'se2016-umb-jnk-02-31.csv', 'se2016-umb-jnk-03-31.csv', 'se2016-umb-jnk-32.csv', 'se2016-umb-jnk-01-32.csv', 'se2016-umb-jnk-02-32.csv', 'se2016-umb-jnk-03-32.csv', 'se2016-umb-jnk-33.csv', 'se2016-umb-jnk-01-33.csv', 'se2016-umb-jnk-02-33.csv', 'se2016-umb-jnk-03-33.csv', 'se2016-umb-jnk-34.csv', 'se2016-umb-jnk-01-34.csv', 'se2016-umb-jnk-02-34.csv', 'se2016-umb-jnk-03-34.csv', 'se2016-umb-jnk-35.csv', 'se2016-umb-jnk-01-35.csv', 'se2016-umb-jnk-02-35.csv', 'se2016-umb-jnk-03-35.csv', \n",
        "  'se2016-umb-jnk-36.csv', 'se2016-umb-jnk-01-36.csv', 'se2016-umb-jnk-02-36.csv', 'se2016-umb-jnk-03-36.csv', 'se2016-umb-jnk-51.csv', 'se2016-umb-jnk-01-51.csv', 'se2016-umb-jnk-02-51.csv', 'se2016-umb-jnk-03-51.csv', 'se2016-umb-jnk-52.csv', 'se2016-umb-jnk-01-52.csv', 'se2016-umb-jnk-02-52.csv', 'se2016-umb-jnk-03-52.csv', 'se2016-umb-jnk-53.csv', 'se2016-umb-jnk-01-53.csv', 'se2016-umb-jnk-02-53.csv', 'se2016-umb-jnk-03-53.csv', 'se2016-umb-jnk-61.csv', 'se2016-umb-jnk-01-61.csv', 'se2016-umb-jnk-02-61.csv', 'se2016-umb-jnk-03-61.csv', \n",
        "  'se2016-umb-jnk-62.csv', 'se2016-umb-jnk-01-62.csv', 'se2016-umb-jnk-02-62.csv', 'se2016-umb-jnk-03-62.csv', 'se2016-umb-jnk-63.csv', 'se2016-umb-jnk-01-63.csv', 'se2016-umb-jnk-02-63.csv', 'se2016-umb-jnk-03-63.csv', 'se2016-umb-jnk-64.csv', 'se2016-umb-jnk-01-64.csv', 'se2016-umb-jnk-02-64.csv', 'se2016-umb-jnk-03-64.csv', 'se2016-umb-jnk-65.csv', 'se2016-umb-jnk-01-65.csv', 'se2016-umb-jnk-02-65.csv', 'se2016-umb-jnk-03-65.csv', 'se2016-umb-jnk-71.csv', 'se2016-umb-jnk-01-71.csv', 'se2016-umb-jnk-02-71.csv', 'se2016-umb-jnk-03-71.csv', \n",
        "  'se2016-umb-jnk-72.csv', 'se2016-umb-jnk-01-72.csv', 'se2016-umb-jnk-02-72.csv', 'se2016-umb-jnk-03-72.csv', 'se2016-umb-jnk-73.csv', 'se2016-umb-jnk-01-73.csv', 'se2016-umb-jnk-02-73.csv', 'se2016-umb-jnk-03-73.csv', 'se2016-umb-jnk-74.csv', 'se2016-umb-jnk-01-74.csv', 'se2016-umb-jnk-02-74.csv', 'se2016-umb-jnk-03-74.csv', 'se2016-umb-jnk-75.csv', 'se2016-umb-jnk-01-75.csv', 'se2016-umb-jnk-02-75.csv', 'se2016-umb-jnk-03-75.csv', 'se2016-umb-jnk-76.csv', 'se2016-umb-jnk-01-76.csv', 'se2016-umb-jnk-02-76.csv', 'se2016-umb-jnk-03-76.csv', \n",
        "  'se2016-umb-jnk-81.csv', 'se2016-umb-jnk-01-81.csv', 'se2016-umb-jnk-02-81.csv', 'se2016-umb-jnk-03-81.csv', 'se2016-umb-jnk-82.csv', 'se2016-umb-jnk-01-82.csv', 'se2016-umb-jnk-02-82.csv', 'se2016-umb-jnk-03-82.csv', 'se2016-umb-jnk-91.csv', 'se2016-umb-jnk-01-91.csv', 'se2016-umb-jnk-02-91.csv', 'se2016-umb-jnk-03-91.csv', 'se2016-umb-jnk-94.csv', 'se2016-umb-jnk-01-94.csv', 'se2016-umb-jnk-02-94.csv', 'se2016-umb-jnk-03-94.csv'\n",
        "]\n",
        "\n",
        "# Merge table se2016-umb-sp\n",
        "listFile = [\n",
        "  'se2016-umb-sp-11.csv', 'se2016-umb-sp-01-11.csv', 'se2016-umb-sp-02-11.csv', 'se2016-umb-sp-03-11.csv', 'se2016-umb-sp-12.csv', 'se2016-umb-sp-01-12.csv', 'se2016-umb-sp-02-12.csv', 'se2016-umb-sp-03-12.csv', 'se2016-umb-sp-13.csv', 'se2016-umb-sp-01-13.csv', 'se2016-umb-sp-02-13.csv', 'se2016-umb-sp-03-13.csv', 'se2016-umb-sp-14.csv', 'se2016-umb-sp-01-14.csv', 'se2016-umb-sp-02-14.csv', 'se2016-umb-sp-03-14.csv', 'se2016-umb-sp-15.csv', 'se2016-umb-sp-01-15.csv', 'se2016-umb-sp-02-15.csv', 'se2016-umb-sp-03-15.csv', \n",
        "  'se2016-umb-sp-16.csv', 'se2016-umb-sp-01-16.csv', 'se2016-umb-sp-02-16.csv', 'se2016-umb-sp-03-16.csv', 'se2016-umb-sp-17.csv', 'se2016-umb-sp-01-17.csv', 'se2016-umb-sp-02-17.csv', 'se2016-umb-sp-03-17.csv', 'se2016-umb-sp-18.csv', 'se2016-umb-sp-01-18.csv', 'se2016-umb-sp-02-18.csv', 'se2016-umb-sp-03-18.csv', 'se2016-umb-sp-19.csv', 'se2016-umb-sp-01-19.csv', 'se2016-umb-sp-02-19.csv', 'se2016-umb-sp-03-19.csv', 'se2016-umb-sp-21.csv', 'se2016-umb-sp-01-21.csv', 'se2016-umb-sp-02-21.csv', 'se2016-umb-sp-03-21.csv', \n",
        "  'se2016-umb-sp-31.csv', 'se2016-umb-sp-01-31.csv', 'se2016-umb-sp-02-31.csv', 'se2016-umb-sp-03-31.csv', 'se2016-umb-sp-32.csv', 'se2016-umb-sp-01-32.csv', 'se2016-umb-sp-02-32.csv', 'se2016-umb-sp-03-32.csv', 'se2016-umb-sp-33.csv', 'se2016-umb-sp-01-33.csv', 'se2016-umb-sp-02-33.csv', 'se2016-umb-sp-03-33.csv', 'se2016-umb-sp-34.csv', 'se2016-umb-sp-01-34.csv', 'se2016-umb-sp-02-34.csv', 'se2016-umb-sp-03-34.csv', 'se2016-umb-sp-35.csv', 'se2016-umb-sp-01-35.csv', 'se2016-umb-sp-02-35.csv', 'se2016-umb-sp-03-35.csv', \n",
        "  'se2016-umb-sp-36.csv', 'se2016-umb-sp-01-36.csv', 'se2016-umb-sp-02-36.csv', 'se2016-umb-sp-03-36.csv', 'se2016-umb-sp-51.csv', 'se2016-umb-sp-01-51.csv', 'se2016-umb-sp-02-51.csv', 'se2016-umb-sp-03-51.csv', 'se2016-umb-sp-52.csv', 'se2016-umb-sp-01-52.csv', 'se2016-umb-sp-02-52.csv', 'se2016-umb-sp-03-52.csv', 'se2016-umb-sp-53.csv', 'se2016-umb-sp-01-53.csv', 'se2016-umb-sp-02-53.csv', 'se2016-umb-sp-03-53.csv', 'se2016-umb-sp-61.csv', 'se2016-umb-sp-01-61.csv', 'se2016-umb-sp-02-61.csv', 'se2016-umb-sp-03-61.csv', \n",
        "  'se2016-umb-sp-62.csv', 'se2016-umb-sp-01-62.csv', 'se2016-umb-sp-02-62.csv', 'se2016-umb-sp-03-62.csv', 'se2016-umb-sp-63.csv', 'se2016-umb-sp-01-63.csv', 'se2016-umb-sp-02-63.csv', 'se2016-umb-sp-03-63.csv', 'se2016-umb-sp-64.csv', 'se2016-umb-sp-01-64.csv', 'se2016-umb-sp-02-64.csv', 'se2016-umb-sp-03-64.csv', 'se2016-umb-sp-65.csv', 'se2016-umb-sp-01-65.csv', 'se2016-umb-sp-02-65.csv', 'se2016-umb-sp-03-65.csv', 'se2016-umb-sp-71.csv', 'se2016-umb-sp-01-71.csv', 'se2016-umb-sp-02-71.csv', 'se2016-umb-sp-03-71.csv', \n",
        "  'se2016-umb-sp-72.csv', 'se2016-umb-sp-01-72.csv', 'se2016-umb-sp-02-72.csv', 'se2016-umb-sp-03-72.csv', 'se2016-umb-sp-73.csv', 'se2016-umb-sp-01-73.csv', 'se2016-umb-sp-02-73.csv', 'se2016-umb-sp-03-73.csv', 'se2016-umb-sp-74.csv', 'se2016-umb-sp-01-74.csv', 'se2016-umb-sp-02-74.csv', 'se2016-umb-sp-03-74.csv', 'se2016-umb-sp-75.csv', 'se2016-umb-sp-01-75.csv', 'se2016-umb-sp-02-75.csv', 'se2016-umb-sp-03-75.csv', 'se2016-umb-sp-76.csv', 'se2016-umb-sp-01-76.csv', 'se2016-umb-sp-02-76.csv', 'se2016-umb-sp-03-76.csv', \n",
        "  'se2016-umb-sp-81.csv', 'se2016-umb-sp-01-81.csv', 'se2016-umb-sp-02-81.csv', 'se2016-umb-sp-03-81.csv', 'se2016-umb-sp-82.csv', 'se2016-umb-sp-01-82.csv', 'se2016-umb-sp-02-82.csv', 'se2016-umb-sp-03-82.csv', 'se2016-umb-sp-91.csv', 'se2016-umb-sp-01-91.csv', 'se2016-umb-sp-02-91.csv', 'se2016-umb-sp-03-91.csv', 'se2016-umb-sp-94.csv', 'se2016-umb-sp-01-94.csv', 'se2016-umb-sp-02-94.csv', 'se2016-umb-sp-03-94.csv'\n",
        "]\n",
        "\n",
        "# Set working directory 01\n",
        "workingDirectory = dictDirectory['se-2016-umb-sp-data']\n",
        "os.chdir(workingDirectory)\n",
        "path = !pwd\n",
        "# print (path)\n",
        "\n",
        "loop = 0\n",
        "for i in range (0, 136, 4):\n",
        "  # print (listFile[i], \"merge with\", listFile[i+1], \"into\", listFile[i+2])\n",
        "  fname = listFile[i]     # Merge data files\n",
        "  fnameA = listFile[i+1]\n",
        "  fnameB  = listFile[i+2]\n",
        "  fnameC  = listFile[i+3]\n",
        "  pathSourceA = path[0] + '/data-01/' + fnameA\n",
        "  pathSourceB = path[0] + '/data-02/' + fnameB\n",
        "  pathSourceC = path[0] + '/data-03/' + fnameC\n",
        "  pathDestination = path[0] + '/' + fname\n",
        "  \n",
        "  # print (pathSourceA, pathSourceB, pathSourceC, pathDestination)\n",
        "  dfBPSDataA = pd.read_csv(pathSourceA)\n",
        "  dfBPSDataB = pd.read_csv(pathSourceB)\n",
        "  dfBPSDataC = pd.read_csv(pathSourceC)\n",
        "\n",
        "  # Rename joining keys\n",
        "  dfBPSDataA.rename({'IDPERUSAHA': 'PERUSAHAAN_ID', 'PROV': 'PROVINSI_IDA', 'SKALAUSAHA': 'SKALAUSAHA_A', 'WEIGHT': 'WEIGHT_A'}, axis='columns', inplace=True)\n",
        "  dfBPSDataB.rename({'IDPERUSAHA': 'PERUSAHAAN_ID', 'JENISKUESI': 'JENISKUESIONER_B', 'PROV': 'PROVINSI_IDB', 'SKALAUSAHA': 'SKALAUSAHA_B', 'WEIGHT': 'WEIGHT_B'}, axis='columns', inplace=True)\n",
        "  dfBPSDataC.rename({'IDPERUSAHA': 'PERUSAHAAN_ID', 'JENISKUESI': 'JENISKUESIONER_C', 'PROV': 'PROVINSI_IDC', 'SKALAUSAHA': 'SKALAUSAHA_C', 'WEIGHT': 'WEIGHT_C'}, axis='columns', inplace=True)\n",
        "  \n",
        "  dfBPSData = [dfBPSDataA, dfBPSDataB, dfBPSDataC]\n",
        "\n",
        "  # Merge data files\n",
        "  # dfBPSData = dfBPSDataA.merge(dfBPSDataB, left_on='IDPERUSAHAAN_A', right_on='IDPERUSAHAAN_B')\n",
        "  dfBPSDataMerge = reduce(lambda left,right: pd.merge(left,right,on='PERUSAHAAN_ID'), dfBPSData)\n",
        "  \n",
        "  # print (pathDestination, \"dfBPSDataA.shape:\", dfBPSDataA.shape, \"dfBPSDataB.shape:\", dfBPSDataB.shape)\n",
        "  # print (pathDestination, \"dfBPSDataA.shape:\", dfBPSDataA.shape, \"dfBPSDataB.shape:\", dfBPSDataB.shape, \"dfBPSDataMerge.shape:\", dfBPSData.shape)\n",
        "  # print (\"dfBPSDataA.shape:\", dfBPSDataA.shape[0], \"dfBPSDataB.shape:\", dfBPSDataB.shape[0], \"dfBPSDataMerge.shape:\", dfBPSData.shape[0])\n",
        "\n",
        "  # Save data from merge data type operation\n",
        "  print (pathDestination, dfBPSDataA.shape, dfBPSDataMerge.shape)\n",
        "  # print (fname, dfBPSDataA.shape, dfBPSDataMerge.shape)\n",
        "  dfBPSDataMerge.to_csv(pathDestination, encoding='utf-8', index=False)\n",
        "\n",
        "  loop += 1\n",
        "\n",
        "print (loop)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZnUlRjp_hK-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "# Merge table se2016-umk\n",
        "# Set working directory 01\n",
        "workingDirectory = dictDirectory['se-2016-umb-sp-data']\n",
        "os.chdir(workingDirectory)\n",
        "path = !pwd\n",
        "# print (path)\n",
        "\n",
        "# List file on working directory 02\n",
        "listFile = [f for f in glob.glob(\"*.csv\")]\n",
        "\n",
        "pathDestination = path[0] + '/' + 'se2016-umb-sp-merge.csv'\n",
        "dfMerges = []\n",
        "totalRows = 0\n",
        "loop = 0\n",
        "for e in listFile:\n",
        "  pathSource = path[0] + '/' + e\n",
        "  # print ('merge ' + pathSource)\n",
        "\n",
        "  # Read and import csv file dataset into pandas data frame, change paths if needed\n",
        "  dfBPSData = pd.read_csv(pathSource)\n",
        "\n",
        "  dfMerges.append(dfBPSData)\n",
        "  print(\"dfBPSData.shape :\", e, dfBPSData.shape)\n",
        "  totalRows += dfBPSData.shape[0]\n",
        "  loop += 1\n",
        "\n",
        "print (loop)\n",
        "\n",
        "\n",
        "dfBPSDataMerge = pd.concat(dfMerges)\n",
        "print(\"dfBPSDataMerge.shape :\", dfBPSDataMerge.shape, totalRows)\n",
        "\n",
        "print (pathDestination)\n",
        "dfBPSDataMerge.to_csv(pathDestination, encoding='utf-8', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fjcS1i_x2zm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Examine dataset, see data type\n",
        "dfBPSDataMerge.info(verbose=True, null_counts=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TDt3_F-1TEC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pwd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foPWry8Evs0o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "# Read and import csv file dataset into pandas data frame, change paths if needed\n",
        "\n",
        "# Set working directory 01\n",
        "os.chdir(dictDirectory['se-2016-umb-jk'])\n",
        "path = !pwd\n",
        "print (path)\n",
        "\n",
        "# List file on working directory 02\n",
        "listFile = [f for f in glob.glob(\"*.csv\")]\n",
        "\n",
        "pathDestination = path[0] + '/' + 'se-2016-umb-jk-merge.csv'\n",
        "dfMerges = []\n",
        "totalRows = 0\n",
        "loop = 0\n",
        "for e in listFile:\n",
        "  pathSource = path[0] + '/' + e\n",
        "  # print ('merge ' + pathSource)\n",
        "\n",
        "  # Read and import csv file dataset into pandas data frame, change paths if needed\n",
        "  dfBPSData = pd.read_csv(pathSource)\n",
        "\n",
        "  dfMerges.append(dfBPSData)\n",
        "  print(\"dfBPSData.shape :\", dfBPSData.shape)\n",
        "  totalRows += dfBPSData.shape[0]\n",
        "  # print(\"type(dfBPSData) :\", type(dfBPSData))\n",
        "  loop += 1\n",
        "\n",
        "print (loop)\n",
        "\n",
        "\n",
        "dfBPSDataMerge = pd.concat(dfMerges)\n",
        "print(\"dfBPSDataMerge.shape :\", dfBPSDataMerge.shape, totalRows)\n",
        "\n",
        "print (pathDestination)\n",
        "dfBPSDataMerge.to_csv(pathDestination, encoding='utf-8', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aI4eQISe648Z",
        "colab_type": "text"
      },
      "source": [
        "## Step 0207 Create Data Description"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUewNpHp7Djt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sample json file for rawdata IBS 1993\n",
        "[\n",
        "\t{\n",
        "\t\t\"name\": \"DSTATS93\",\n",
        "\t\t\"type\": \"String\",\n",
        "\t\t\"description\": \"Status Permodalan\"\n",
        "\t},\n",
        "\t{\n",
        "\t\t\"name\": \"DETYPE93\",\n",
        "\t\t\"type\": \"String\",\n",
        "\t\t\"description\": \"Bentuk Badan Hukum\"\n",
        "\t},\n",
        "\t{\n",
        "\t\t\"name\": \"DPROVI93\",\n",
        "\t\t\"type\": \"String\",\n",
        "\t\t\"description\": \"Propinsi\"\n",
        "\t},\n",
        "\t{\n",
        "\t\t\"name\": \"DKABUP93\",\n",
        "\t\t\"type\": \"String\",\n",
        "\t\t\"description\": \"Kabupaten/Kotamadya\"\n",
        "\t},\n",
        "\t{\n",
        "\t\t\"name\": \"DSRVYR93\",\n",
        "\t\t\"type\": \"String\",\n",
        "\t\t\"description\": \"Tahun Survei\"\n",
        "\t},\n",
        "\t{\n",
        "\t\t\"name\": \"DYRSTR93\",\n",
        "\t\t\"type\": \"String\",\n",
        "\t\t\"description\": \"Tahun Mulai Produksi Komersial di Propinsi ini\"\n",
        "\t},\n",
        " \n",
        "...\n",
        "\n",
        "\t{\n",
        "\t\t\"name\": \"LPDNOU93\",\n",
        "\t\t\"type\": \"Integer\",\n",
        "\t\t\"description\": \"Jumlah Banyaknya Pekerja/Karyawan Pekerja (Produksi + Lainnya) (Laki-laki + Perempuan) dibayar rata-rata setiap bulan\"\n",
        "\t},\n",
        "\t{\n",
        "\t\t\"name\": \"LTLNOU93\",\n",
        "\t\t\"type\": \"Integer\",\n",
        "\t\t\"description\": \"Jumlah Banyaknya Pekerja/Karyawan Pekerja (Produksi + Lainnya) (dibayar + tidak dibayar) (Laki-laki + Perempuan) rata-rata setiap bulan\"\n",
        "\t},\n",
        "\n",
        " ...\n",
        "\n",
        "\t{\n",
        "\t\t\"name\": \"EWOVCE93\",\n",
        "\t\t\"type\": \"Integer\",\n",
        "\t\t\"description\": \"Nilai Kayu Bakar dipakai selama tahun 1993 (Pembangkit Listrik)\"\n",
        "\t},\n",
        "\t{\n",
        "\t\t\"name\": \"ENCVCE93\",\n",
        "\t\t\"type\": \"Integer\",\n",
        "\t\t\"description\": \"Nilai Bahan Bakar Lainnya dipakai selama tahun 1993 (Pembangkit Listrik)\"\n",
        "\t},\n",
        "\t{\n",
        "\t\t\"name\": \"ETLQUE93\",\n",
        "\t\t\"type\": \"Integer\",\n",
        "\t\t\"description\": \"Banyaknya Bahan Bakar Lainnya dipakai selama tahun 1993 (Pembangkit Listrik)\"\n",
        "\t},\n",
        "\t{\n",
        "\t\t\"name\": \"NST93\",\n",
        "\t\t\"type\": \"String\",\n",
        "\t\t\"description\": \"NST93 Variabel tidak digunakan\"\n",
        "\t},\n",
        "\t{\n",
        "\t\t\"name\": \"PSID\",\n",
        "\t\t\"type\": \"Integer\",\n",
        "\t\t\"description\": \"PSID Variabel\"\n",
        "\t}\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaN2SrfQsBwO",
        "colab_type": "text"
      },
      "source": [
        "# Step 03 - Data Preparation\n",
        "In this step, we pre-process the data, clean it, wrangle it, and\n",
        "manipulate it as needed. Initial exploratory data analysis is also carried out.\n",
        "* **Data Processing & Wrangling**: \n",
        "  Mainly concerned with data processing, cleaning, munging, wrangling and performing initial descriptive and exploratory data analysis\n",
        "* **Feature Extraction & Engineering**: Here, we extract important features or attributes from the raw data and even create or engineer new features from existing features.\n",
        "* **Feature Scaling & Selection**: Data features often need to be normalized and scaled to prevent Machine Learning algorithms from getting biased. Besides this, often we need to select a subset of all available features based on feature importance and quality.\n",
        "\n",
        "Final Update 20200315"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gLlMZJdsBwP",
        "colab_type": "text"
      },
      "source": [
        "## Step 0301 Dataset Summary Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qb8GwcDVsBwQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Examine dataset, shape, rows and columns\n",
        "print(\"dfTrain shape   :\", dfTrain.shape)\n",
        "print(\"type(dfTrain)   :\", type(dfTrain))\n",
        "print(\"dfTrain.index   :\", dfTrain.index)\n",
        "print(\"dfTrain.columns :\", dfTrain.columns, \"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7YKu4bCsBwU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Examine dataset, first 5 rows\n",
        "# dfTrain.head()\n",
        "dfTrain.head().T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snS2w77YsBwX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Examine dataset, types of all features and total dataframe size in memory\n",
        "dfTrain.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxoJLmrksBwb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Examine dataset, types of all features and total dataframe size in memory\n",
        "dfTrain.describe().T\n",
        "# dfTrain.describe(include='all').T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UkNenLVsBwf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dfTrain.columns.isna().any()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8PxB-22sBw9",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Step 04 - Deployment and Monitoring\n",
        "Datawarehouse are deployed in production and are constantly monitored based on their performance and transformation.\n",
        "\n",
        "Final Update 20200315"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-WBgjeTnOT8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "# Big Query delete table se2016-listing\n",
        "listBQFile = [\n",
        "  'se_2016_listing_11', 'se_2016_listing_12', 'se_2016_listing_13', 'se_2016_listing_14', 'se_2016_listing_15', \n",
        "  'se_2016_listing_16', 'se_2016_listing_17', 'se_2016_listing_18', 'se_2016_listing_19', 'se_2016_listing_21', \n",
        "  'se_2016_listing_31', 'se_2016_listing_32', 'se_2016_listing_33', 'se_2016_listing_34', 'se_2016_listing_35', \n",
        "  'se_2016_listing_36', 'se_2016_listing_51', 'se_2016_listing_52', 'se_2016_listing_53', 'se_2016_listing_61', \n",
        "  'se_2016_listing_62', 'se_2016_listing_63', 'se_2016_listing_64', 'se_2016_listing_65', 'se_2016_listing_71', \n",
        "  'se_2016_listing_72', 'se_2016_listing_73', 'se_2016_listing_74', 'se_2016_listing_75', 'se_2016_listing_76', \n",
        "  'se_2016_listing_81', 'se_2016_listing_82', 'se_2016_listing_91', 'se_2016_listing_94', 'se_2016_listing_merge' \n",
        "]\n",
        "\n",
        "# Set working directory on Google Big Query 01\n",
        "projectId = 'datawarehouse-001'\n",
        "directoryBQ = ['datawarehouse-001:04_sensus_ekonomi', 'datawarehouse-001:04_sensus_ekonomi_rawdata']\n",
        "\n",
        "# List file on Google Big Query working directory 02\n",
        "# !bq ls --max_results=1000 {directoryBQ[0]}\n",
        "\n",
        "# !bq rm --help\n",
        "\n",
        "loop = 0\n",
        "for e in listBQFile:\n",
        "  bqFileName = directoryBQ[0] + \".\" + e\n",
        "  # !bq rm -f -t {bqFileName}\n",
        "  print (bqFileName)\n",
        "  loop += 1\n",
        "\n",
        "print (loop)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oa_1isjNsBw-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01-DW Project-Data-PreProcessing-20210801",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ygautomo/02-Prospera-Datawarehouse/blob/master/01-DW-Project-Data-PreProcessing-20210801.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZBBBH88sBv3"
      },
      "source": [
        "# **Data Warehouse Project- Google Drive, Google Cloud Storage & AWS S3 PreProcessing Files**\n",
        "## Data Warehouse Project Steps and Code\n",
        "Status : Last Update 20210801"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9jcGOKSsBv6"
      },
      "source": [
        "## **Python Environment Setup**\n",
        "We will be using a several different libraries throughout this steps. If you've successfully completed the [installation instructions](https://github.com/cs109/content/wiki/Installing-Python), all of the following statements should run."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYojFb3vUSbm"
      },
      "source": [
        "### Setup Python Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmTtlwW1sBv8"
      },
      "source": [
        "# Final Update 20201201\n",
        "# Reference https://docs.python.org/3/py-modindex.html\n",
        "# Reference https://towardsdatascience.com/10-tips-for-a-better-google-colab-experience-33f8fe721b82\n",
        "\n",
        "# Access system-specific parameters and functions. This module provides a portable way of using operating system dependent functionality\n",
        "import sys\n",
        "print(\"Python version:        %6.6s(need at least 3.5.0)\" % sys.version)              # (need at least 3.5.0)\n",
        "\n",
        "# IPython: tools for interactive and parallel computing in Python\n",
        "import IPython\n",
        "print(\"IPython version:      %6.6s (need at least 6.0.0)\" % IPython.__version__)      # (need at least 6.0.0)\n",
        "\n",
        "# Matplotlib is a comprehensive library for creating static, animated, and interactive visualizations in Python\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "print(\"Mapltolib version:    %6.6s (need at least 3.0.0)\" % matplotlib.__version__)   # (need at least 3.0.0)\n",
        "\n",
        "# NumPy is the fundamental package for scientific computing with Python\n",
        "import numpy as np\n",
        "print(\"Numpy version:        %6.6s (need at least 1.15.0)\" % np.__version__)          # (need at least 1.15.0)\n",
        "\n",
        "# Pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool,\n",
        "# built on top of the Python programming language\n",
        "import pandas as pd\n",
        "print(\"Pandas version:       %6.6s (need at least 0.20.0)\" % pd.__version__)          # (need at least 0.20.0)\n",
        "\n",
        "# Scikit-learn is an open source machine learning library that supports supervised and unsupervised learning. It also provides various tools for model fitting, data preprocessing, model selection and evaluation, and many other utilities\n",
        "import sklearn as sk\n",
        "print(\"Scikit-Learn version: %6.6s (need at least 0.15.0)\" % sk.__version__)          # (need at least 0.15.0)\n",
        "\n",
        "# Seaborn is a library for making statistical graphics in Python. It builds on top of matplotlib and integrates closely with pandas data structures\n",
        "import seaborn as sns\n",
        "print(\"Seaborn version:      %6.6s (need at least 0.5.0)\" % sns.__version__)          # (need at least 0.5.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewifNF5csBwC"
      },
      "source": [
        "# Customized python environment Setup\n",
        "pd.set_option('display.precision', 2)\n",
        "\n",
        "# Pure python package for reading/writing dBase, FoxPro, and Visual FoxPro .dbf files (including memos)\n",
        "!pip install dbf\n",
        "import dbf\n",
        "\n",
        "# Higher-order functions and operations on callable objects. The functools module is for higher-order functions: functions that act on or return other functions\n",
        "from functools import reduce\n",
        "\n",
        "# Unix style pathname pattern expansion. The glob module finds all the pathnames matching a specified pattern according to the rules used by the Unix shell, although results are returned in arbitrary order\n",
        "import glob\n",
        "\n",
        "# Core tools for working with streams. The io module provides Pythonâ€™s main facilities for dealing with various types of I/O\n",
        "from io import BytesIO\n",
        "\n",
        "# Encode and decode the JSON format- standard library module\n",
        "import json\n",
        "\n",
        "# Mathematical functions (sin() etc.). This module provides access to the mathematical functions defined by the C standard\n",
        "# import math\n",
        "\n",
        "# Miscellaneous operating system interfaces. This module provides a portable way of using operating system dependent functionality\n",
        "import os\n",
        "\n",
        "# Generate pseudo-random numbers. This module implements pseudo-random number generators for various distributions\n",
        "import random\n",
        "\n",
        "# Convert DBF files to CSV, DataFrames, HDF5 tables, and SQL tables. Python3 compatible\n",
        "!pip install simpledbf\n",
        "from simpledbf import Dbf5\n",
        "\n",
        "# Urllib is a package that collects several modules for working with URLs\n",
        "import urllib.request"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBxS6bCAsBwF"
      },
      "source": [
        "# **Machine Learning Pipeline:**\n",
        "![alt text](https://drive.google.com/uc?id=1zUK9aLiPk1zReXV19RMUQjqe3BrcvbyM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AP-MC1sqI7A"
      },
      "source": [
        "# **Step 01 - Project Goals & Problems**\n",
        "* Develop Datawarehouse for Prospera, which data is taken from Egnyte nad transform the data into Google BigQuery as Datawarehouse Platform."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3B9doOdrqOMu"
      },
      "source": [
        "# **Step 02 - Data Retrieval**\n",
        "Data retrieval: This is mainly data collection, extraction, and acquisition from various data sources and data stores.\n",
        "\n",
        "Data retrieval process: \n",
        "1. Take raw data from Egnyte\n",
        "2. Standardize file name (linux file system)\n",
        "3. Convert into csv files\n",
        "4. Check and review data\n",
        "5. Convert data type if neccessary\n",
        "6. Merge data if necessary\n",
        "7. Create data description and save into json\n",
        "8. Put raw data into Google Cloud Storage\n",
        "9. Upload and transform the data into Google BigQuery\n",
        "\n",
        "Final Update 20210801"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MN4ak1gLHd0g"
      },
      "source": [
        "## 0201 01 Data Retrieval Environment Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLHxodMCH1qO"
      },
      "source": [
        "# Create Google Drive Environment Variables\n",
        "# Sakernas data pre-processing\n",
        "workingDirectory = '/content/drive/MyDrive/04\\ Rawdata/04-sakernas'\n",
        "\n",
        "# Susenas data pre-processing\n",
        "# workingDirectory = \"/content/drive/MyDrive/04\\ Rawdata/06-susenas\"\n",
        "\n",
        "print(workingDirectory, type(workingDirectory))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tXrBSQzUDoQ"
      },
      "source": [
        "## 0202 Mount Google Drive, Google Cloud Storage & AWS S3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fpAhnwvHu4p"
      },
      "source": [
        "### 0202 01 Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfKD-Ihlj4a2"
      },
      "source": [
        "# Mount Google Drive\n",
        "\n",
        "# Colaboratory-specific python libraries- non-standard library module\n",
        "# !pip install google-colab\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# drive.flush_and_unmount()\n",
        "# drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "!pwd\n",
        "!ls -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTa1MQ6IW3Di"
      },
      "source": [
        "!ls drive/My\\ Drive/*/ -d\n",
        "# !ls drive/My\\ Drive/* -d\n",
        "# !stat drive/My\\ Drive/04\\ Rawdata/04-sakernas/sakernas-2000 --format=%n:%s *\n",
        "# os.chdir(workingDirectory)\n",
        "# !du -ak"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcSP3izeQknN"
      },
      "source": [
        "# Create Google Drive dictionary\n",
        "# import json                     # Encode and decode the JSON format- standard library module\n",
        "# import os                       # Miscellaneous operating system interfaces- standard library module\n",
        "\n",
        "# Sakernas data pre-processing\n",
        "# workingDirectory = '/content/drive/MyDrive/04\\ Rawdata/04-sakernas'\n",
        "# print(workingDirectoy, type(workingDirectory))\n",
        "# os.chdir(workingDirectory)\n",
        "# !ls -1\n",
        "\n",
        "# Susenas data pre-processing\n",
        "# workingDirectory = \"/content/drive/MyDrive/04\\ Rawdata/06-susenas\"\n",
        "# print(workingDirectory, type(workingDirectory))\n",
        "# os.chdir(workingDirectory)\n",
        "# !ls -1\n",
        "\n",
        "lDirectoryKeys = !ls {workingDirectory} -1\n",
        "lDirectoryValues = []\n",
        "for idx, el in enumerate(lDirectoryKeys):\n",
        "  strDrive = workingDirectory\n",
        "  # strDrive = workingDirectory.replace(\"\\\\\",\"\\\")\n",
        "  lDirectoryValues.append(strDrive+\"/\"+el)\n",
        "  # print(idx, el)\n",
        "\n",
        "dictDrive = dict(zip(lDirectoryKeys, lDirectoryValues))\n",
        "# print(dictDrive)\n",
        "print(json.dumps(dictDrive, indent = 4))\n",
        "\n",
        "# Copy data using gsutil cp command\n",
        "# !gsutil -m cp -r /content/drive/My\\ Drive/04\\ Rawdata/06-susenas/susenas-2000/ gs://bucket-prospera-datawarehouse-201014-01/01-rawdata/01-bps/06-susenas/\n",
        "# !gsutil -m cp gs://prospera-spending-bucket-201115/DataRKASAwal/v_rkas.csv /content/drive/MyDrive/04\\ Rawdata/07-bos/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5aIKZg9QD1UP"
      },
      "source": [
        "# Sakernas data pre-processing\n",
        "!ls {dictDrive[\"sakernas-2000\"]}/data/*\n",
        "\n",
        "# Susenas data pre-processing\n",
        "# !ls {dictDrive[\"susenas-2007\"]}/data\\ tnp2k/*.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMOs7c0XH8Yd"
      },
      "source": [
        "### 0202 02 Mount Google Cloud Storage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWNH0VHoYhdk"
      },
      "source": [
        "# Mount Google Cloud Storage\n",
        "\n",
        "# Colaboratory-specific python libraries non-standard library module\n",
        "# !pip install google-colab\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Project Datawarehouse Prospera {billing: prospera}\n",
        "# gcpProjectID = 'datawarehouse-001'\n",
        "# gcpBucketID = 'bucket-prospera-01'\n",
        "\n",
        "# Project Datawarehouse Prospera {acct: ygautomo; billing: GCP Billing Account 02}\n",
        "gcpProjectID = 'prospera-datawarehouse-201014'\n",
        "# gcpBucketID = 'bucket-prospera-datawarehouse-201014-01'\n",
        "\n",
        "# Project Datawarehouse Prospera {acct: ygautomo; billing: GCP Billing Account 02}\n",
        "# gcpProjectID = 'smile-database-210122'\n",
        "# gcpBucketID = 'bucket-smile-database-01'\n",
        "\n",
        "# !gcloud config set project {gcpProjectID}\n",
        "# !gsutil ls gs://{gcpBucketID}/01-rawdata/\n",
        "\n",
        "# Project Datawarehouse Prospera Spending\n",
        "# gcpProjectID = 'prospera-spending-201115'\n",
        "# gcpBucketID = 'prospera-spending-bucket-201115'\n",
        "\n",
        "!gcloud config set project {gcpProjectID}\n",
        "!gsutil ls\n",
        "# !gsutil ls gs://{gcpBucketID}/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1A0qDyTywYaS"
      },
      "source": [
        "# Create Google Storage Bucket dictionary\n",
        "lBucketValues = !gsutil ls\n",
        "lBucketKeys = []\n",
        "for idx, el in enumerate(lBucketValues):\n",
        "  strBucket = lambda x: \"bucket0\" if x < 10 else \"bucket\"\n",
        "  lBucketKeys.append(strBucket(idx+1)+str(idx+1))\n",
        "  # print(idx, el)\n",
        "\n",
        "dictBucket = dict(zip(lBucketKeys, lBucketValues))\n",
        "print(dictBucket)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UR7gvXEIlDo6"
      },
      "source": [
        "# Mount Google Cloud Storage -- Setup Boto files\n",
        "\n",
        "# Overview current credentials\n",
        "!cat '/content/.config/legacy_credentials/{{email}}/.boto'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuceIfMlt6EC"
      },
      "source": [
        "# Cloud Storage Client Libraries\n",
        "\n",
        "# Create/interact with Google Cloud Storage binary large objects (blob)\n",
        "from google.cloud import storage\n",
        "client = storage.Client(project=gcpProjectID)\n",
        "bucket = client.get_bucket(gcpBucketID)\n",
        "\n",
        "# all_blobs = list(bucket.list_blobs())\n",
        "blobs = bucket.blob('DataRKASAwal/v_rkas.zip')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXRIFsJ4IXv-"
      },
      "source": [
        "### 0202 03 Access AWS S3\n",
        "Setup boto files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oUp5fNcemb_"
      },
      "source": [
        "# Access AWS S3 -- Boto files\n",
        "\n",
        "# Boto is the Amazon Web Services (AWS) SDK for Python- non-standard pyhton libraries\n",
        "# !pip install boto3\n",
        "# import boto3\n",
        "\n",
        "# reference https://realpython.com/python-boto3-aws-s3/\n",
        "# Add Credentials within .boto files\n",
        "# [OAuth2]\n",
        "# client_id = \n",
        "# client_secret = \n",
        "#\n",
        "# [Credentials]\n",
        "# gs_oauth2_refresh_token = \n",
        "# aws_access_key_id = \n",
        "# aws_secret_access_key = \n",
        "#\n",
        "# [s3]\n",
        "# use-sigv4=True\n",
        "# host=s3.us-east-2.amazonaws.com\n",
        "\n",
        "# !gcloud init\n",
        "# !gsutil config\n",
        "# !gsutil version -l\n",
        "# !gcloud config list\n",
        "\n",
        "# !cat '/content/drive/My Drive/07 Google Colab/.boto'\n",
        "# !cat '/content/.config/legacy_credentials/{{email}}/.boto'\n",
        "\n",
        "# !gsutil cp '/content/drive/My Drive/07 Google Colab/.boto' '/content/.config/legacy_credentials/{{email}}/.boto'\n",
        "\n",
        "awsBucketID = '01-rawdata'\n",
        "!gsutil ls s3://{awsBucketID}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNk3UHCoIymM"
      },
      "source": [
        "### 0202 04 Access Google BigQuery\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rqsq0gYvkM8K"
      },
      "source": [
        "# Access Google Big Query\n",
        "\n",
        "# Google Colaboratory tools\n",
        "# !pip install google-colab\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Project Datawarehouse Prospera {billing: prospera} -- Set BQ working directory\n",
        "# gcpProjectID = 'datawarehouse-001'\n",
        "# gcpBucketID = 'bucket-prospera-01'\n",
        "dictBQDirectory = {\n",
        "  'ifls': 'datawarehouse-001:01_ifls',\n",
        "  'mfg': 'datawarehouse-001:02_manufacturing',\n",
        "  'podes': 'datawarehouse-001:03_podess',\n",
        "  'sakernas': 'datawarehouse-001:04_sakernas',\n",
        "  'se': 'datawarehouse-001:05_sensus_ekonomi',\n",
        "  'susenas': 'datawarehouse-001:06_susenas'\n",
        "}\n",
        "\n",
        "# Project Datawarehouse Prospera {billing: ygautomo} -- Set BQ working directory\n",
        "gcpProjectID = 'prospera-datawarehouse-201014'\n",
        "gcpBucketID = 'bucket-prospera-datawarehouse-201014-01'\n",
        "dictBQDirectory = {}\n",
        "\n",
        "!gcloud config set project {gcpProjectID}\n",
        "!bq ls --project_id={gcpProjectID}\n",
        "# !bq ls --project_id={gcpProjectID} {dictBQDirectory['susenas']}\n",
        "# !bq help"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UyR_NEBJtvW"
      },
      "source": [
        "# Create BigQuery Dataset dictionary\n",
        "lDataset = !bq ls --project_id={gcpProjectID}\n",
        "lDataset = lDataset.fields(0)\n",
        "lDataset = lDataset[2:len(lDataset)]\n",
        "print(lDataset)\n",
        "lDatasetKeys = []\n",
        "lDatasetValues = []\n",
        "for el in lDataset:\n",
        "  lDatasetKeys.append(el[3:])\n",
        "  lDatasetValues.append(gcpProjectID+\":\"+el)\n",
        "\n",
        "print(lDatasetKeys)\n",
        "dictBQDirectory = dict(zip(lDatasetKeys, lDatasetValues))\n",
        "print(dictBQDirectory)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2LN_sCYdqYB"
      },
      "source": [
        "!bq ls 06_susenas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATNGUvSexpk4"
      },
      "source": [
        "## Step 0203 Google Drive PreProcessing Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyvmhXFn6MZn"
      },
      "source": [
        "# List Google Drive directory contents\n",
        "\n",
        "# Encode and decode the JSON format standard library module\n",
        "# import json\n",
        "\n",
        "# print(dictDrive)\n",
        "print(json.dumps(dictDrive, indent = 4))\n",
        "!ls {dictDrive[\"sakernas-2000\"]}/*\n",
        "# !ls {dictDrive[\"susenas-2000\"]}/*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wJwvA-PT-14"
      },
      "source": [
        "# List Google Cloud Storage directory contents \n",
        "print(dictBucket[\"bucket01\"]+\"01-rawdata/\")\n",
        "# !gsutil ls {dictBucket[\"bucket01\"]}\"*\"\n",
        "!gsutil ls {dictBucket[\"bucket01\"]}\"01-rawdata/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvJd2MUijVaJ"
      },
      "source": [
        "# Copy data from Google Drive to Google Cloud Storage\n",
        "# Miscellaneous operating system interfaces\n",
        "import os\n",
        "\n",
        "# workingDirectory = '/content/drive/My Drive/04 Rawdata/06-susenas/susenas-2000'\n",
        "workingDirectory = '/content/drive/My Drive/99 Shared/Database Plan'\n",
        "print(workingDirectory)\n",
        "os.chdir(workingDirectory)\n",
        "!ls\n",
        "\n",
        "# Copy data using gsutil cp command\n",
        "# !gsutil -m cp -r {dictDrive[\"susenas-2006\"]} {dictBucket[\"bucket01\"]}\"01-rawdata/01-bps/06-susenas/\"\n",
        "# !gsutil -m cp -r /content/drive/My\\ Drive/04\\ Rawdata/06-susenas/susenas-2000/ gs://bucket-prospera-datawarehouse-201014-01/01-rawdata/01-bps/06-susenas/\n",
        "# !gsutil -m cp /content/drive/MyDrive/04\\ Rawdata/07-bos/v_rkas.dta gs://bucket-prospera-datawarehouse-201014-01/01-rawdata/03-bos/\n",
        "# !gsutil -m cp gs://prospera-spending-bucket-201115/DataRKASAwal/v_rkas.csv /content/drive/MyDrive/04\\ Rawdata/07-bos/\n",
        "!gsutil -m cp /content/drive/My\\ Drive/99\\ Shared/Database\\ Plan/202101240724_smile_prod.sql gs://bucket-smile-database-01/01-database/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZL6O1oaIeEq9"
      },
      "source": [
        "# Copy data from Google Drive to Google Cloud Storage\n",
        "# Miscellaneous operating system interfaces\n",
        "import os\n",
        "\n",
        "'''workingDirectory = '/content/drive/My Drive/04 Rawdata/06-susenas/susenas-2000'\n",
        "print(workingDirectory)\n",
        "os.chdir(workingDirectory)\n",
        "!ls'''\n",
        "\n",
        "# Copy data using gsutil cp command\n",
        "# !gsutil -m cp /content/drive/MyDrive/04\\ Rawdata/07-bos/v_rkas.dta gs://bucket-prospera-datawarehouse-201014-01/01-rawdata/03-bos/\n",
        "# !gsutil -m cp -r /content/drive/MyDrive/04\\ Rawdata/04-sakernas/sakernas-2012/ gs://bucket-prospera-datawarehouse-201014-01/01-rawdata/01-bps/04-sakernas/\n",
        "!gsutil -m cp -r /content/drive/MyDrive/10\\ Egnyte/ gs://bucket-prospera-datawarehouse-201014-01/01-rawdata/\n",
        "# !gsutil -m cp /content/drive/MyDrive/04\\ Rawdata/04-sakernas/sakernas-2008/data/sakernas0808-dbf-xxx.csv gs://bucket-prospera-datawarehouse-201014-01/01-rawdata/01-bps/04-sakernas/sakernas-2008/data/\n",
        "# !gsutil -m cp gs://prospera-spending-bucket-201115/DataRKASAwal/v_rkas.csv /content/drive/MyDrive/04\\ Rawdata/07-bos/\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujuP2Pyvxl54"
      },
      "source": [
        "# Check the directory and put into a list\n",
        "arrDirectory00 = !ls '/content/drive/My Drive/04 Rawdata'\n",
        "arrDirectory01 = []\n",
        "arrDirectory02 = []\n",
        "\n",
        "\n",
        "for e in arrDirectory00:\n",
        "  print(type(e))\n",
        "  (head, tail) = os.path.splitdrive(e)\n",
        "  print(head)\n",
        "  if not (tail):\n",
        "    arrDirectory01.append(e)\n",
        "\n",
        "for e in arrDirectory01:\n",
        "  arrDirectory00 = !gsutil ls {e}\n",
        "  print(e, arrDirectory00)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIv_s_6y03LH"
      },
      "source": [
        "# Import PyDrive and associated libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "\n",
        "# List .txt files in the root.\n",
        "#\n",
        "# Search query reference:\n",
        "# https://developers.google.com/drive/v2/web/search-parameters\n",
        "listed = drive.ListFile({'q': \"mimeType = 'application/zip' and trashed=false\"}).GetList()\n",
        "# listed = drive.ListFile().GetList()\n",
        "for file in listed:\n",
        "  print('title {}, id {}'.format(file['title'], file['id']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIho8KfFsu8L"
      },
      "source": [
        "# List Rar & Zip Files\n",
        "arrZipFiles = !ls '/content/drive/My Drive/04 Rawdata/07-bos/'*.zip\n",
        "\n",
        "arrZipFiles\n",
        "# print(len(arrZipFiles))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9UgGLJmPxkT"
      },
      "source": [
        "# %%time\n",
        "\n",
        "# Read and write ZIP-format archive files. This module provides tools to create, read, write, append, and list a ZIP file\n",
        "import zipfile\n",
        "\n",
        "# !unzip '/content/drive/My Drive/04 Rawdata/07-bos/v_rkas.zip' -d '/content/drive/My Drive/04 Rawdata/07-bos/'\n",
        "!zip '/content/drive/My Drive/04 Rawdata/07-bos/v_rkas.zip' -d '/content/drive/My Drive/04 Rawdata/07-bos/'\n",
        "\n",
        "\"\"\"\n",
        "for i in range(0,1):\n",
        "  print(arrZipFiles[i])\n",
        "  (head, tail) = os.path.split(arrZipFiles[i])\n",
        "  !unzip '/content/drive/My Drive/04 Rawdata/07-bos/DataRKASAwal.zip' -d '/content/drive/My Drive/04 Rawdata/07-bos/'\n",
        "\"\"\"      \n",
        "print('Process Completed')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVM5GsxK-t4o"
      },
      "source": [
        "!# cd '/content/drive/My Drive/Database/susenas/susenas-2007'\n",
        "workingDirectory = '/content/drive/My Drive'\n",
        "print(workingDirectory)\n",
        "os.chdir(workingDirectory)\n",
        "!ls -d */\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t16ktdNBA3tP"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/gdrive')\n",
        "import glob\n",
        "\n",
        "file_path = glob.glob(\"//content/drive/My Drive/**.zip\")\n",
        "for file in file_path:\n",
        "    print(file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYxQjn9MBRGO"
      },
      "source": [
        "!gsutil ls gs://content/drive\n",
        "# !gsutil ls https://www.googleapis.com/drive/v2/files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6IGDAaw3PR2"
      },
      "source": [
        "## Step 0203 AWS S3 PreProcessing Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17Kn7mvF7EaD"
      },
      "source": [
        "# Miscellaneous operating system interfaces- standard library module\n",
        "import os\n",
        "\n",
        "# Check the directory and put into a list\n",
        "arrDirectory00 = !gsutil ls s3://{awsBucketID}\n",
        "arrDirectory01 = []\n",
        "arrDirectory02 = []\n",
        "\n",
        "\n",
        "for e in arrDirectory00:\n",
        "  (head, tail) = os.path.split(e)\n",
        "  if not (tail):\n",
        "    arrDirectory01.append(e)\n",
        "\n",
        "for e in arrDirectory01:\n",
        "  arrDirectory00 = !gsutil ls {e}\n",
        "  print(e, arrDirectory00)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5dSJE1QlOGS"
      },
      "source": [
        "# Copy data from AWS S3 to Google Drive (Not Used)\n",
        "\n",
        "# Copy data using gsutil cp command\n",
        "!gsutil -m cp s3://{awsBucketID}/01-bps/03-sakernas/sakernas_2003.dta /content/drive/My\\ Drive/04\\ Rawdata/04-sakernas/sakernas-2003"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8S9q4ESNZLb"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELUKwJdENGYE"
      },
      "source": [
        "!ls --help"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-M3FjTTpDjn"
      },
      "source": [
        "# List Rar & Zip Files\n",
        "\n",
        "# !gsutil ls --help\n",
        "arrDirectory = ['01-bps', '02-rand', '03-tnp2k']\n",
        "\n",
        "workingDirdirectory\n",
        "print('01-bps')\n",
        "print('01-bps/01-industri')\n",
        "!gsutil ls -h -l s3://{awsBucketID}/01-bps/01-industri/**.zip\n",
        "!gsutil ls -h -l s3://{awsBucketID}/01-bps/01-industri/**.rar\n",
        "\n",
        "print('01-bps/02-podes')\n",
        "!gsutil ls -h -l s3://{awsBucketID}/01-bps/02-podes/**.zip\n",
        "!gsutil ls -h -l s3://{awsBucketID}/01-bps/02-podes/**.rar\n",
        "\n",
        "print('01-bps/03-sakernas')\n",
        "!gsutil ls -h -l s3://{awsBucketID}/01-bps/03-sakernas/**.zip\n",
        "!gsutil ls -h -l s3://{awsBucketID}/01-bps/03-sakernas/**.rar\n",
        "\n",
        "print('01-bps/04-sensus-ekonomi')\n",
        "!gsutil ls -h -l s3://{awsBucketID}/01-bps/04-sensus-ekonomi/**.zip\n",
        "!gsutil ls -h -l s3://{awsBucketID}/01-bps/04-sensus-ekonomi/**.rar\n",
        "# !gsutil ls s3://{awsBucketID}/01-bps/04-sensus-ekonomi/se-2016/**.zip\n",
        "# !gsutil ls s3://{awsBucketID}/01-bps/04-sensus-ekonomi/se-2016/**.rar\n",
        "\n",
        "print('01-bps/05-susenas')\n",
        "!gsutil ls -h -l s3://{awsBucketID}/01-bps/05-susenas/**.zip\n",
        "!gsutil ls -h -l s3://{awsBucketID}/01-bps/05-susenas/**.rar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPRDCgHSA7Pq"
      },
      "source": [
        "## Step 0204A Unzip Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QadHflrK6ltT"
      },
      "source": [
        "# List Zip Files\n",
        "# arrZipFiles = !gsutil ls s3://{awsBucketID}/01-bps/05-susenas/**.zip\n",
        "arrZipFiles = !gsutil ls gs://{gcpBucketID}/DataRKASAwal/**.zip\n",
        "\n",
        "arrZipFiles\n",
        "# print(len(arrZipFiles))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaBRYYaSeFA_"
      },
      "source": [
        "# %%time\n",
        "\n",
        "# Read and write ZIP-format archive files. This module provides tools to create, read, write, append, and list a ZIP file\n",
        "import zipfile\n",
        "\n",
        "s3_resource = boto3.resource('s3',\n",
        "         aws_access_key_id='AKIAYGZEZFZROUAV2D6K',\n",
        "         aws_secret_access_key='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')\n",
        "\n",
        "for i in range(4,7):\n",
        "  print(arrZipFiles[i])\n",
        "  (head, tail) = os.path.split(arrZipFiles[i])\n",
        "  zip_obj = s3_resource.Object(bucket_name=awsBucketID, key=arrZipFiles[i][16:])\n",
        "  # zip_obj = s3_resource.Object(bucket_name=awsBucketID, key='01-bps/04-sensus-ekonomi/se-2006/Data SE2006 Listing.zip')\n",
        "  buffer = BytesIO(zip_obj.get()[\"Body\"].read())\n",
        "\n",
        "  z = zipfile.ZipFile(buffer)\n",
        "  for filename in z.namelist():\n",
        "      file_info = z.getinfo(filename)\n",
        "      Key=head[16:]+'/'+f'{filename}'\n",
        "      s3_resource.meta.client.upload_fileobj(\n",
        "        z.open(filename),\n",
        "        Bucket=awsBucketID,\n",
        "        Key=head[16:]+'/'+f'{filename}'\n",
        "      )\n",
        "      print(file_info, Key)\n",
        "\n",
        "print('Process Completed')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTB9vbaGx9bN"
      },
      "source": [
        "# %%time\n",
        "\n",
        "# Read and write ZIP-format archive files. This module provides tools to create, read, write, append, and list a ZIP file\n",
        "import io\n",
        "import zipfile\n",
        "from google.cloud import storage\n",
        "\n",
        "storage_client = storage.Client(project=gcpProjectID)\n",
        "bucket = storage_client.get_bucket(gcpBucketID)\n",
        "# all_blobs = list(bucket.list_blobs())\n",
        "\n",
        "filesource = 'DataRKASAwal/v_rkas.zip'\n",
        "(head, tail) = os.path.split(filesource)\n",
        "zip_blob = bucket.blob(filesource)\n",
        "# zip_bytes = BytesIO(zip_blob.get()[\"Body\"].read())\n",
        "zip_bytes = io.BytesIO(zip_blob.download_as_string())\n",
        "\n",
        "z = zipfile.ZipFile(zip_bytes)\n",
        "for filename in z.namelist():\n",
        "      file_info = z.getinfo(filename)\n",
        "      file_content = z.read(filename)\n",
        "      blob = bucket.blob(head + \"/\" + filename)\n",
        "      blob.upload_from_string(file_content)\n",
        "      print(file_info)            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IO0iNV-08xCI"
      },
      "source": [
        "from google.cloud import storage\n",
        "from zipfile import ZipFile\n",
        "from zipfile import is_zipfile\n",
        "import io\n",
        "\n",
        "def zipextract(bucketname, zipfilename_with_path):\n",
        "\n",
        "    storage_client = storage.Client(project=gcpProjectID)\n",
        "    bucket = storage_client.get_bucket(bucketname)\n",
        "\n",
        "    destination_blob_pathname = zipfilename_with_path\n",
        "\n",
        "    blob = bucket.blob(destination_blob_pathname)\n",
        "    zipbytes = io.BytesIO(blob.download_as_string())\n",
        "\n",
        "    if is_zipfile(zipbytes):\n",
        "        with ZipFile(zipbytes, 'r') as myzip:\n",
        "            for contentfilename in myzip.namelist():\n",
        "                contentfile = myzip.read(contentfilename)\n",
        "                blob = bucket.blob(zipfilename_with_path + \"/\" + contentfilename)\n",
        "                blob.upload_from_string(contentfile)\n",
        "\n",
        "zipextract('prospera-spending-bucket-201115', 'DataRKASAwal/v_rkas.zip') # if the file is gs://mybucket/path/file.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39R1bZExycum"
      },
      "source": [
        "(head, tail) = os.path.split(filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2AlcMi_y0Nc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3KL1UKpgAzf"
      },
      "source": [
        "for i in range(4,7):\n",
        "  print(arrZipFiles[i])\n",
        "\n",
        "  '''\n",
        "    (head, tail) = os.path.split(arrZipFiles[i])\n",
        "    print(head[16:])\n",
        "    print(tail)\n",
        "    (drive, tail) = os.path.splitext(arrZipFiles[i])\n",
        "    print(drive)\n",
        "    print(tail)\n",
        "    print(os.path.dirname(head))\n",
        "    print(arrZipFiles[i][16:])\n",
        "  '''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yS8GP9L_sLeq"
      },
      "source": [
        "# Copy data from AWS S3 to Google Drive (Not Used)\n",
        "\n",
        "# Copy data using gsutil cp command\n",
        "# !gsutil -m cp s3://{awsBucketID}/01-bps/04-sensus-ekonomi/se-2006/sensus_ekonomi_2006__listing_1/L1.zip /content/drive/My\\ Drive/07\\ Google\\ Colab\\"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-oCvtIW5L2A"
      },
      "source": [
        "# %%time\n",
        "\n",
        "# Read and write ZIP-format archive files. This module provides tools to create, read, write, append, and list a ZIP file\n",
        "import zipfile\n",
        "\n",
        "s3_resource = boto3.resource('s3',\n",
        "         aws_access_key_id='AKIAYGZEZFZROUAV2D6K',\n",
        "         aws_secret_access_key='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')\n",
        "\n",
        "for i in range(4,7):\n",
        "  print(arrZipFiles[i])\n",
        "  (head, tail) = os.path.split(arrZipFiles[i])\n",
        "  zip_obj = s3_resource.Object(bucket_name=awsBucketID, key=arrZipFiles[i][16:])\n",
        "  # zip_obj = s3_resource.Object(bucket_name=awsBucketID, key='01-bps/04-sensus-ekonomi/se-2006/Data SE2006 Listing.zip')\n",
        "  buffer = BytesIO(zip_obj.get()[\"Body\"].read())\n",
        "\n",
        "  z = zipfile.ZipFile(buffer)\n",
        "  for filename in z.namelist():\n",
        "      file_info = z.getinfo(filename)\n",
        "      Key=head[16:]+'/'+f'{filename}'\n",
        "      s3_resource.meta.client.upload_fileobj(\n",
        "        z.open(filename),\n",
        "        Bucket=awsBucketID,\n",
        "        Key=head[16:]+'/'+f'{filename}'\n",
        "      )\n",
        "      print(file_info, Key)\n",
        "\n",
        "print('Process Completed')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LXeYjjz4i0I"
      },
      "source": [
        "head[16:]+'/'+f'{filename}'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bw7uwR0VrQ67"
      },
      "source": [
        "## Step 0204B Unrar Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqoMOw2Xra2U"
      },
      "source": [
        "# List Rar Files\n",
        "arrRarFiles = !gsutil ls s3://{awsBucketID}/01-bps/04-sensus-ekonomi/se-2006/**.rar\n",
        "\n",
        "print(arrRarFiles)\n",
        "print(len(arrRarFiles))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiTMMkPdrkaE"
      },
      "source": [
        "for i in range(len(arrRarFiles)):\n",
        "  print(arrRarFiles[i])\n",
        "\n",
        "  (head, tail) = os.path.split(arrRarFiles[i])\n",
        "  print(head[16:])\n",
        "  print(tail)\n",
        "  \n",
        "  (drive, tail) = os.path.splitext(arrRarFiles[i])\n",
        "  print(drive)\n",
        "  print(tail)\n",
        "  \n",
        "  print(os.path.dirname(head))\n",
        "  print(arrRarFiles[i][16:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Fy05hFpPgJm"
      },
      "source": [
        "# Copy data from AWS S3 to Google Drive (Not Used)\n",
        "\n",
        "# Copy data using gsutil cp command\n",
        "!gsutil -m cp s3://{awsBucketID}/01-bps/04-sensus-ekonomi/se-2006/sensus_ekonomi_2006__listing_1/L1.rar /content/drive/My\\ Drive/07\\ Google\\ Colab\\"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vcy97osrHX8o"
      },
      "source": [
        "# %%time\n",
        "\n",
        "# RAR archive reader for Python\n",
        "!pip install rarfile\n",
        "import rarfile\n",
        "\n",
        "# Wrapper for UnRAR library, ctypes-based\n",
        "# !pip install unrar\n",
        "# from unrar import rarfile\n",
        "\n",
        "s3_resource = boto3.resource('s3',\n",
        "         aws_access_key_id='AKIAYGZEZFZROUAV2D6K',\n",
        "         aws_secret_access_key='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')\n",
        "\n",
        "for i in range(len(arrRarFiles)):\n",
        "  print(arrRarFiles[i])\n",
        "  (head, tail) = os.path.split(arrRarFiles[i])\n",
        "  rar_obj = s3_resource.Object(bucket_name=awsBucketID, key=arrRarFiles[0][16:])\n",
        "  # rar_obj = s3_resource.Object(bucket_name=awsBucketID, key='01-bps/04-sensus-ekonomi/se-2006/sensus_ekonomi_2006__listing_1/L1.rar')\n",
        "\n",
        "  buffer = BytesIO(rar_obj.get()[\"Body\"].read())\n",
        "  z = rarfile.RarFile(buffer)\n",
        "  for filename in z.namelist():\n",
        "      file_info = z.getinfo(filename)\n",
        "      Key=head[16:]+'/'+f'{filename}'\n",
        "      s3_resource.meta.client.upload_fileobj(\n",
        "        z.open(filename),\n",
        "        Bucket=awsBucketID,\n",
        "        Key=head[16:]+'/L1-rar/'+f'{filename}'\n",
        "      )\n",
        "      print(file_info, Key)\n",
        "\n",
        "print('Process Completed')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5rfZx1wm-zy"
      },
      "source": [
        "head[16:]\n",
        "filename"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xbxq6SOqa86u"
      },
      "source": [
        "### List File within working Directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4v-wmgalSZC"
      },
      "source": [
        "%%time\n",
        "# Set working directory 01\n",
        "workingDirectory = dictDirectory['sakernas']\n",
        "# workingDirectory = dictDirectory['sakernas'] + '/data'\n",
        "os.chdir(workingDirectory)\n",
        "path = !pwd\n",
        "print(path)\n",
        "\n",
        "# List file on working directory 02\n",
        "listFile = [f for f in glob.glob('*.csv')]\n",
        "# listFile = [f for f in glob.glob('*.dta')]\n",
        "# listFile = [f for f in glob.glob('*.*')]\n",
        "\n",
        "# Prints all files within directory 03\n",
        "loop = 0\n",
        "for e in listFile:\n",
        "  print(e)\n",
        "  loop += 1\n",
        "\n",
        "print(loop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAFdTPM7nXzG"
      },
      "source": [
        "# Copy data from Google Cloud Storage into AWS S3\n",
        "bucketName = 'bucket-prospera-01'\n",
        "bucketDirectory = dictGCSDirectory['se-2016-direktori']\n",
        "\n",
        "# !gsutil ls s3://{bucketDirectory}\n",
        "\n",
        "# Delete data using gsutil cp command\n",
        "# !gsutil rm gs://{bucketName}/01-rawdata/01-bps/04-sensus-ekonomi/se-2016/se-2016-umb-jk/**\n",
        "\n",
        "# Copy data using gsutil cp command\n",
        "# !gsutil -m cp -r /content/drive/My\\ Drive/Database/se-2016-umb-keuangan/data/* gs://{bucketName}/01-rawdata/01-bps/04-sensus-ekonomi/se-2016/se-2016-umb-jk/data\n",
        "\n",
        "# !gsutil cp /content/drive/My\\ Drive/Database/se-2016-listing/data/se2016-listing-merge.csv gs://{bucketName}/01-rawdata/01-bps/04-sensus-ekonomi/se-2016/se-2016-listing/data\n",
        "# !gsutil cp /content/drive/My\\ Drive/Database/se-2016-listing/data/se2016-listing-33-convert.csv gs://{bucketName}/01-rawdata/01-bps/04-sensus-ekonomi/se-2016/se-2016-listing/data\n",
        "# !gsutil -m cp -r /content/drive/My\\ Drive/Data/* gs://bucket-prospera-01/01-rawdata/01-bps/04-sensus-ekonomi/se-2016/se-2016-listing/data\n",
        "\n",
        "# Copy data using gsutil rsync command exclude directories\n",
        "# !gsutil rsync -d /content/drive/My\\ Drive/Database/se-2016-umb-nonkeuangan/ gs://{bucketName}/01-rawdata/01-bps/04-sensus-ekonomi/se-2016/se-2016-umb-nonkeuangan/\n",
        "# !gsutil rsync -d /content/drive/My\\ Drive/Database/se-2016-umb-produksi/ gs://{bucketDirectory}\n",
        "\n",
        "# Copy data using gsutil rsync command include directories\n",
        "!gsutil rsync -d -r gs://{bucketDirectory} s3://{bucketDirectory}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2MrpDWrCXeN"
      },
      "source": [
        "### Google Big Query Command"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sbtq21f9cg8"
      },
      "source": [
        "# Set working directory on Google Big Query 01\n",
        "projectId = 'datawarehouse-001'\n",
        "directoryBQ = ['datawarehouse-001:04_sensus_ekonomi', 'datawarehouse-001:04_sensus_ekonomi']\n",
        "\n",
        "!gcloud config set project {projectID}\n",
        "# List file on Google Big Query working directory 02\n",
        "# !bq show datawarehouse-001:04_sensus_ekonomi.se_2016_listing_merge\n",
        "!bq ls  {directoryBQ[0]}\n",
        "#  !bq ls --max_results=1000 {directoryBQ[0]}\n",
        "# !bq rm --help"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DJ701ULOU8m"
      },
      "source": [
        "Big Query Delete Table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QywdOf7L7QJ"
      },
      "source": [
        "%%time\n",
        "# Big Query delete table se2016-listing\n",
        "listBQFile = [\n",
        "  'se_2016_listing_11', 'se_2016_listing_12', 'se_2016_listing_13', 'se_2016_listing_14', 'se_2016_listing_15', \n",
        "  'se_2016_listing_16', 'se_2016_listing_17', 'se_2016_listing_18', 'se_2016_listing_19', 'se_2016_listing_21', \n",
        "  'se_2016_listing_31', 'se_2016_listing_32', 'se_2016_listing_33', 'se_2016_listing_34', 'se_2016_listing_35', \n",
        "  'se_2016_listing_36', 'se_2016_listing_51', 'se_2016_listing_52', 'se_2016_listing_53', 'se_2016_listing_61', \n",
        "  'se_2016_listing_62', 'se_2016_listing_63', 'se_2016_listing_64', 'se_2016_listing_65', 'se_2016_listing_71', \n",
        "  'se_2016_listing_72', 'se_2016_listing_73', 'se_2016_listing_74', 'se_2016_listing_75', 'se_2016_listing_76', \n",
        "  'se_2016_listing_81', 'se_2016_listing_82', 'se_2016_listing_91', 'se_2016_listing_94', 'se_2016_listing_merge' \n",
        "]\n",
        "\n",
        "# Big Query delete table se2016-direktori\n",
        "listBQFile = [\n",
        "]\n",
        "\n",
        "# Big Query delete table se2016-umk\n",
        "listBQFile = [\n",
        "]\n",
        "\n",
        "# Big Query delete table se2016-umb-jk\n",
        "listBQFile = [\n",
        "]\n",
        "\n",
        "# Big Query delete table se2016-umb-jnk\n",
        "listBQFile = [\n",
        "]\n",
        "\n",
        "# Big Query delete table se2016-umb-sp\n",
        "listBQFile = [\n",
        "]\n",
        "\n",
        "# Set working directory on Google Big Query 01\n",
        "projectId = 'datawarehouse-001'\n",
        "directoryBQ = ['datawarehouse-001:03_sakernas']\n",
        "\n",
        "# List file on Google Big Query working directory 02\n",
        "# !bq ls --max_results=1000 {directoryBQ[0]}\n",
        "\n",
        "# !bq rm --help\n",
        "\n",
        "loop = 0\n",
        "for e in listBQFile:\n",
        "  bqFileName = directoryBQ[0] + \".\" + e\n",
        "  !bq rm -f -t {bqFileName}\n",
        "  # print(\"delete\", e)\n",
        "  print(\"delete\", bqFileName)\n",
        "  loop += 1\n",
        "\n",
        "print(loop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4bk_H_tPCaN"
      },
      "source": [
        "Big Query Create Table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T__9kwZ-5c7Z"
      },
      "source": [
        "# Upload file on working directory to Google Big Query\n",
        "\n",
        "# Set Working Diretory 01\n",
        "workingDirectory = dictDirectory['sakernas']\n",
        "\n",
        "!bq load \\\n",
        "    --source_format=CSV \\\n",
        "    --skip_leading_rows=1 \\\n",
        "    datawarehouse-001:04_sensus_ekonomi.se_2016_umb_jk_02_21 \\\n",
        "    gs://bucket-prospera-01/01-rawdata/01-bps/04-sensus-ekonomi/se-2016/se-2016-umb-jk/data/data-01/se2016-umb-jk-01-21.csv \\\n",
        "    ./se2016-umb-jka-layout.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8yDeMxHPN_D"
      },
      "source": [
        "%%time\n",
        "# Big Query create table susenas\n",
        "listBQFile = [\n",
        "  'susenas00_ki', 'susenas00-ki.csv', 'susenas00-ki-layout-prospera.json', 'susenas00_kr', 'susenas00-kr.csv', 'susenas00-kr-layout-prospera.json', 'susenas00_kna', 'susenas00-kna.csv', 'susenas00-kna-layout-prospera.json'\n",
        "]\n",
        "\n",
        "# Set working directory 01\n",
        "workingDirectory = dictDirectory['susenas-2000']\n",
        "os.chdir(workingDirectory)\n",
        "path = !pwd\n",
        "\n",
        "pathData = 'gs://bucket-prospera-01/01-rawdata/01-bps/05-susenas/susenas-2000/data/'\n",
        "fileLayout = ''\n",
        "\n",
        "loop = 0\n",
        "for i in range (0, len(listBQFile), 3):\n",
        "  pathSource = pathData + listBQFile[i+1]\n",
        "  fileLayout = listBQFile[i+2]\n",
        "  pathDestination = 'datawarehouse-001:05_susenas.' + listBQFile[i]\n",
        "\n",
        "  # Upload file on working directory to Google Big Query\n",
        "  !bq load \\\n",
        "    --source_format=CSV \\\n",
        "    --skip_leading_rows=1 \\\n",
        "    --replace=True \\\n",
        "    {pathDestination} \\\n",
        "    {pathSource} \\\n",
        "    ./{fileLayout}\n",
        "\n",
        "  print(pathDestination, pathSource, fileLayout)\n",
        "  loop += 1\n",
        "\n",
        "print(loop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCsx8TVEB4Ln"
      },
      "source": [
        "%%time\n",
        "# Big Query create table\n",
        "listBQFile = [\n",
        "\t'sakernas_1994', 'sakernas_1994.csv',\n",
        "\t'sakernas_1995', 'sakernas_1995.csv',\n",
        "\t'sakernas_1996', 'sakernas_1996.csv',\n",
        "\t'sakernas_1997', 'sakernas_1997.csv',\n",
        "\t'sakernas_1998', 'sakernas_1998.csv',\n",
        "\t'sakernas_1999', 'sakernas_1999.csv',\n",
        "\t'sakernas_2000', 'sakernas_2000.csv',\n",
        "\t'sakernas_2001', 'sakernas_2001.csv',\n",
        "\t'sakernas_2002', 'sakernas_2002.csv',\n",
        "\t'sakernas_2003', 'sakernas_2003.csv',\n",
        "\t'sakernas_2004', 'sakernas_2004.csv',\n",
        "\t'sakernas_2005nov', 'sakernas_2005nov.csv',\n",
        "\t'sakernas_2006aug', 'sakernas_2006aug.csv',\n",
        "\t'sakernas_2007aug', 'sakernas_2007aug.csv',\n",
        "\t'sakernas_2007feb', 'sakernas_2007feb.csv',\n",
        "\t'sakernas_2008aug', 'sakernas_2008aug.csv',\n",
        "\t'sakernas_2008feb', 'sakernas_2008feb.csv',\n",
        "\t'sakernas_2009aug', 'sakernas_2009aug.csv',\n",
        "\t'sakernas_2009feb', 'sakernas_2009feb.csv',\n",
        "\t'sakernas_2010aug', 'sakernas_2010aug.csv',\n",
        "\t'sakernas_2010feb', 'sakernas_2010feb.csv',\n",
        "\t'sakernas_2011aug_rev', 'sakernas_2011aug_rev.csv',\n",
        "\t'sakernas_2011feb', 'sakernas_2011feb.csv',\n",
        "\t'sakernas_2012aug_rev', 'sakernas_2012aug_rev.csv',\n",
        "\t'sakernas_2012feb', 'sakernas_2012feb.csv',\n",
        "\t'sakernas_2013aug_rev', 'sakernas_2013aug_rev.csv',\n",
        "\t'sakernas_2013feb', 'sakernas_2013feb.csv',\n",
        "\t'sakernas_2014aug', 'sakernas_2014aug.csv',\n",
        "\t'sakernas_2014feb', 'sakernas_2014feb.csv',\n",
        "\t'sakernas_2015aug', 'sakernas_2015aug.csv',\n",
        "\t'sakernas_2015feb', 'sakernas_2015feb.csv',\n",
        "\t'sakernas_2016aug', 'sakernas_2016aug.csv',\n",
        "\t'sakernas_2016feb', 'sakernas_2016feb.csv',\n",
        "\t'sakernas_2017aug', 'sakernas_2017aug.csv',\n",
        "\t'sakernas_2017feb', 'sakernas_2017feb.csv',\n",
        "\t'sakernas_2018aug', 'sakernas_2018aug.csv',\n",
        "\t'sakernas_2018feb', 'sakernas_2018feb.csv',\n",
        "\t'sakernas_2019aug', 'sakernas_2019aug.csv'\n",
        "]\n",
        "\n",
        "# Set working directory 01\n",
        "os.chdir(workingDirectory)\n",
        "path = !pwd\n",
        "\n",
        "loop = 0\n",
        "for i in range (0, 76, 2):\n",
        "  pathData = 'gs://bucket-prospera-01/01-rawdata/01-bps/03-sakernas/'\n",
        "\n",
        "  if listBQFile[i] == 'se2016_umb_jk_01_11':\n",
        "    pathData = 'gs://bucket-prospera-01/01-rawdata/01-bps/04-sensus-ekonomi/se-2016/se-2016-umb-jk/data/data-01/'\n",
        "    # fileLayout = 'se2016-umb-jka-layout.json'\n",
        "  elif listBQFile[i] == 'se2016_umb_jk_02_11':\n",
        "    pathData = 'gs://bucket-prospera-01/01-rawdata/01-bps/04-sensus-ekonomi/se-2016/se-2016-umb-jk/data/data-02/'\n",
        "    # fileLayout = 'se2016-umb-jkb-layout.json'\n",
        "  elif listBQFile[i] == 'se2016_umb_jk_03_11':\n",
        "    pathData = 'gs://bucket-prospera-01/01-rawdata/01-bps/04-sensus-ekonomi/se-2016/se-2016-umb-jk/data/data-03/'\n",
        "    # fileLayout = 'se2016-umb-jkc-layout.json'\n",
        "  elif listBQFile[i] == 'se2016_umb_jk_11':\n",
        "    pathData = 'gs://bucket-prospera-01/01-rawdata/01-bps/04-sensus-ekonomi/se-2016/se-2016-umb-jk/data/'\n",
        "    # fileLayout = 'se2016-umb-jk-layout.json'\n",
        "\n",
        "  pathSource = pathData + listBQFile[i+1]\n",
        "  pathDestination = 'datawarehouse-001:03_sakernas.' + listBQFile[i]\n",
        "\n",
        "  # Upload file on working directory to Google Big Query\n",
        "  !bq load \\\n",
        "      --autodetect \\\n",
        "      --source_format=CSV \\\n",
        "      --skip_leading_rows=1 \\\n",
        "      {pathDestination} \\\n",
        "      {pathSource}      # \\\n",
        "      #./{fileLayout}\n",
        "\n",
        "  print(listBQFile[i])\n",
        "  loop += 1\n",
        "\n",
        "print(loop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ha6JKL57cCs8"
      },
      "source": [
        "## Step 0205 Standardize File Names"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8juUBz3NDaKD"
      },
      "source": [
        "!ls {dict'susenas-1979'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuNnCdMDADXc"
      },
      "source": [
        "%%time\n",
        "dictFile = {\n",
        "  'susenas-2000': {'source': 'susenas-2000-', 'dest': 'susenas00-'},\n",
        "  'susenas-2001': {'source': 'susenas-2001-', 'dest': 'susenas01-'},\n",
        "  'susenas-2002': {'source': 'susenas-2002-', 'dest': 'susenas02-'},\n",
        "  'susenas-2003': {'source': 'se-2016-umk-', 'dest': 'susenas-umk-'},\n",
        "  'susenas-2004': {'source': '_data1_umk_v1', 'dest': 'susenas-umk-01-'},\n",
        "  'susenas-2005': {'source': '_data2_umk_v1', 'dest': 'susenas-umk-02-'},\n",
        "  'susenas-2006': {'source': 'se-2016-umb-jk', 'dest': 'susenas-umb-jk'},\n",
        "  'susenas-2007': {'source': '_data1_umb-jk_v1', 'dest': 'susenas-umb-jk-01-'},\n",
        "  'susenas-2008': {'source': '_data2_umb-jk_v1', 'dest': 'susenas-umb-jk-02-'},\n",
        "  'susenas-2009': {'source': '_data3_umb-jk_v1', 'dest': 'susenas-umb-jk-03-'},\n",
        "  'susenas-2010': {'source': 'se-2016-umb-jnk', 'dest': 'susenas-umb-jnk'},\n",
        "  'susenas-2011': {'source': '_data1_umb-jnk_v1', 'dest': 'susenas-umb-jnk-01-'},\n",
        "  'susenas-2012': {'source': '_data2_umb-jnk_v1', 'dest': 'susenas-umb-jnk-02-'},\n",
        "  'susenas-2013': {'source': '_data3_umb-jnk_v1', 'dest': 'susenas-umb-jnk-03-'},\n",
        "  'susenas-2014': {'source': 'se-2016-umb-sp', 'dest': 'susenas-umb-sp'},\n",
        "  'susenas-2015': {'source': '_data1_umb-sp_v1', 'dest': 'susenas-umb-sp-01-'},\n",
        "  'susenas-2016': {'source': '_data2_umb-sp_v1', 'dest': 'susenas-umb-sp-02-'},\n",
        "  'susenas-2017': {'source': '_data3_umb-sp_v1', 'dest': 'susenas-umb-sp-03-'},\n",
        "  'susenas-2018': {'source': '_data3_umb-sp_v1', 'dest': 'susenas-umb-sp-03-'}\n",
        "}\n",
        "\n",
        "# Set working directory 01\n",
        "workingDirectory = dictDirectory['susenas-2002']\n",
        "os.chdir(workingDirectory)\n",
        "path = !pwd\n",
        "\n",
        "# List dbf file within directory 02\n",
        "listFile = [f for f in glob.glob('*.*')]\n",
        "fileSource = dictFile['susenas-2002']['source']\n",
        "fileDestination = dictFile['susenas-2002']['dest']\n",
        "\n",
        "# Rename dbf file within directory 03\n",
        "loop = 0\n",
        "for e in listFile:\n",
        "  pathSource = path[0] + '/' + e\n",
        "  pathDestination = path[0] + '/' + e\n",
        "  # pathDestination = path[0] + '/' + fileDestination + e\n",
        "  # print('renaming ' + pathSource)\n",
        "  fname,ext = os.path.splitext(pathDestination)\n",
        "  fname = fname.replace(fileSource,fileDestination)\n",
        "\n",
        "  os.rename(pathSource, fname + ext)\n",
        "  print(e, fname)\n",
        "  loop += 1\n",
        "\n",
        "print(loop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJblK3UpIePl"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pv1lRMoWISzy"
      },
      "source": [
        "# Set working directory 01\n",
        "workingDirectory = dictDirectory['susenas-2002']\n",
        "os.chdir(workingDirectory)\n",
        "path = !pwd\n",
        "\n",
        "# List dbf file within directory 02\n",
        "listFile = [f for f in glob.glob('*.*')]\n",
        "fileSource = dictFile['susenas-2002']['source']\n",
        "fileDestination = dictFile['susenas-2002']['dest']\n",
        "\n",
        "# Rename dbf file within directory 03\n",
        "loop = 0\n",
        "for e in listFile:\n",
        "  pathSource = path[0] + '/' + e\n",
        "  pathDestination = path[0] + '/' + e\n",
        "  # pathDestination = path[0] + '/' + fileDestination + e\n",
        "  # print('renaming ' + pathSource)\n",
        "  fname,ext = os.path.splitext(pathDestination)\n",
        "  fname = fname.replace(fileSource,fileDestination)\n",
        "\n",
        "  os.rename(pathSource, fname + ext)\n",
        "  print(e, fname)\n",
        "  loop += 1\n",
        "\n",
        "print(loop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_cUm_tdcRdb"
      },
      "source": [
        "## Step 0206A Convert File from stata into csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRd5IauKsBwG"
      },
      "source": [
        "%%time\n",
        "# Unix style pathname pattern expansion. The glob module finds all the pathnames matching a specified pattern according to the rules used by the Unix shell, although results are returned in arbitrary order\n",
        "# import glob\n",
        "\n",
        "# Miscellaneous operating system interfaces\n",
        "import os\n",
        "\n",
        "# Pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool,\n",
        "# built on top of the Python programming language\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# List dbf file within directory 01\n",
        "# listFile = [f for f in glob.glob('*.dta')]\n",
        "listFile = !ls {dictDrive[\"sakernas-2012\"]}/data/*.dta\n",
        "# listFile = ['/content/drive/MyDrive/04 Rawdata/04-sakernas/sakernas-2002/data/sakernas02.dta']\n",
        "# listFile = !ls {dictDrive[\"susenas-2007\"]}/data\\ tnp2k/*.dta\n",
        "# listFile = ['/content/drive/MyDrive/04 Rawdata/06-susenas/susenas-2007/data tnp2k/susenas07panel-modul-blokq-egnyte.dta']\n",
        "\n",
        "# Convert dta file into csv 02\n",
        "loop = 0\n",
        "for e in listFile:\n",
        "  # fileSource = path[0] + '/' + e\n",
        "  fileSource = e.replace(\"'\",\"\")\n",
        "  fname,ext = os.path.splitext(fileSource)\n",
        "  fileDestination = fname + '-dta.csv'\n",
        "\n",
        "  print(fileSource, type(fileSource))\n",
        "  \n",
        "  # Convert file from stata into csv\n",
        "  dfStata = pd.io.stata.read_stata(fileSource, convert_categoricals=False)\n",
        "  dfStata.to_csv(fileDestination, encoding='utf-8', index=False)\n",
        "\n",
        "  # Read and import csv file dataset into pandas data frame, change paths if needed\n",
        "  dfBPSData = pd.read_csv(fileDestination)\n",
        "\n",
        "  print(\"dfBPSData.shape :\", dfBPSData.shape)\n",
        "  print(\"type(dfBPSData) :\", type(dfBPSData))\n",
        "  print(\"converting\", e, \"to\", fileDestination)\n",
        "  loop += 1\n",
        "\n",
        "print(\"Number of converted files: \",loop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3axcyRV6gzYg"
      },
      "source": [
        "%%time\n",
        "# Unix style pathname pattern expansion. The glob module finds all the pathnames matching a specified pattern according to the rules used by the Unix shell, although results are returned in arbitrary order\n",
        "import glob\n",
        "\n",
        "# Pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool,\n",
        "# built on top of the Python programming language\n",
        "import pandas as pd\n",
        "\n",
        "listFile = [f for f in glob.glob('*.dta')]\n",
        "# listFile = ['ind96a.dta', 'ind96b.dta']\n",
        "\n",
        "path = !pwd\n",
        "loop = 0\n",
        "for e in listFile:\n",
        "  pathSource = path[0] + '/' + e\n",
        "  fname,ext = os.path.splitext(pathSource)\n",
        "  pathDestination = fname + '.csv'\n",
        "  print(\"converting\", e, \"to\", pathDestination)\n",
        "  \n",
        "  # Convert file from stata into csv\n",
        "  dfStata = pd.io.stata.read_stata(pathSource, convert_categoricals=False)\n",
        "  dfStata.to_csv(pathDestination, encoding='utf-8', index=False)\n",
        "\n",
        "  # Read and import csv file dataset into pandas data frame, change paths if needed\n",
        "  dfBPSData = pd.read_csv(pathDestination)\n",
        "\n",
        "  print(\"dfBPSData.shape :\", dfBPSData.shape)\n",
        "  print(\"type(dfBPSData) :\", type(dfBPSData))\n",
        "  print(\"converting\", e, \"to\", pathDestination)\n",
        "  loop += 1\n",
        "\n",
        "print(loop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnxjYhKyPR88"
      },
      "source": [
        "## Step 0206B Convert File from dbf into csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTMFw5hBGCAm"
      },
      "source": [
        "### 0206B 01 Convert file dbf to csv using dbfread"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHZFsnmFcvrF"
      },
      "source": [
        "!ls {dictDrive[\"sakernas-2003\"]}/data/\n",
        "# !ls {dictDrive[\"susenas-2007\"]}/data\\ tnp2k/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnQrS9xMHGzB"
      },
      "source": [
        "listFile = !ls {dictDrive[\"sakernas-2008\"]}/data/*.dbf\n",
        "# listFile = !ls {dictDrive[\"susenas-2007\"]}/data\\ tnp2k/*.dbf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fhtfbn_7HIaq"
      },
      "source": [
        "listFile\n",
        "# listFile = listFile[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6_uO0QuISPB"
      },
      "source": [
        "workingDirectory = dictDrive[\"susenas-2007\"]\n",
        "print(workingDirectory)\n",
        "bebek = workingDirectory.replace(\"\\\\\",\"\")\n",
        "print(bebek)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKq9DhBUWtTq"
      },
      "source": [
        "# %%time\n",
        "# Unix style pathname pattern expansion. The glob module finds all the pathnames matching a specified pattern according to the rules used by the Unix shell, although results are returned in arbitrary order\n",
        "import glob\n",
        "\n",
        "# Pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool,\n",
        "# built on top of the Python programming language\n",
        "import pandas as pd\n",
        "\n",
        "# Miscellaneous operating system interfaces. This module provides a portable way of using operating system dependent functionality\n",
        "import os\n",
        "\n",
        "# import sys\n",
        "import csv\n",
        "!pip install dbfread\n",
        "from dbfread import DBF\n",
        "from dbfread import FieldParser\n",
        "\n",
        "class MyFieldParser(FieldParser):\n",
        "    def parseN(self, field, data):\n",
        "        data = data.strip().strip(b'*\\x00')  # Had to strip out the other characters first before \\x00, as per super function specs.\n",
        "        return super(MyFieldParser, self).parseN(field, data)\n",
        "\n",
        "    def parseD(self, field, data):\n",
        "        data = data.strip(b'\\x00')\n",
        "        return super(MyFieldParser, self).parseD(field, data)\n",
        "\n",
        "# Set working directory 01\n",
        "\"\"\"\n",
        "workingDirectory = dictDrive[\"susenas-2007\"]+\"/data tnp2k/\"\n",
        "workingDirectory = workingDirectory.replace(\"\\\\\",\"\")\n",
        "print(type(workingDirectory))\n",
        "os.chdir(workingDirectory)\n",
        "path = !pwd\n",
        "print(path)\n",
        "!ls -1\n",
        "\"\"\"\n",
        "# List dbf file within directory 02\n",
        "# listFile = [f for f in glob.glob('*.dbf')]\n",
        "listFile = !ls {dictDrive[\"sakernas-2011\"]}/data/*.dbf\n",
        "# listFile = !ls {dictDrive[\"susenas-2007\"]}/data\\ tnp2k/*.dbf\n",
        "# listFile = ['/content/drive/MyDrive/04 Rawdata/06-susenas/susenas-2007/data tnp2k/susenas07panel-modul-blokvl-egnyte.dbf']\n",
        "# Convert dbf file into csv 03\n",
        "loop = 0\n",
        "for e in listFile:\n",
        "  # fileSource = path[0] + '/' + e\n",
        "  fileSource = e.replace(\"'\",\"\")\n",
        "  fname,ext = os.path.splitext(fileSource)\n",
        "  fileDestination = fname + '-dbf.csv'\n",
        "\n",
        "  print(fileSource, type(fileSource))\n",
        "  \n",
        "  # Convert file from dbf into csv (using dbfread)\n",
        "  table = DBF(fileSource, parserclass=MyFieldParser)     # table variable is a DBF object\n",
        "  print(table.header)\n",
        "  # print(table.field_names)\n",
        "  # print(table.fields)\n",
        "  \n",
        "  with open(fileDestination, 'w', newline = '') as f:               # create a csv file, fill it with dbf content\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(table.field_names)\n",
        "    for record in table:\n",
        "      # print (record.values())\n",
        "      writer.writerow(list(record.values()))\n",
        "\n",
        "    # for i, record in enumerate(table):\n",
        "      # print('bebek')\n",
        "      # writer.writerow(list(record.values()))\n",
        "\n",
        "  # Read and import csv file dataset into pandas data frame, change paths if needed\n",
        "  dfBPSData = pd.read_csv(fileDestination)\n",
        "\n",
        "  print(fileDestination, fname)\n",
        "  print(\"dfBPSData.shape :\", dfBPSData.shape)\n",
        "  # print(\"type(dfBPSData) :\", type(dfBPSData))\n",
        "  \n",
        "  # print(\"converting\", e, \"to\", fileDestination)\n",
        "  loop += 1\n",
        "\n",
        "print(\"Number of converted files: \",loop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjYPi76DIYyf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPoyn9zzLH4m"
      },
      "source": [
        "print(workingDirectory)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boCXFJ8aPLbE"
      },
      "source": [
        "# %%time\n",
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "# import sys\n",
        "import csv\n",
        "from dbfread import DBF\n",
        "from dbfread import FieldParser\n",
        "\n",
        "class MyFieldParser(FieldParser):\n",
        "    def parseN(self, field, data):\n",
        "        data = data.strip().strip(b'*\\x00')  # Had to strip out the other characters first before \\x00, as per super function specs.\n",
        "        return super(MyFieldParser, self).parseN(field, data)\n",
        "\n",
        "    def parseD(self, field, data):\n",
        "        data = data.strip(b'\\x00')\n",
        "        return super(MyFieldParser, self).parseD(field, data)\n",
        "\n",
        "# Set working directory 01\n",
        "\"\"\"\n",
        "workingDirectory = dictDrive[\"susenas-2007\"]+\"/data tnp2k/\"\n",
        "workingDirectory = workingDirectory.replace(\"\\\\\",\"\")\n",
        "print(type(workingDirectory))\n",
        "os.chdir(workingDirectory)\n",
        "path = !pwd\n",
        "print(path)\n",
        "!ls -1\n",
        "\"\"\"\n",
        "# List dbf file within directory 02\n",
        "# listFile = [f for f in glob.glob('*.dbf')]\n",
        "listFile = !ls {dictDrive[\"susenas-2007\"]}/data\\ tnp2k/*.dbf\n",
        "listFile = ['/content/drive/MyDrive/04 Rawdata/06-susenas/susenas-2007/data tnp2k/susenas07panel-modul-blokvl-egnyte.dbf']\n",
        "# Convert dbf file into csv 03\n",
        "loop = 0\n",
        "for e in listFile:\n",
        "  # fileSource = path[0] + '/' + e\n",
        "  fileSource = e.replace(\"'\",\"\")\n",
        "  fname,ext = os.path.splitext(fileSource)\n",
        "  fileDestination = fname + '-dbf.csv'\n",
        "\n",
        "  print(fileSource, type(fileSource))\n",
        "  \n",
        "  # Convert file from dbf into csv (using dbfread)\n",
        "  table = DBF(fileSource, parserclass=MyFieldParser)     # table variable is a DBF object\n",
        "  print(table.header)\n",
        "  # print(table.field_names)\n",
        "  # print(table.fields)\n",
        "  \n",
        "  with open(fileDestination, 'w', newline = '') as f:               # create a csv file, fill it with dbf content\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(table.field_names)\n",
        "    for record in table:\n",
        "      # print (record.values())\n",
        "      writer.writerow(list(record.values()))\n",
        "\n",
        "    # for i, record in enumerate(table):\n",
        "      # print('bebek')\n",
        "      # writer.writerow(list(record.values()))\n",
        "\n",
        "  # Read and import csv file dataset into pandas data frame, change paths if needed\n",
        "  dfBPSData = pd.read_csv(fileDestination)\n",
        "\n",
        "  print(fileDestination, fname)\n",
        "  print(\"dfBPSData.shape :\", dfBPSData.shape)\n",
        "  # print(\"type(dfBPSData) :\", type(dfBPSData))\n",
        "  \n",
        "  # print(\"converting\", e, \"to\", fileDestination)\n",
        "  loop += 1\n",
        "\n",
        "print(loop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yl4BlGp-nDIW"
      },
      "source": [
        "import pandas as pd\n",
        "dfBPSData = pd.read_csv(fileDestination)\n",
        "\n",
        "print(\"dfBPSData.shape :\", dfBPSData.shape)\n",
        "print(\"type(dfBPSData) :\", type(dfBPSData))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAhmAD0jnZbk"
      },
      "source": [
        "fileDestination2 = '/content/drive/MyDrive/04 Rawdata/06-susenas/susenas-2007/data tnp2k backup/susenas07-ki.csv'\n",
        "dfBPSData = pd.read_csv(fileDestination)\n",
        "\n",
        "print(\"dfBPSData.shape :\", dfBPSData.shape)\n",
        "print(\"type(dfBPSData) :\", type(dfBPSData))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJi4JeSRUizG"
      },
      "source": [
        "print(sys.version)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bi8KIcdNGog"
      },
      "source": [
        "# %%time\n",
        "import os\n",
        "# import glob\n",
        "# import sys\n",
        "# import csv\n",
        "# from dbfread import DBF\n",
        "# from dbfread import FieldParser\n",
        "\n",
        "\"\"\"\n",
        "class MyFieldParser(FieldParser):\n",
        "    def parseN(self, field, data):\n",
        "        data = data.strip().strip(b'*\\x00')  # Had to strip out the other characters first before \\x00, as per super function specs.\n",
        "        return super(MyFieldParser, self).parseN(field, data)\n",
        "\n",
        "    def parseD(self, field, data):\n",
        "        data = data.strip(b'\\x00')\n",
        "        return super(MyFieldParser, self).parseD(field, data)\n",
        "\"\"\"\n",
        "\n",
        "# Set working directory 01\n",
        "# workingDirectory = dictDrive[\"susenas-2007\"]+\"/data\\ tnp2k/\"\n",
        "# workingDirectory = workingDirectory.replace(\"\\\\\",\"\")\n",
        "# os.chdir(workingDirectory)\n",
        "# path = !pwd\n",
        "# print(path)\n",
        "\n",
        "# List dbf file within directory 01\n",
        "listFile = !ls {dictDrive[\"susenas-2007\"]}/data\\ tnp2k/*.dbf\n",
        "\n",
        "# Convert dbf file into csv 03\n",
        "loop = 0\n",
        "for e in listFile:\n",
        "  fileSource = e\n",
        "  fname,ext = os.path.splitext(fileSource)\n",
        "  fileDestination = fname + '.csv'\n",
        "\n",
        "  # Convert file from dbf into csv (using Dbf5)\n",
        "  # dbfFile = Dbf5(fileSource, codec='utf-8')\n",
        "  # dfDbf = dbfFile.to_dataframe()\n",
        "  # dfDbf.to_csv(fileDestination, encoding='utf-8', index=False)\n",
        "  # dbfFile.to_csv(fileDestination)\n",
        "\n",
        "  # Convert file from dbf into csv (using dbf)\n",
        "  # with dbf.Table(fileSource) as table:\n",
        "  #  dbf.export(table, fileDestination)\n",
        "  \"\"\"\n",
        "  # Convert file from dbf into csv (using dbf)\n",
        "  table = DBF(fileSource, load=True, parserclass=MyFieldParser)     # table variable is a DBF object\n",
        "  print(table.header)\n",
        "  print(table.field_names)\n",
        "  # print(table.fields)\n",
        "  with open(fileDestination, 'w', newline = '') as f: # create a csv file, fill it with dbf content\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(table.field_names)\n",
        "    for record in table:\n",
        "      # print (record.values())\n",
        "    # for i, record in enumerate(table):\n",
        "      # print('bebek')\n",
        "      writer.writerow(list(record.values()))\n",
        "\n",
        "  # Read and import csv file dataset into pandas data frame, change paths if needed\n",
        "  # dfBPSData = pd.read_csv(pathDestination)\n",
        "\n",
        "  # print(\"dfBPSData.shape :\", dfBPSData.shape)\n",
        "  # print(\"type(dfBPSData) :\", type(dfBPSData))\n",
        "  \"\"\"\n",
        "  print(\"converting\", e, \"to\", fileDestination)\n",
        "  loop += 1\n",
        "\n",
        "print(loop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1sE4IW4n31K"
      },
      "source": [
        "!pip install dbfread\n",
        "!pip install dbfpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fW5ee70nZ_U"
      },
      "source": [
        "import csv\n",
        "from dbfread import DBF\n",
        "\n",
        "dbf_table_pth = '/content/drive/My Drive/04 Rawdata/07-bos/v_rkas2.dbf'\n",
        "# dbf_table_pth = '/content/drive/My Drive/04 Rawdata/07-bos/v_sumber_dana.dbf'\n",
        "\n",
        "def dbf_to_csv(dbf_table_pth):#Input a dbf, output a csv, same name, same path, except extension\n",
        "    csv_fn = dbf_table_pth[:-4]+ \".csv\" #Set the csv file name\n",
        "    table = DBF(dbf_table_pth)# table variable is a DBF object\n",
        "    with open(csv_fn, 'w', newline = '') as f:# create a csv file, fill it with dbf content\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(table.field_names)# write the column name\n",
        "        for record in table:# write the rows\n",
        "            writer.writerow(list(record.values()))\n",
        "    return csv_fn# return the csv name\n",
        "\n",
        "\n",
        "dbf_to_csv(dbf_table_pth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3HhaJlvobaR"
      },
      "source": [
        "import csv\n",
        "from dbfread import DBF\n",
        "\n",
        "dbf_table = DBF('/content/drive/My Drive/04 Rawdata/07-bos/v_rkas2.dbf')\n",
        "\n",
        "for record in dbf_table:\n",
        "  print(record)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MYDAdrzlJEw"
      },
      "source": [
        "import csv\n",
        "from dbfpy import dbf\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# filename = sys.argv[1]\n",
        "filename = '/content/drive/My Drive/04 Rawdata/07-bos/v_rkas2.dbf'\n",
        "if filename.endswith('.dbf'):\n",
        "    print(\"Converting %s to csv\" % filename)\n",
        "    \"\"\"csv_fn = filename[:-4]+ \".csv\"\n",
        "    with open(csv_fn,'wb') as csvfile:\n",
        "        in_db = dbf.Dbf(filename)\n",
        "        out_csv = csv.writer(csvfile)\n",
        "        names = []\n",
        "        for field in in_db.header.fields:\n",
        "            names.append(field.name)\n",
        "        #out_csv.writerow(names)\n",
        "        for rec in in_db:\n",
        "            row = [i.decode('utf8').encode('cp1250') if isinstance(i, str) else i for i in rec.fieldData]\n",
        "            out_csv.writerow(rec.fieldData)\n",
        "        in_db.close()\n",
        "        print(\"Done...\")\"\"\"\n",
        "else:\n",
        "  print(\"Filename does not end with .dbf\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDgQuZ3HEYeg"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "from simpledbf import Dbf5\n",
        "\n",
        "dbf = Dbf5('/content/drive/My Drive/04 Rawdata/07-bos/v_rkas2.dbf')\n",
        "df = dbf.to_dataframe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46xbxRFkF2cS"
      },
      "source": [
        "# !pip install pysal\n",
        "!pip install giddy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6hoIu5GGnkj"
      },
      "source": [
        "!pip install pysal\n",
        "import pysal as ps"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGWFuXpkFq_A"
      },
      "source": [
        "import pysal as ps\n",
        "import pandas as pd\n",
        "'''\n",
        "Arguments\n",
        "---------\n",
        "dbfile  : DBF file - Input to be imported\n",
        "upper   : Condition - If true, make column heads upper case\n",
        "'''\n",
        "def dbf2DF(dbfile, upper=True): #Reads in DBF files and returns Pandas DF\n",
        "    db = ps.open(dbfile) #Pysal to open DBF\n",
        "    d = {col: db.by_col(col) for col in db.header} #Convert dbf to dictionary\n",
        "    #pandasDF = pd.DataFrame(db[:]) #Convert to Pandas DF\n",
        "    pandasDF = pd.DataFrame(d) #Convert to Pandas DF\n",
        "    if upper == True: #Make columns uppercase if wanted \n",
        "        pandasDF.columns = map(str.upper, db.header) \n",
        "    db.close() \n",
        "    return pandasDF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQhBHbZyFzJ2"
      },
      "source": [
        "df = dbf2DF('../input/afrbeep020.dbf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYWlW7Y0cdvZ"
      },
      "source": [
        "## Step 0207 Check & Review Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJIWFRwY4Po9"
      },
      "source": [
        "# Pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool,\n",
        "# built on top of the Python programming language- non-standard python libraries\n",
        "import pandas as pd\n",
        "\n",
        "# Read and import csv file dataset into pandas data frame, change paths if needed\n",
        "dictFileReview = {\n",
        "  'susenas00-ki': 'susenas00-ki.csv', 'susenas00-kr': 'susenas00-kr.csv', 'susenas00-kna': 'susenas00-kna.csv', 'susenas00-mod-ki': 'susenas00-mod-ki.csv', 'susenas00-mod-kr': 'susenas00-mod-kr.csv',\n",
        "  'susenas01-ki': 'susenas01-ki.csv', 'susenas01-kr': 'susenas01-kr.csv', 'susenas01-ind-km': 'susenas01-ind-km.csv', 'susenas01-rt-km': 'susenas01-rt-km.csv',\n",
        "  'susenas02-ki': 'susenas02jul-ki.csv', 'susenas02-kr': 'susenas02jul-kr.csv'\n",
        "  \n",
        "}\n",
        "\n",
        "# Set working directory 01\n",
        "# workingDirectory = dictDirectory['sakernas']\n",
        "# workingDirectory = dictDirectory['sakernas'] + '/data'\n",
        "# os.chdir(workingDirectory)\n",
        "path = !pwd\n",
        "\n",
        "# Review dataset 02\n",
        "# fname = dictFileReview['susenas00-kna']\n",
        "fname = 'v_rkas.csv'\n",
        "# fname = 'susenas02jul-module-consumption.csv'\n",
        "\n",
        "print(workingDirectory)\n",
        "pathSource = path[0] + '/' + fname\n",
        "print(pathSource)\n",
        "dfBPSData = pd.read_csv(pathSource, sep=';', engine='python', nrows=100)\n",
        "\n",
        "print(fname)\n",
        "print(\"dfBPSData.shape  :\", dfBPSData.shape)\n",
        "print(\"type(dfBPSData)  :\", type(dfBPSData))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfJKvzLAiGUh"
      },
      "source": [
        "dfBPSData"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgYO_jkZX2sC"
      },
      "source": [
        "### 0207 01 Check & review data Sakernas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJULAHsasrAb"
      },
      "source": [
        "%%time\n",
        "# Sakernas data pre-processing\n",
        "\n",
        "# Pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool,\n",
        "# built on top of the Python programming language- non-standard python libraries\n",
        "import pandas as pd\n",
        "\n",
        "# Set working directory 01\n",
        "# workingDirectory = dictDirectory['sakernas']\n",
        "# os.chdir(workingDirectory)\n",
        "# path = !pwd\n",
        "\n",
        "# List csv file within directory 02\n",
        "listFile = !ls {dictDrive[\"sakernas-2000\"]}/data/*.csv\n",
        "\n",
        "# Prints all files within directory 03\n",
        "loop = 0\n",
        "for fname in listFile:\n",
        "  fileSource = fname.replace(\"'\",\"\")\n",
        "  # pathDestination = pathSource\n",
        "  dfBPSData = pd.read_csv(fileSource)\n",
        "\n",
        "  print(fname, \".shape  :\", dfBPSData.shape)\n",
        "  print(dfBPSData.info(verbose=True, null_counts=True))\n",
        "  loop += 1\n",
        "\n",
        "print(\"Number of files: \",loop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2DC9JaDu3cZ"
      },
      "source": [
        "### 0207 02 Check & review data Susenas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsvdH403RwSH"
      },
      "source": [
        "%%time\n",
        "# Set working directory 01\n",
        "# workingDirectory = dictDirectory['sakernas']\n",
        "# os.chdir(workingDirectory)\n",
        "# path = !pwd\n",
        "\n",
        "# List csv file within directory 02\n",
        "listFile = !ls {dictDrive[\"susenas-2007\"]}/data\\ tnp2k/*.csv\n",
        "\n",
        "# Prints all files within directory 03\n",
        "loop = 0\n",
        "for fname in listFile:\n",
        "  fileSource = fname.replace(\"'\",\"\")\n",
        "  # pathDestination = pathSource\n",
        "  dfBPSData = pd.read_csv(fileSource)\n",
        "\n",
        "  print(fname, \".shape  :\", dfBPSData.shape)\n",
        "  loop += 1\n",
        "\n",
        "print(loop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqeuGrJj-RFA"
      },
      "source": [
        "# Examine dataset, see data type\n",
        "print(fileSource)\n",
        "dfBPSData.info(verbose=True, null_counts=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUYzQXRCqpVf"
      },
      "source": [
        "# Examine dataset, see data type\n",
        "print(dfBPSData.describe(percentiles=[], include='all').transpose().to_string())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4HNyNlp5783"
      },
      "source": [
        "# Examine dataset, see data values\n",
        "dfBPSData.head()\n",
        "# dfBPSData.tail()\n",
        "# dfBPSData.sort_values(by=['psid'], ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGloACERum6K"
      },
      "source": [
        "# Examine dataset, see data values\n",
        "dfBPSData.head()\n",
        "# dfBPSData.tail()\n",
        "# dfBPSData.sort_values(by=['psid'], ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nyd6i3s6Ykb"
      },
      "source": [
        "# basic info about columns in each dataset\n",
        "for name, df in dfs.items():\n",
        "    print(\"df: %s\\n\" %name)\n",
        "    print(\"df:\", name, \"type:\", type(df), \"\\n\")\n",
        "    print(\"shape: %d rows, %d cols\\n\" %df.shape)\n",
        "    \n",
        "    print(\"column info:\")\n",
        "    for col in df.columns:\n",
        "        print(\"* %s: %d nulls, %d nans, %d unique vals, most common: %s\" % (\n",
        "            col, \n",
        "            df[col].isnull().sum(),\n",
        "            df[col].isna().sum(),\n",
        "            df[col].nunique(),\n",
        "            df[col].value_counts().head(2).to_dict()\n",
        "        ))\n",
        "    print(\"\\n------\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q362ArQ_HXIn"
      },
      "source": [
        "# Examine dataset\n",
        "# print(dfBPSData.describe(percentiles=[], include='all').transpose().to_string())\n",
        "print(dfBPSData.count().transpose().to_string())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pneHPWaGBjsR"
      },
      "source": [
        "pd.reset_option('display.show_dimensions')\n",
        "pd.set_option('display.show_dimensions', False)\n",
        "print(pd.options.display.max_rows, pd.options.display.show_dimensions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v78u6YBW86_c"
      },
      "source": [
        "# Examine dataset, first 5 rows\n",
        "# dfBPSData['DDESA94'].isna().any()\n",
        "# dfBPSData.sort_values(by='psid')\n",
        "# dfBPSData.tail(10)\n",
        "dfBPSData.isna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjXJiFPxgu-Y"
      },
      "source": [
        "## Step 0208 Convert Data Type\n",
        "Convert data type float into integer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eH1j5M24iv4F"
      },
      "source": [
        "%%time\n",
        "# Read and import csv file dataset into pandas data frame, change paths if needed\n",
        "fname = 'se2016-listing-11.csv'\n",
        "\n",
        "path = !pwd\n",
        "# print(path)\n",
        "\n",
        "pathSource = path[0] + '/' + fname\n",
        "pathDestination = pathSource\n",
        "# print(pathSource)\n",
        "dfBPSData = pd.read_csv(pathSource)\n",
        "\n",
        "print(fname)\n",
        "print(\"dfBPSData.shape  :\", dfBPSData.shape)\n",
        "print(\"type(dfBPSData)  :\", type(dfBPSData))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DusZEXqGjAY7"
      },
      "source": [
        "# Examine dataset, found isna & maximum values\n",
        "# dfBPSData.columns.isna().any()\n",
        "# dfBPSData['psid'].max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48fPO0PYgsPF"
      },
      "source": [
        "# Examine dataset, create data type dictionary\n",
        "dfBPSTypeSeries  = dict(dfBPSData.dtypes)\n",
        "print(dfBPSTypeSeries)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7Rk2oELhMXJ"
      },
      "source": [
        "# Convert data type float into integer\n",
        "for (key, values) in dfBPSTypeSeries.items():\n",
        "  if values=='float64':\n",
        "    print(key, values)\n",
        "    dfBPSData[key] = dfBPSData[key].astype('Int64')\n",
        "\n",
        "    # Special case on certain field\n",
        "    # if key!='D94_VNOB':\n",
        "      # dfBPSData[key] = dfBPSData[key].astype('Int64')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eFX_WNOyRRs"
      },
      "source": [
        "# Save data from convert data type operation\n",
        "print(pathDestination)\n",
        "dfBPSData.to_csv(pathDestination, encoding='utf-8', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hfng_tp1uJhE"
      },
      "source": [
        "Convert data type float into integer (Loop)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aY5RjFlpuHJW"
      },
      "source": [
        "%%time\n",
        "# list csv file within directory\n",
        "listFile = [f for f in glob.glob('*.csv')]\n",
        "\n",
        "# data type to convert\n",
        "dDataType = {\n",
        "  'provinsi':'object',\n",
        "  'nama_prov':'object',\n",
        "  'kabupaten':'object',\n",
        "  'nama_kab':'object',\n",
        "  'idperkab':'Int64',\n",
        "  'b1r11d':'object',\n",
        "  'b1r13':'object',\n",
        "  'b1r14a':'object',\n",
        "  'b1r14b':'object',\n",
        "  'kategori':'object',\n",
        "  'b1r15c':'object',\n",
        "  'b1r15d':'object',\n",
        "  'b1r16':'object',\n",
        "  'b1r19a':'Int64',\n",
        "  'b1r21':'object',\n",
        "  'b1r22a':'object',\n",
        "  'b1r22b':'object',\n",
        "  'kat_omset':'Int64',\n",
        "  'skalausaha':'object',\n",
        "  'penimbang':'object',\n",
        "  'renum':'Int64'\n",
        "    }\n",
        "\n",
        "path = !pwd\n",
        "print(path)\n",
        "loop = 0\n",
        "\n",
        "for e in listFile:\n",
        "  pathSource = path[0] + '/' + e\n",
        "  fname,ext = os.path.splitext(pathSource)\n",
        "  pathDestination = fname + \"-convert\" + ext\n",
        "  dfBPSData = pd.read_csv(pathSource)\n",
        "\n",
        "  # print(fname, \".shape  :\", dfBPSData.shape)\n",
        "  # print(pathSource)\n",
        "  print(pathDestination)\n",
        "  \n",
        "  # Examine dataset, create data type dictionary\n",
        "  dfBPSTypeSeries  = dict(dfBPSData.dtypes)\n",
        "\n",
        "  # Convert data type float into integer\n",
        "  for (key, values) in dfBPSTypeSeries.items():\n",
        "    dfBPSData[key] = dfBPSData[key].astype(dDataType[key.lower()])\n",
        "    # print(key, values, \"convert to\", dDataType[key.lower()])\n",
        "\n",
        "  # Save data from convert data type operation\n",
        "  # print(pathDestination)\n",
        "  dfBPSData.to_csv(pathDestination, encoding='utf-8', index=False)\n",
        "  loop += 1\n",
        "\n",
        "print(loop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uX3tNt4zFM4O"
      },
      "source": [
        "# save data into google cloud storage\n",
        "bucket_name = 'bucket-prospera-01'\n",
        "!gsutil cp /content/drive/My\\ Drive/Database/se-2016-listing/data/se2016-listing-33.csv gs://{bucket_name}/01-rawdata/01-bps/04-sensus-ekonomi/se-2016/se-2016-listing/data\n",
        "!gsutil cp /content/drive/My\\ Drive/Database/se-2016-listing/data/se-2016-listing-33-convert.csv gs://{bucket_name}/01-rawdata/01-bps/04-sensus-ekonomi/se-2016/se-2016-listing/data\n",
        "# !gsutil -m cp -r /content/drive/My\\ Drive/Data/* gs://bucket-prospera-01/01-rawdata/01-bps/04-sensus-ekonomi/se-2016/se-2016-listing/data\n",
        "/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAoEpo1PjNO4"
      },
      "source": [
        "## Step 0209 Merge Dataset\n",
        "Merge Dataset if required"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ndyaSVwzo7o"
      },
      "source": [
        "%%time\n",
        "# Read and import csv file dataset into pandas data frame, change paths if needed\n",
        "fname  = 'ind95.csv' # Merge data files\n",
        "fnameA = 'ind95a.csv'\n",
        "fnameB = 'ind95b.csv'\n",
        "\n",
        "path = !pwd\n",
        "pathSourceA = path[0] + '/' + fnameA\n",
        "pathSourceB = path[0] + '/' + fnameB\n",
        "pathDestination = path[0] + '/' + fname\n",
        "print(pathSourceA)\n",
        "print(pathSourceB)\n",
        "dfBPSDataA = pd.read_csv(pathSourceA)\n",
        "dfBPSDataB = pd.read_csv(pathSourceB)\n",
        "\n",
        "print(\"dfBPSDataA.shape  :\", dfBPSDataA.shape)\n",
        "print(\"type(dfBPSDataA)  :\", type(dfBPSDataA))\n",
        "print(\"dfBPSDataB.shape  :\", dfBPSDataB.shape)\n",
        "print(\"type(dfBPSDataB)  :\", type(dfBPSDataB))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YAl2E_m2RjP"
      },
      "source": [
        "# Rename joining keys\n",
        "dfBPSDataA.rename({'nomor': 'NOMOR_A'}, axis='columns', inplace=True)\n",
        "dfBPSDataB.rename({'nomor': 'NOMOR_B'}, axis='columns', inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksYR16Zzj7g_"
      },
      "source": [
        "# Merge data files\n",
        "dfBPSData = dfBPSDataA.merge(dfBPSDataB, left_on='NOMOR_A', right_on='NOMOR_B')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhHx-SgAkXmB"
      },
      "source": [
        "dfBPSData[['NOMOR_A','NOMOR_B']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_NEl1xr_3mu"
      },
      "source": [
        "# Save data from convert data type operation\n",
        "print(pathDestination)\n",
        "dfBPSData.to_csv(pathDestination, encoding='utf-8', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bz91TM1WofAg"
      },
      "source": [
        "### 0209 01 List Files within Working Directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhsjEz6jwMVW"
      },
      "source": [
        "%%time\n",
        "# Set working directory 01\n",
        "workingDirectory = dictDirectory['se-2016-direktori-data']\n",
        "os.chdir(workingDirectory)\n",
        "path = !pwd\n",
        "print(path)\n",
        "\n",
        "# List file on working directory 02\n",
        "listFile = [f for f in glob.glob('*.csv')]\n",
        "# listFile = [f for f in glob.glob('*.dbf')]\n",
        "# listFile = [f for f in glob.glob('*.*')]\n",
        "\n",
        "# Prints all files within directory 03\n",
        "loop = 0\n",
        "for e in listFile:\n",
        "  print(e)\n",
        "  loop += 1\n",
        "\n",
        "print(loop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iov9X9Hx9Skg"
      },
      "source": [
        "### 0209 02 Merge File 2 Tables tableA + tableB -> tableMerge"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_gQnmQAodv0"
      },
      "source": [
        "%%time\n",
        "# Merge table se2016-umk\n",
        "listFile = [\n",
        "  'se2016-umk-11.csv', 'se2016-umk-01-11.csv', 'se2016-umk-02-11.csv', 'se2016-umk-12.csv', 'se2016-umk-01-12.csv', 'se2016-umk-02-12.csv', 'se2016-umk-13.csv', 'se2016-umk-01-13.csv', 'se2016-umk-02-13.csv', 'se2016-umk-14.csv', 'se2016-umk-01-14.csv', 'se2016-umk-02-14.csv', 'se2016-umk-15.csv', 'se2016-umk-01-15.csv', 'se2016-umk-02-15.csv', \n",
        "  'se2016-umk-16.csv', 'se2016-umk-01-16.csv', 'se2016-umk-02-16.csv', 'se2016-umk-17.csv', 'se2016-umk-01-17.csv', 'se2016-umk-02-17.csv', 'se2016-umk-18.csv', 'se2016-umk-01-18.csv', 'se2016-umk-02-18.csv', 'se2016-umk-19.csv', 'se2016-umk-01-19.csv', 'se2016-umk-02-19.csv', 'se2016-umk-21.csv', 'se2016-umk-01-21.csv', 'se2016-umk-02-21.csv', \n",
        "  'se2016-umk-31.csv', 'se2016-umk-01-31.csv', 'se2016-umk-02-31.csv', 'se2016-umk-32.csv', 'se2016-umk-01-32.csv', 'se2016-umk-02-32.csv', 'se2016-umk-33.csv', 'se2016-umk-01-33.csv', 'se2016-umk-02-33.csv', 'se2016-umk-34.csv', 'se2016-umk-01-34.csv', 'se2016-umk-02-34.csv', 'se2016-umk-35.csv', 'se2016-umk-01-35.csv', 'se2016-umk-02-35.csv', \n",
        "  'se2016-umk-36.csv', 'se2016-umk-01-36.csv', 'se2016-umk-02-36.csv', 'se2016-umk-51.csv', 'se2016-umk-01-51.csv', 'se2016-umk-02-51.csv', 'se2016-umk-52.csv', 'se2016-umk-01-52.csv', 'se2016-umk-02-52.csv', 'se2016-umk-53.csv', 'se2016-umk-01-53.csv', 'se2016-umk-02-53.csv', 'se2016-umk-61.csv', 'se2016-umk-01-61.csv', 'se2016-umk-02-61.csv', \n",
        "  'se2016-umk-62.csv', 'se2016-umk-01-62.csv', 'se2016-umk-02-62.csv', 'se2016-umk-63.csv', 'se2016-umk-01-63.csv', 'se2016-umk-02-63.csv', 'se2016-umk-64.csv', 'se2016-umk-01-64.csv', 'se2016-umk-02-64.csv', 'se2016-umk-65.csv', 'se2016-umk-01-65.csv', 'se2016-umk-02-65.csv', 'se2016-umk-71.csv', 'se2016-umk-01-71.csv', 'se2016-umk-02-71.csv', \n",
        "  'se2016-umk-72.csv', 'se2016-umk-01-72.csv', 'se2016-umk-02-72.csv', 'se2016-umk-73.csv', 'se2016-umk-01-73.csv', 'se2016-umk-02-73.csv', 'se2016-umk-74.csv', 'se2016-umk-01-74.csv', 'se2016-umk-02-74.csv', 'se2016-umk-75.csv', 'se2016-umk-01-75.csv', 'se2016-umk-02-75.csv', 'se2016-umk-76.csv', 'se2016-umk-01-76.csv', 'se2016-umk-02-76.csv', \n",
        "  'se2016-umk-81.csv', 'se2016-umk-01-81.csv', 'se2016-umk-02-81.csv', 'se2016-umk-82.csv', 'se2016-umk-01-82.csv', 'se2016-umk-02-82.csv', 'se2016-umk-91.csv', 'se2016-umk-01-91.csv', 'se2016-umk-02-91.csv', 'se2016-umk-94.csv', 'se2016-umk-01-94.csv', 'se2016-umk-02-94.csv'\n",
        "]\n",
        "\n",
        "# Set working directory 01\n",
        "workingDirectory = dictDirectory['se-2016-umk-data']\n",
        "os.chdir(workingDirectory)\n",
        "path = !pwd\n",
        "# print(path)\n",
        "\n",
        "loop = 0\n",
        "for i in range (0, len(listFile), 3):\n",
        "  fname = listFile[i]     # Merge data files\n",
        "  fnameA = listFile[i+1]\n",
        "  fnameB  = listFile[i+2]\n",
        "  pathSourceA = path[0] + '/data-01/' + fnameA\n",
        "  pathSourceB = path[0] + '/data-02/' + fnameB\n",
        "  pathDestination = path[0] + '/' + fname\n",
        "\n",
        "  # print(pathSourceA, pathSourceB, pathDestination)\n",
        "  dfBPSDataA = pd.read_csv(pathSourceA)\n",
        "  dfBPSDataB = pd.read_csv(pathSourceB)\n",
        "\n",
        "  # Rename joining keys\n",
        "  dfBPSDataA.rename({'IDPERUSAHA': 'PERUSAHAAN_ID', 'JENISKUESI': 'JENISKUESIONER_A', 'PROV': 'PROVINSI_IDA', 'SKALAUSAHA': 'SKALAUSAHA_A', 'WEIGHT': 'WEIGHT_A'}, axis='columns', inplace=True)\n",
        "  dfBPSDataB.rename({'IDPERUSAHA': 'PERUSAHAAN_ID', 'JENISKUESI': 'JENISKUESIONER_B', 'PROV': 'PROVINSI_IDB', 'SKALAUSAHA': 'SKALAUSAHA_B', 'WEIGHT': 'WEIGHT_B'}, axis='columns', inplace=True)\n",
        "\n",
        "  dfBPSData = [dfBPSDataA, dfBPSDataB]\n",
        "\n",
        "  # Merge data files\n",
        "  # dfBPSData = dfBPSDataA.merge(dfBPSDataB, left_on='IDPERUSAHAAN_A', right_on='IDPERUSAHAAN_B')\n",
        "  dfBPSDataMerge = reduce(lambda left,right: pd.merge(left,right,on='PERUSAHAAN_ID'), dfBPSData)\n",
        "  \n",
        "  # print(pathDestination, \"dfBPSDataA.shape:\", dfBPSDataA.shape, \"dfBPSDataB.shape:\", dfBPSDataB.shape)\n",
        "  # print(pathDestination, \"dfBPSDataA.shape:\", dfBPSDataA.shape, \"dfBPSDataB.shape:\", dfBPSDataB.shape, \"dfBPSDataMerge.shape:\", dfBPSData.shape)\n",
        "  # print(\"dfBPSDataA.shape:\", dfBPSDataA.shape[0], \"dfBPSDataB.shape:\", dfBPSDataB.shape[0], \"dfBPSDataMerge.shape:\", dfBPSData.shape[0])\n",
        "\n",
        "  # Save data from merge data type operation\n",
        "  print(pathDestination, dfBPSDataMerge.shape)\n",
        "  # dfBPSDataMerge.to_csv(pathDestination, encoding='utf-8', index=False)\n",
        "\n",
        "  loop += 1\n",
        "\n",
        "print(loop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3Pezw3vtu9-"
      },
      "source": [
        "%%time\n",
        "# Merge all table\n",
        "# Set working directory 01\n",
        "workingDirectory = dictDirectory['se-2016-direktori-data']\n",
        "os.chdir(workingDirectory)\n",
        "path = !pwd\n",
        "# print(path)\n",
        "\n",
        "# List file on working directory 02\n",
        "listFile = [f for f in glob.glob('*.csv')]\n",
        "\n",
        "pathDestination = path[0] + '/' + 'se2016-direktori-merge.csv'\n",
        "dfMerges = []\n",
        "totalRows = 0\n",
        "loop = 0\n",
        "for e in listFile:\n",
        "  pathSource = path[0] + '/' + e\n",
        "  # print('merge ' + pathSource)\n",
        "\n",
        "  # Read and import csv file dataset into pandas data frame, change paths if needed\n",
        "  dfBPSData = pd.read_csv(pathSource)\n",
        "\n",
        "  dfMerges.append(dfBPSData)\n",
        "  print(\"dfBPSData.shape :\", e, dfBPSData.shape)\n",
        "  totalRows += dfBPSData.shape[0]\n",
        "  loop += 1\n",
        "\n",
        "print(loop)\n",
        "\n",
        "\n",
        "dfBPSDataMerge = pd.concat(dfMerges)\n",
        "print(\"dfBPSDataMerge.shape :\", dfBPSDataMerge.shape, totalRows)\n",
        "\n",
        "print(pathDestination)\n",
        "dfBPSDataMerge.to_csv(pathDestination, encoding='utf-8', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqhUYaU_9smX"
      },
      "source": [
        "### 0209 02 Merge File 3 Tables tableA + tableB + tableC -> tableMerge"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lfsv8YVMZyPf"
      },
      "source": [
        "%%time\n",
        "# Merge table se2016-umb-jk\n",
        "listFile = [\n",
        "  'se2016-umb-jk-11.csv', 'se2016-umb-jk-01-11.csv', 'se2016-umb-jk-02-11.csv', 'se2016-umb-jk-03-11.csv', 'se2016-umb-jk-12.csv', 'se2016-umb-jk-01-12.csv', 'se2016-umb-jk-02-12.csv', 'se2016-umb-jk-03-12.csv', 'se2016-umb-jk-13.csv', 'se2016-umb-jk-01-13.csv', 'se2016-umb-jk-02-13.csv', 'se2016-umb-jk-03-13.csv', 'se2016-umb-jk-14.csv', 'se2016-umb-jk-01-14.csv', 'se2016-umb-jk-02-14.csv', 'se2016-umb-jk-03-14.csv', 'se2016-umb-jk-15.csv', 'se2016-umb-jk-01-15.csv', 'se2016-umb-jk-02-15.csv', 'se2016-umb-jk-03-15.csv', \n",
        "  'se2016-umb-jk-16.csv', 'se2016-umb-jk-01-16.csv', 'se2016-umb-jk-02-16.csv', 'se2016-umb-jk-03-16.csv', 'se2016-umb-jk-17.csv', 'se2016-umb-jk-01-17.csv', 'se2016-umb-jk-02-17.csv', 'se2016-umb-jk-03-17.csv', 'se2016-umb-jk-18.csv', 'se2016-umb-jk-01-18.csv', 'se2016-umb-jk-02-18.csv', 'se2016-umb-jk-03-18.csv', 'se2016-umb-jk-19.csv', 'se2016-umb-jk-01-19.csv', 'se2016-umb-jk-02-19.csv', 'se2016-umb-jk-03-19.csv', 'se2016-umb-jk-21.csv', 'se2016-umb-jk-01-21.csv', 'se2016-umb-jk-02-21.csv', 'se2016-umb-jk-03-21.csv', \n",
        "  'se2016-umb-jk-31.csv', 'se2016-umb-jk-01-31.csv', 'se2016-umb-jk-02-31.csv', 'se2016-umb-jk-03-31.csv', 'se2016-umb-jk-32.csv', 'se2016-umb-jk-01-32.csv', 'se2016-umb-jk-02-32.csv', 'se2016-umb-jk-03-32.csv', 'se2016-umb-jk-33.csv', 'se2016-umb-jk-01-33.csv', 'se2016-umb-jk-02-33.csv', 'se2016-umb-jk-03-33.csv', 'se2016-umb-jk-34.csv', 'se2016-umb-jk-01-34.csv', 'se2016-umb-jk-02-34.csv', 'se2016-umb-jk-03-34.csv', 'se2016-umb-jk-35.csv', 'se2016-umb-jk-01-35.csv', 'se2016-umb-jk-02-35.csv', 'se2016-umb-jk-03-35.csv', \n",
        "  'se2016-umb-jk-36.csv', 'se2016-umb-jk-01-36.csv', 'se2016-umb-jk-02-36.csv', 'se2016-umb-jk-03-36.csv', 'se2016-umb-jk-51.csv', 'se2016-umb-jk-01-51.csv', 'se2016-umb-jk-02-51.csv', 'se2016-umb-jk-03-51.csv', 'se2016-umb-jk-52.csv', 'se2016-umb-jk-01-52.csv', 'se2016-umb-jk-02-52.csv', 'se2016-umb-jk-03-52.csv', 'se2016-umb-jk-53.csv', 'se2016-umb-jk-01-53.csv', 'se2016-umb-jk-02-53.csv', 'se2016-umb-jk-03-53.csv', 'se2016-umb-jk-61.csv', 'se2016-umb-jk-01-61.csv', 'se2016-umb-jk-02-61.csv', 'se2016-umb-jk-03-61.csv', \n",
        "  'se2016-umb-jk-62.csv', 'se2016-umb-jk-01-62.csv', 'se2016-umb-jk-02-62.csv', 'se2016-umb-jk-03-62.csv', 'se2016-umb-jk-63.csv', 'se2016-umb-jk-01-63.csv', 'se2016-umb-jk-02-63.csv', 'se2016-umb-jk-03-63.csv', 'se2016-umb-jk-64.csv', 'se2016-umb-jk-01-64.csv', 'se2016-umb-jk-02-64.csv', 'se2016-umb-jk-03-64.csv', 'se2016-umb-jk-65.csv', 'se2016-umb-jk-01-65.csv', 'se2016-umb-jk-02-65.csv', 'se2016-umb-jk-03-65.csv', 'se2016-umb-jk-71.csv', 'se2016-umb-jk-01-71.csv', 'se2016-umb-jk-02-71.csv', 'se2016-umb-jk-03-71.csv', \n",
        "  'se2016-umb-jk-72.csv', 'se2016-umb-jk-01-72.csv', 'se2016-umb-jk-02-72.csv', 'se2016-umb-jk-03-72.csv', 'se2016-umb-jk-73.csv', 'se2016-umb-jk-01-73.csv', 'se2016-umb-jk-02-73.csv', 'se2016-umb-jk-03-73.csv', 'se2016-umb-jk-74.csv', 'se2016-umb-jk-01-74.csv', 'se2016-umb-jk-02-74.csv', 'se2016-umb-jk-03-74.csv', 'se2016-umb-jk-75.csv', 'se2016-umb-jk-01-75.csv', 'se2016-umb-jk-02-75.csv', 'se2016-umb-jk-03-75.csv', 'se2016-umb-jk-76.csv', 'se2016-umb-jk-01-76.csv', 'se2016-umb-jk-02-76.csv', 'se2016-umb-jk-03-76.csv', \n",
        "  'se2016-umb-jk-81.csv', 'se2016-umb-jk-01-81.csv', 'se2016-umb-jk-02-81.csv', 'se2016-umb-jk-03-81.csv', 'se2016-umb-jk-82.csv', 'se2016-umb-jk-01-82.csv', 'se2016-umb-jk-02-82.csv', 'se2016-umb-jk-03-82.csv', 'se2016-umb-jk-91.csv', 'se2016-umb-jk-01-91.csv', 'se2016-umb-jk-02-91.csv', 'se2016-umb-jk-03-91.csv', 'se2016-umb-jk-94.csv', 'se2016-umb-jk-01-94.csv', 'se2016-umb-jk-02-94.csv', 'se2016-umb-jk-03-94.csv'\n",
        "]\n",
        "\n",
        "# Merge table se2016-umb-jnk\n",
        "listFile = [\n",
        "  'se2016-umb-jnk-11.csv', 'se2016-umb-jnk-01-11.csv', 'se2016-umb-jnk-02-11.csv', 'se2016-umb-jnk-03-11.csv', 'se2016-umb-jnk-12.csv', 'se2016-umb-jnk-01-12.csv', 'se2016-umb-jnk-02-12.csv', 'se2016-umb-jnk-03-12.csv', 'se2016-umb-jnk-13.csv', 'se2016-umb-jnk-01-13.csv', 'se2016-umb-jnk-02-13.csv', 'se2016-umb-jnk-03-13.csv', 'se2016-umb-jnk-14.csv', 'se2016-umb-jnk-01-14.csv', 'se2016-umb-jnk-02-14.csv', 'se2016-umb-jnk-03-14.csv', 'se2016-umb-jnk-15.csv', 'se2016-umb-jnk-01-15.csv', 'se2016-umb-jnk-02-15.csv', 'se2016-umb-jnk-03-15.csv', \n",
        "  'se2016-umb-jnk-16.csv', 'se2016-umb-jnk-01-16.csv', 'se2016-umb-jnk-02-16.csv', 'se2016-umb-jnk-03-16.csv', 'se2016-umb-jnk-17.csv', 'se2016-umb-jnk-01-17.csv', 'se2016-umb-jnk-02-17.csv', 'se2016-umb-jnk-03-17.csv', 'se2016-umb-jnk-18.csv', 'se2016-umb-jnk-01-18.csv', 'se2016-umb-jnk-02-18.csv', 'se2016-umb-jnk-03-18.csv', 'se2016-umb-jnk-19.csv', 'se2016-umb-jnk-01-19.csv', 'se2016-umb-jnk-02-19.csv', 'se2016-umb-jnk-03-19.csv', 'se2016-umb-jnk-21.csv', 'se2016-umb-jnk-01-21.csv', 'se2016-umb-jnk-02-21.csv', 'se2016-umb-jnk-03-21.csv', \n",
        "  'se2016-umb-jnk-31.csv', 'se2016-umb-jnk-01-31.csv', 'se2016-umb-jnk-02-31.csv', 'se2016-umb-jnk-03-31.csv', 'se2016-umb-jnk-32.csv', 'se2016-umb-jnk-01-32.csv', 'se2016-umb-jnk-02-32.csv', 'se2016-umb-jnk-03-32.csv', 'se2016-umb-jnk-33.csv', 'se2016-umb-jnk-01-33.csv', 'se2016-umb-jnk-02-33.csv', 'se2016-umb-jnk-03-33.csv', 'se2016-umb-jnk-34.csv', 'se2016-umb-jnk-01-34.csv', 'se2016-umb-jnk-02-34.csv', 'se2016-umb-jnk-03-34.csv', 'se2016-umb-jnk-35.csv', 'se2016-umb-jnk-01-35.csv', 'se2016-umb-jnk-02-35.csv', 'se2016-umb-jnk-03-35.csv', \n",
        "  'se2016-umb-jnk-36.csv', 'se2016-umb-jnk-01-36.csv', 'se2016-umb-jnk-02-36.csv', 'se2016-umb-jnk-03-36.csv', 'se2016-umb-jnk-51.csv', 'se2016-umb-jnk-01-51.csv', 'se2016-umb-jnk-02-51.csv', 'se2016-umb-jnk-03-51.csv', 'se2016-umb-jnk-52.csv', 'se2016-umb-jnk-01-52.csv', 'se2016-umb-jnk-02-52.csv', 'se2016-umb-jnk-03-52.csv', 'se2016-umb-jnk-53.csv', 'se2016-umb-jnk-01-53.csv', 'se2016-umb-jnk-02-53.csv', 'se2016-umb-jnk-03-53.csv', 'se2016-umb-jnk-61.csv', 'se2016-umb-jnk-01-61.csv', 'se2016-umb-jnk-02-61.csv', 'se2016-umb-jnk-03-61.csv', \n",
        "  'se2016-umb-jnk-62.csv', 'se2016-umb-jnk-01-62.csv', 'se2016-umb-jnk-02-62.csv', 'se2016-umb-jnk-03-62.csv', 'se2016-umb-jnk-63.csv', 'se2016-umb-jnk-01-63.csv', 'se2016-umb-jnk-02-63.csv', 'se2016-umb-jnk-03-63.csv', 'se2016-umb-jnk-64.csv', 'se2016-umb-jnk-01-64.csv', 'se2016-umb-jnk-02-64.csv', 'se2016-umb-jnk-03-64.csv', 'se2016-umb-jnk-65.csv', 'se2016-umb-jnk-01-65.csv', 'se2016-umb-jnk-02-65.csv', 'se2016-umb-jnk-03-65.csv', 'se2016-umb-jnk-71.csv', 'se2016-umb-jnk-01-71.csv', 'se2016-umb-jnk-02-71.csv', 'se2016-umb-jnk-03-71.csv', \n",
        "  'se2016-umb-jnk-72.csv', 'se2016-umb-jnk-01-72.csv', 'se2016-umb-jnk-02-72.csv', 'se2016-umb-jnk-03-72.csv', 'se2016-umb-jnk-73.csv', 'se2016-umb-jnk-01-73.csv', 'se2016-umb-jnk-02-73.csv', 'se2016-umb-jnk-03-73.csv', 'se2016-umb-jnk-74.csv', 'se2016-umb-jnk-01-74.csv', 'se2016-umb-jnk-02-74.csv', 'se2016-umb-jnk-03-74.csv', 'se2016-umb-jnk-75.csv', 'se2016-umb-jnk-01-75.csv', 'se2016-umb-jnk-02-75.csv', 'se2016-umb-jnk-03-75.csv', 'se2016-umb-jnk-76.csv', 'se2016-umb-jnk-01-76.csv', 'se2016-umb-jnk-02-76.csv', 'se2016-umb-jnk-03-76.csv', \n",
        "  'se2016-umb-jnk-81.csv', 'se2016-umb-jnk-01-81.csv', 'se2016-umb-jnk-02-81.csv', 'se2016-umb-jnk-03-81.csv', 'se2016-umb-jnk-82.csv', 'se2016-umb-jnk-01-82.csv', 'se2016-umb-jnk-02-82.csv', 'se2016-umb-jnk-03-82.csv', 'se2016-umb-jnk-91.csv', 'se2016-umb-jnk-01-91.csv', 'se2016-umb-jnk-02-91.csv', 'se2016-umb-jnk-03-91.csv', 'se2016-umb-jnk-94.csv', 'se2016-umb-jnk-01-94.csv', 'se2016-umb-jnk-02-94.csv', 'se2016-umb-jnk-03-94.csv'\n",
        "]\n",
        "\n",
        "# Merge table se2016-umb-sp\n",
        "listFile = [\n",
        "  'se2016-umb-sp-11.csv', 'se2016-umb-sp-01-11.csv', 'se2016-umb-sp-02-11.csv', 'se2016-umb-sp-03-11.csv', 'se2016-umb-sp-12.csv', 'se2016-umb-sp-01-12.csv', 'se2016-umb-sp-02-12.csv', 'se2016-umb-sp-03-12.csv', 'se2016-umb-sp-13.csv', 'se2016-umb-sp-01-13.csv', 'se2016-umb-sp-02-13.csv', 'se2016-umb-sp-03-13.csv', 'se2016-umb-sp-14.csv', 'se2016-umb-sp-01-14.csv', 'se2016-umb-sp-02-14.csv', 'se2016-umb-sp-03-14.csv', 'se2016-umb-sp-15.csv', 'se2016-umb-sp-01-15.csv', 'se2016-umb-sp-02-15.csv', 'se2016-umb-sp-03-15.csv', \n",
        "  'se2016-umb-sp-16.csv', 'se2016-umb-sp-01-16.csv', 'se2016-umb-sp-02-16.csv', 'se2016-umb-sp-03-16.csv', 'se2016-umb-sp-17.csv', 'se2016-umb-sp-01-17.csv', 'se2016-umb-sp-02-17.csv', 'se2016-umb-sp-03-17.csv', 'se2016-umb-sp-18.csv', 'se2016-umb-sp-01-18.csv', 'se2016-umb-sp-02-18.csv', 'se2016-umb-sp-03-18.csv', 'se2016-umb-sp-19.csv', 'se2016-umb-sp-01-19.csv', 'se2016-umb-sp-02-19.csv', 'se2016-umb-sp-03-19.csv', 'se2016-umb-sp-21.csv', 'se2016-umb-sp-01-21.csv', 'se2016-umb-sp-02-21.csv', 'se2016-umb-sp-03-21.csv', \n",
        "  'se2016-umb-sp-31.csv', 'se2016-umb-sp-01-31.csv', 'se2016-umb-sp-02-31.csv', 'se2016-umb-sp-03-31.csv', 'se2016-umb-sp-32.csv', 'se2016-umb-sp-01-32.csv', 'se2016-umb-sp-02-32.csv', 'se2016-umb-sp-03-32.csv', 'se2016-umb-sp-33.csv', 'se2016-umb-sp-01-33.csv', 'se2016-umb-sp-02-33.csv', 'se2016-umb-sp-03-33.csv', 'se2016-umb-sp-34.csv', 'se2016-umb-sp-01-34.csv', 'se2016-umb-sp-02-34.csv', 'se2016-umb-sp-03-34.csv', 'se2016-umb-sp-35.csv', 'se2016-umb-sp-01-35.csv', 'se2016-umb-sp-02-35.csv', 'se2016-umb-sp-03-35.csv', \n",
        "  'se2016-umb-sp-36.csv', 'se2016-umb-sp-01-36.csv', 'se2016-umb-sp-02-36.csv', 'se2016-umb-sp-03-36.csv', 'se2016-umb-sp-51.csv', 'se2016-umb-sp-01-51.csv', 'se2016-umb-sp-02-51.csv', 'se2016-umb-sp-03-51.csv', 'se2016-umb-sp-52.csv', 'se2016-umb-sp-01-52.csv', 'se2016-umb-sp-02-52.csv', 'se2016-umb-sp-03-52.csv', 'se2016-umb-sp-53.csv', 'se2016-umb-sp-01-53.csv', 'se2016-umb-sp-02-53.csv', 'se2016-umb-sp-03-53.csv', 'se2016-umb-sp-61.csv', 'se2016-umb-sp-01-61.csv', 'se2016-umb-sp-02-61.csv', 'se2016-umb-sp-03-61.csv', \n",
        "  'se2016-umb-sp-62.csv', 'se2016-umb-sp-01-62.csv', 'se2016-umb-sp-02-62.csv', 'se2016-umb-sp-03-62.csv', 'se2016-umb-sp-63.csv', 'se2016-umb-sp-01-63.csv', 'se2016-umb-sp-02-63.csv', 'se2016-umb-sp-03-63.csv', 'se2016-umb-sp-64.csv', 'se2016-umb-sp-01-64.csv', 'se2016-umb-sp-02-64.csv', 'se2016-umb-sp-03-64.csv', 'se2016-umb-sp-65.csv', 'se2016-umb-sp-01-65.csv', 'se2016-umb-sp-02-65.csv', 'se2016-umb-sp-03-65.csv', 'se2016-umb-sp-71.csv', 'se2016-umb-sp-01-71.csv', 'se2016-umb-sp-02-71.csv', 'se2016-umb-sp-03-71.csv', \n",
        "  'se2016-umb-sp-72.csv', 'se2016-umb-sp-01-72.csv', 'se2016-umb-sp-02-72.csv', 'se2016-umb-sp-03-72.csv', 'se2016-umb-sp-73.csv', 'se2016-umb-sp-01-73.csv', 'se2016-umb-sp-02-73.csv', 'se2016-umb-sp-03-73.csv', 'se2016-umb-sp-74.csv', 'se2016-umb-sp-01-74.csv', 'se2016-umb-sp-02-74.csv', 'se2016-umb-sp-03-74.csv', 'se2016-umb-sp-75.csv', 'se2016-umb-sp-01-75.csv', 'se2016-umb-sp-02-75.csv', 'se2016-umb-sp-03-75.csv', 'se2016-umb-sp-76.csv', 'se2016-umb-sp-01-76.csv', 'se2016-umb-sp-02-76.csv', 'se2016-umb-sp-03-76.csv', \n",
        "  'se2016-umb-sp-81.csv', 'se2016-umb-sp-01-81.csv', 'se2016-umb-sp-02-81.csv', 'se2016-umb-sp-03-81.csv', 'se2016-umb-sp-82.csv', 'se2016-umb-sp-01-82.csv', 'se2016-umb-sp-02-82.csv', 'se2016-umb-sp-03-82.csv', 'se2016-umb-sp-91.csv', 'se2016-umb-sp-01-91.csv', 'se2016-umb-sp-02-91.csv', 'se2016-umb-sp-03-91.csv', 'se2016-umb-sp-94.csv', 'se2016-umb-sp-01-94.csv', 'se2016-umb-sp-02-94.csv', 'se2016-umb-sp-03-94.csv'\n",
        "]\n",
        "\n",
        "# Set working directory 01\n",
        "workingDirectory = dictDirectory['se-2016-umb-sp-data']\n",
        "os.chdir(workingDirectory)\n",
        "path = !pwd\n",
        "# print(path)\n",
        "\n",
        "loop = 0\n",
        "for i in range (0, 136, 4):\n",
        "  # print(listFile[i], \"merge with\", listFile[i+1], \"into\", listFile[i+2])\n",
        "  fname = listFile[i]     # Merge data files\n",
        "  fnameA = listFile[i+1]\n",
        "  fnameB  = listFile[i+2]\n",
        "  fnameC  = listFile[i+3]\n",
        "  pathSourceA = path[0] + '/data-01/' + fnameA\n",
        "  pathSourceB = path[0] + '/data-02/' + fnameB\n",
        "  pathSourceC = path[0] + '/data-03/' + fnameC\n",
        "  pathDestination = path[0] + '/' + fname\n",
        "  \n",
        "  # print(pathSourceA, pathSourceB, pathSourceC, pathDestination)\n",
        "  dfBPSDataA = pd.read_csv(pathSourceA)\n",
        "  dfBPSDataB = pd.read_csv(pathSourceB)\n",
        "  dfBPSDataC = pd.read_csv(pathSourceC)\n",
        "\n",
        "  # Rename joining keys\n",
        "  dfBPSDataA.rename({'IDPERUSAHA': 'PERUSAHAAN_ID', 'PROV': 'PROVINSI_IDA', 'SKALAUSAHA': 'SKALAUSAHA_A', 'WEIGHT': 'WEIGHT_A'}, axis='columns', inplace=True)\n",
        "  dfBPSDataB.rename({'IDPERUSAHA': 'PERUSAHAAN_ID', 'JENISKUESI': 'JENISKUESIONER_B', 'PROV': 'PROVINSI_IDB', 'SKALAUSAHA': 'SKALAUSAHA_B', 'WEIGHT': 'WEIGHT_B'}, axis='columns', inplace=True)\n",
        "  dfBPSDataC.rename({'IDPERUSAHA': 'PERUSAHAAN_ID', 'JENISKUESI': 'JENISKUESIONER_C', 'PROV': 'PROVINSI_IDC', 'SKALAUSAHA': 'SKALAUSAHA_C', 'WEIGHT': 'WEIGHT_C'}, axis='columns', inplace=True)\n",
        "  \n",
        "  dfBPSData = [dfBPSDataA, dfBPSDataB, dfBPSDataC]\n",
        "\n",
        "  # Merge data files\n",
        "  # dfBPSData = dfBPSDataA.merge(dfBPSDataB, left_on='IDPERUSAHAAN_A', right_on='IDPERUSAHAAN_B')\n",
        "  dfBPSDataMerge = reduce(lambda left,right: pd.merge(left,right,on='PERUSAHAAN_ID'), dfBPSData)\n",
        "  \n",
        "  # print(pathDestination, \"dfBPSDataA.shape:\", dfBPSDataA.shape, \"dfBPSDataB.shape:\", dfBPSDataB.shape)\n",
        "  # print(pathDestination, \"dfBPSDataA.shape:\", dfBPSDataA.shape, \"dfBPSDataB.shape:\", dfBPSDataB.shape, \"dfBPSDataMerge.shape:\", dfBPSData.shape)\n",
        "  # print(\"dfBPSDataA.shape:\", dfBPSDataA.shape[0], \"dfBPSDataB.shape:\", dfBPSDataB.shape[0], \"dfBPSDataMerge.shape:\", dfBPSData.shape[0])\n",
        "\n",
        "  # Save data from merge data type operation\n",
        "  print(pathDestination, dfBPSDataA.shape, dfBPSDataMerge.shape)\n",
        "  # print(fname, dfBPSDataA.shape, dfBPSDataMerge.shape)\n",
        "  dfBPSDataMerge.to_csv(pathDestination, encoding='utf-8', index=False)\n",
        "\n",
        "  loop += 1\n",
        "\n",
        "print(loop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZnUlRjp_hK-"
      },
      "source": [
        "%%time\n",
        "# Merge table se2016-umk\n",
        "# Set working directory 01\n",
        "workingDirectory = dictDirectory['se-2016-umb-sp-data']\n",
        "os.chdir(workingDirectory)\n",
        "path = !pwd\n",
        "# print(path)\n",
        "\n",
        "# List file on working directory 02\n",
        "listFile = [f for f in glob.glob('*.csv')]\n",
        "\n",
        "pathDestination = path[0] + '/' + 'se2016-umb-sp-merge.csv'\n",
        "dfMerges = []\n",
        "totalRows = 0\n",
        "loop = 0\n",
        "for e in listFile:\n",
        "  pathSource = path[0] + '/' + e\n",
        "  # print('merge ' + pathSource)\n",
        "\n",
        "  # Read and import csv file dataset into pandas data frame, change paths if needed\n",
        "  dfBPSData = pd.read_csv(pathSource)\n",
        "\n",
        "  dfMerges.append(dfBPSData)\n",
        "  print(\"dfBPSData.shape :\", e, dfBPSData.shape)\n",
        "  totalRows += dfBPSData.shape[0]\n",
        "  loop += 1\n",
        "\n",
        "print(loop)\n",
        "\n",
        "\n",
        "dfBPSDataMerge = pd.concat(dfMerges)\n",
        "print(\"dfBPSDataMerge.shape :\", dfBPSDataMerge.shape, totalRows)\n",
        "\n",
        "print(pathDestination)\n",
        "dfBPSDataMerge.to_csv(pathDestination, encoding='utf-8', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fjcS1i_x2zm"
      },
      "source": [
        "# Examine dataset, see data type\n",
        "dfBPSDataMerge.info(verbose=True, null_counts=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TDt3_F-1TEC"
      },
      "source": [
        "pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foPWry8Evs0o"
      },
      "source": [
        "%%time\n",
        "# Read and import csv file dataset into pandas data frame, change paths if needed\n",
        "\n",
        "# Set working directory 01\n",
        "os.chdir(dictDirectory['se-2016-umb-jk'])\n",
        "path = !pwd\n",
        "print(path)\n",
        "\n",
        "# List file on working directory 02\n",
        "listFile = [f for f in glob.glob('*.csv')]\n",
        "\n",
        "pathDestination = path[0] + '/' + 'se-2016-umb-jk-merge.csv'\n",
        "dfMerges = []\n",
        "totalRows = 0\n",
        "loop = 0\n",
        "for e in listFile:\n",
        "  pathSource = path[0] + '/' + e\n",
        "  # print('merge ' + pathSource)\n",
        "\n",
        "  # Read and import csv file dataset into pandas data frame, change paths if needed\n",
        "  dfBPSData = pd.read_csv(pathSource)\n",
        "\n",
        "  dfMerges.append(dfBPSData)\n",
        "  print(\"dfBPSData.shape :\", dfBPSData.shape)\n",
        "  totalRows += dfBPSData.shape[0]\n",
        "  # print(\"type(dfBPSData) :\", type(dfBPSData))\n",
        "  loop += 1\n",
        "\n",
        "print(loop)\n",
        "\n",
        "\n",
        "dfBPSDataMerge = pd.concat(dfMerges)\n",
        "print(\"dfBPSDataMerge.shape :\", dfBPSDataMerge.shape, totalRows)\n",
        "\n",
        "print(pathDestination)\n",
        "dfBPSDataMerge.to_csv(pathDestination, encoding='utf-8', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aI4eQISe648Z"
      },
      "source": [
        "## Step 0208 Create Data Description"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUewNpHp7Djt"
      },
      "source": [
        "# Sample json file for rawdata IBS 1993\n",
        "[\n",
        "\t{\n",
        "\t\t\"name\": \"DSTATS93\",\n",
        "\t\t\"type\": \"String\",\n",
        "\t\t\"description\": \"Status Permodalan\"\n",
        "\t},\n",
        "\t{\n",
        "\t\t\"name\": \"DETYPE93\",\n",
        "\t\t\"type\": \"String\",\n",
        "\t\t\"description\": \"Bentuk Badan Hukum\"\n",
        "\t},\n",
        "\t{\n",
        "\t\t\"name\": \"DPROVI93\",\n",
        "\t\t\"type\": \"String\",\n",
        "\t\t\"description\": \"Propinsi\"\n",
        "\t},\n",
        "\t{\n",
        "\t\t\"name\": \"DKABUP93\",\n",
        "\t\t\"type\": \"String\",\n",
        "\t\t\"description\": \"Kabupaten/Kotamadya\"\n",
        "\t},\n",
        "\t{\n",
        "\t\t\"name\": \"DSRVYR93\",\n",
        "\t\t\"type\": \"String\",\n",
        "\t\t\"description\": \"Tahun Survei\"\n",
        "\t},\n",
        "\t{\n",
        "\t\t\"name\": \"DYRSTR93\",\n",
        "\t\t\"type\": \"String\",\n",
        "\t\t\"description\": \"Tahun Mulai Produksi Komersial di Propinsi ini\"\n",
        "\t},\n",
        " \n",
        "...\n",
        "\n",
        "\t{\n",
        "\t\t\"name\": \"LPDNOU93\",\n",
        "\t\t\"type\": \"Integer\",\n",
        "\t\t\"description\": \"Jumlah Banyaknya Pekerja/Karyawan Pekerja (Produksi + Lainnya) (Laki-laki + Perempuan) dibayar rata-rata setiap bulan\"\n",
        "\t},\n",
        "\t{\n",
        "\t\t\"name\": \"LTLNOU93\",\n",
        "\t\t\"type\": \"Integer\",\n",
        "\t\t\"description\": \"Jumlah Banyaknya Pekerja/Karyawan Pekerja (Produksi + Lainnya) (dibayar + tidak dibayar) (Laki-laki + Perempuan) rata-rata setiap bulan\"\n",
        "\t},\n",
        "\n",
        " ...\n",
        "\n",
        "\t{\n",
        "\t\t\"name\": \"EWOVCE93\",\n",
        "\t\t\"type\": \"Integer\",\n",
        "\t\t\"description\": \"Nilai Kayu Bakar dipakai selama tahun 1993 (Pembangkit Listrik)\"\n",
        "\t},\n",
        "\t{\n",
        "\t\t\"name\": \"ENCVCE93\",\n",
        "\t\t\"type\": \"Integer\",\n",
        "\t\t\"description\": \"Nilai Bahan Bakar Lainnya dipakai selama tahun 1993 (Pembangkit Listrik)\"\n",
        "\t},\n",
        "\t{\n",
        "\t\t\"name\": \"ETLQUE93\",\n",
        "\t\t\"type\": \"Integer\",\n",
        "\t\t\"description\": \"Banyaknya Bahan Bakar Lainnya dipakai selama tahun 1993 (Pembangkit Listrik)\"\n",
        "\t},\n",
        "\t{\n",
        "\t\t\"name\": \"NST93\",\n",
        "\t\t\"type\": \"String\",\n",
        "\t\t\"description\": \"NST93 Variabel tidak digunakan\"\n",
        "\t},\n",
        "\t{\n",
        "\t\t\"name\": \"PSID\",\n",
        "\t\t\"type\": \"Integer\",\n",
        "\t\t\"description\": \"PSID Variabel\"\n",
        "\t}\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaN2SrfQsBwO"
      },
      "source": [
        "# Step 03 - Data Preparation\n",
        "In this step, we pre-process the data, clean it, wrangle it, and\n",
        "manipulate it as needed. Initial exploratory data analysis is also carried out.\n",
        "* **Data Processing & Wrangling**: \n",
        "  Mainly concerned with data processing, cleaning, munging, wrangling and performing initial descriptive and exploratory data analysis\n",
        "* **Feature Extraction & Engineering**: Here, we extract important features or attributes from the raw data and even create or engineer new features from existing features.\n",
        "* **Feature Scaling & Selection**: Data features often need to be normalized and scaled to prevent Machine Learning algorithms from getting biased. Besides this, often we need to select a subset of all available features based on feature importance and quality.\n",
        "\n",
        "Final Update 20200315"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gLlMZJdsBwP"
      },
      "source": [
        "## Step 0301 Dataset Summary Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qb8GwcDVsBwQ"
      },
      "source": [
        "# Examine dataset, shape, rows and columns\n",
        "print(\"dfTrain shape   :\", dfTrain.shape)\n",
        "print(\"type(dfTrain)   :\", type(dfTrain))\n",
        "print(\"dfTrain.index   :\", dfTrain.index)\n",
        "print(\"dfTrain.columns :\", dfTrain.columns, \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7YKu4bCsBwU"
      },
      "source": [
        "# Examine dataset, first 5 rows\n",
        "# dfTrain.head()\n",
        "dfTrain.head().T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snS2w77YsBwX"
      },
      "source": [
        "# Examine dataset, types of all features and total dataframe size in memory\n",
        "dfTrain.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxoJLmrksBwb"
      },
      "source": [
        "# Examine dataset, types of all features and total dataframe size in memory\n",
        "dfTrain.describe().T\n",
        "# dfTrain.describe(include='all').T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UkNenLVsBwf"
      },
      "source": [
        "dfTrain.columns.isna().any()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8PxB-22sBw9"
      },
      "source": [
        "\n",
        "# Step 04 - Deployment and Monitoring\n",
        "Datawarehouse are deployed in production and are constantly monitored based on their performance and transformation.\n",
        "\n",
        "Final Update 20200315"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-WBgjeTnOT8"
      },
      "source": [
        "%%time\n",
        "# Big Query delete table se2016-listing\n",
        "listBQFile = [\n",
        "  'se_2016_listing_11', 'se_2016_listing_12', 'se_2016_listing_13', 'se_2016_listing_14', 'se_2016_listing_15', \n",
        "  'se_2016_listing_16', 'se_2016_listing_17', 'se_2016_listing_18', 'se_2016_listing_19', 'se_2016_listing_21', \n",
        "  'se_2016_listing_31', 'se_2016_listing_32', 'se_2016_listing_33', 'se_2016_listing_34', 'se_2016_listing_35', \n",
        "  'se_2016_listing_36', 'se_2016_listing_51', 'se_2016_listing_52', 'se_2016_listing_53', 'se_2016_listing_61', \n",
        "  'se_2016_listing_62', 'se_2016_listing_63', 'se_2016_listing_64', 'se_2016_listing_65', 'se_2016_listing_71', \n",
        "  'se_2016_listing_72', 'se_2016_listing_73', 'se_2016_listing_74', 'se_2016_listing_75', 'se_2016_listing_76', \n",
        "  'se_2016_listing_81', 'se_2016_listing_82', 'se_2016_listing_91', 'se_2016_listing_94', 'se_2016_listing_merge' \n",
        "]\n",
        "\n",
        "# Set working directory on Google Big Query 01\n",
        "projectId = 'datawarehouse-001'\n",
        "directoryBQ = ['datawarehouse-001:04_sensus_ekonomi', 'datawarehouse-001:04_sensus_ekonomi_rawdata']\n",
        "\n",
        "# List file on Google Big Query working directory 02\n",
        "# !bq ls --max_results=1000 {directoryBQ[0]}\n",
        "\n",
        "# !bq rm --help\n",
        "\n",
        "loop = 0\n",
        "for e in listBQFile:\n",
        "  bqFileName = directoryBQ[0] + \".\" + e\n",
        "  # !bq rm -f -t {bqFileName}\n",
        "  print(bqFileName)\n",
        "  loop += 1\n",
        "\n",
        "print(loop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oa_1isjNsBw-"
      },
      "source": [
        "bebek = 100"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
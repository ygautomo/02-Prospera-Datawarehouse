{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "DW Prospera - AWS S3 Processing Files",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ygautomo/02-Prospera-Datawarehouse/blob/master/DW_Prospera_AWS_S3_Processing_Files.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZBBBH88sBv3"
      },
      "source": [
        "# **01- Data Warehouse Prospera- AWS S3 Processing Files**\n",
        "## Data Warehouse Prospera Steps and Code\n",
        "### Status : Last Update 20201001"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9jcGOKSsBv6"
      },
      "source": [
        "## **Python Environment Setup**\n",
        "We will be using a several different libraries throughout this steps. If you've successfully completed the [installation instructions](https://github.com/cs109/content/wiki/Installing-Python), all of the following statements should run."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYojFb3vUSbm"
      },
      "source": [
        "### Setup Python Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmTtlwW1sBv8",
        "outputId": "85799107-48fa-4c41-8a4e-6e7d7b85e44d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# Final Update 20200301\n",
        "# System-specific parameters and functions module provides the version number of the Python interpreter\n",
        "import sys\n",
        "print(\"Python version:        %6.6s(need at least 3.5.0)\" % sys.version)\n",
        "\n",
        "# IPython provides a rich architecture for interactive computing\n",
        "import IPython\n",
        "print(\"IPython version:      %6.6s (need at least 6.0.0)\" % IPython.__version__)     # (need at least 1.0.0)\n",
        "\n",
        "# Mathematical functions module provides access to the mathematical functions defined by the C standard\n",
        "# import math\n",
        "\n",
        "# Matplotlib is a Python 2D plotting library which produces publication quality figures in a variety of hardcopy formats \n",
        "# and interactive environments across platforms.\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "print(\"Mapltolib version:    %6.6s (need at least 3.0.0)\" % matplotlib.__version__)   # (need at least 1.2.1)\n",
        "\n",
        "# NumPy is the fundamental package for scientific computing with Python\n",
        "import numpy as np\n",
        "print(\"Numpy version:        %6.6s (need at least 1.15.0)\" % np.__version__)         # (need at least 1.7.1)\n",
        "\n",
        "# Pandas is a library providing high-performance, easy-to-use data structures and data analysis tools for Python\n",
        "import pandas as pd\n",
        "print(\"Pandas version:       %6.6s (need at least 0.20.0)\" % pd.__version__)         # (need at least 0.11.0)\n",
        "\n",
        "# Generate pseudo-random numbers. This module implements pseudo-random number generators for various distributions.\n",
        "# import random\n",
        "\n",
        "# Scikit-Learn a Machine Learning library in Python. Simple and efficient tools for data mining and data analysis\n",
        "import sklearn as sk\n",
        "print(\"Scikit-Learn version: %6.6s (need at least 0.15.0)\" % sk.__version__)         # (need at least 0.5.0)\n",
        "\n",
        "# Seaborn is a Python data visualization library based on matplotlib\n",
        "import seaborn as sns\n",
        "print(\"Seaborn version:      %6.6s (need at least 0.5.0)\" % sns.__version__)         # (need at least 0.5.0)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python version:        3.6.9 (need at least 3.5.0)\n",
            "IPython version:       5.5.0 (need at least 6.0.0)\n",
            "Mapltolib version:     3.2.2 (need at least 3.0.0)\n",
            "Numpy version:        1.18.5 (need at least 1.15.0)\n",
            "Pandas version:        1.1.2 (need at least 0.20.0)\n",
            "Scikit-Learn version: 0.22.2 (need at least 0.15.0)\n",
            "Seaborn version:      0.11.0 (need at least 0.5.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewifNF5csBwC",
        "outputId": "23a52b99-5c0e-41f8-ff86-c4e56b7409ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        }
      },
      "source": [
        "# Customized python environment Setup\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# from matplotlib import pyplot as plt\n",
        "pd.set_option('display.precision', 2)\n",
        "\n",
        "# Module for higher-order functions\n",
        "from functools import reduce\n",
        "\n",
        "# Unix style pathname pattern expansion\n",
        "import glob\n",
        "\n",
        "# Miscellaneous operating system interfaces\n",
        "import os\n",
        "\n",
        "# Pure python package for reading/writing dBase, FoxPro, and Visual FoxPro .dbf files (including memos)\n",
        "!pip install dbf\n",
        "import dbf\n",
        "\n",
        "# Convert DBF files to CSV, DataFrames, HDF5 tables, and SQL tables. Python3 compatible.\n",
        "!pip install simpledbf\n",
        "from simpledbf import Dbf5"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting dbf\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/e1/192378726e2570c47d8f5c028897f7c896762729605629fc63588f32d3aa/dbf-0.99.0-py3-none-any.whl (71kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 1.9MB/s \n",
            "\u001b[?25hCollecting aenum\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5b/58/1007ed4a287c6dd3dc953ae478145bab28e8abc529627b2ee797f84b201e/aenum-2.2.4-py3-none-any.whl (41kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.8MB/s \n",
            "\u001b[?25hInstalling collected packages: aenum, dbf\n",
            "Successfully installed aenum-2.2.4 dbf-0.99.0\n",
            "Collecting simpledbf\n",
            "  Downloading https://files.pythonhosted.org/packages/8a/d3/e4c25cd8f739dd7ddd19c255cd5552e08cdd439ac51a36ae12640ce8a748/simpledbf-0.2.6.tar.gz\n",
            "Building wheels for collected packages: simpledbf\n",
            "  Building wheel for simpledbf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for simpledbf: filename=simpledbf-0.2.6-cp36-none-any.whl size=13794 sha256=69ea18f10602c53499052316b25332d33785b78b5b5822c28d3ee226e2edff99\n",
            "  Stored in directory: /root/.cache/pip/wheels/72/dd/df/cdfb970a508ef9750ebb4b6e035a3410c3d62b5a6a91d2aafc\n",
            "Successfully built simpledbf\n",
            "Installing collected packages: simpledbf\n",
            "Successfully installed simpledbf-0.2.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBxS6bCAsBwF"
      },
      "source": [
        "# **Machine Learning Pipeline:**\n",
        "![alt text](https://drive.google.com/uc?id=1zUK9aLiPk1zReXV19RMUQjqe3BrcvbyM)\n",
        "\n",
        "# **Step 01 - Project Goals & Problems**\n",
        "* Develop Datawarehouse for Prospera, which data is taken from Egnyte nad transform the data into Google BigQuery as Datawarehouse Platform.\n",
        "\n",
        "# **Step 02 - Data Retrieval**\n",
        "Data retrieval: This is mainly data collection, extraction, and acquisition from various data sources and data stores.\n",
        "\n",
        "Data retrieval process: \n",
        "1. Take raw data from Egnyte\n",
        "2. Standardize file name (linux file system)\n",
        "3. Convert into csv files\n",
        "4. Check and review data\n",
        "5. Convert data type if neccessary\n",
        "6. Merge data if necessary\n",
        "7. Create data description and save into json\n",
        "8. Put raw data into Google Cloud Storage\n",
        "9. Upload and transform the data into Google BigQuery\n",
        "\n",
        "Final Update 20200401"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tXrBSQzUDoQ"
      },
      "source": [
        "### Step 0201 Mount AWS S3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWNH0VHoYhdk",
        "outputId": "de6b8077-5fa5-4654-dcc1-4890cc1ee0ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AiV79IOfksO"
      },
      "source": [
        "!pip install boto3\n",
        "# import boto3\n",
        "# s3_client = boto3.client('s3')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqkzXseGs9gw",
        "outputId": "34c2a7bb-0ceb-4e58-a35b-e26018dbcc68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!gcloud init"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Welcome! This command will take you through the configuration of gcloud.\n",
            "\n",
            "Settings from your current configuration [default] are:\n",
            "component_manager:\n",
            "  disable_update_check: 'True'\n",
            "compute:\n",
            "  gce_metadata_read_timeout_sec: '0'\n",
            "\n",
            "Pick configuration to use:\n",
            " [1] Re-initialize this configuration [default] with new settings \n",
            " [2] Create a new configuration\n",
            "Please enter your numeric choice:  1\n",
            "\n",
            "Your current configuration has been set to: [default]\n",
            "\n",
            "You can skip diagnostics next time by using the following flag:\n",
            "  gcloud init --skip-diagnostics\n",
            "\n",
            "Network diagnostic detects and fixes local network connection issues.\n",
            "Reachability Check passed.\n",
            "Network diagnostic passed (1/1 checks passed).\n",
            "\n",
            "You must log in to continue. Would you like to log in (Y/n)?  Y\n",
            "\n",
            "Go to the following link in your browser:\n",
            "\n",
            "    https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&code_challenge=wvQtOhKfWH85dTKX4K_p6M2uimZnSaNXSWomDOcM2Kg&code_challenge_method=S256&access_type=offline&response_type=code&prompt=select_account\n",
            "\n",
            "\n",
            "Enter verification code: 4/1AfDhmrioO27gxUcJUGK6lc4_eeI4UWk-kjuWhCSGHchvEYzizeB60a9sHNo\n",
            "You are logged in as: [y.gautomo@gmail.com].\n",
            "\n",
            "Pick cloud project to use: \n",
            " [1] appointmentschduler-jlhh\n",
            " [2] coursera-20200101\n",
            " [3] coursera-bigquery-260614\n",
            " [4] data-warehouse-252102\n",
            " [5] datawarehouse-001\n",
            " [6] dflow-covid19-agent-200801\n",
            " [7] dflow-prospera-agent-201001\n",
            " [8] dflow-survey-agent-200901\n",
            " [9] dialogflow-bikeshop-agent\n",
            " [10] ethereal-atlas-285208\n",
            " [11] hadoop-cluster-160709\n",
            " [12] prospera-260908\n",
            " [13] prospera-datawarehouse-201014\n",
            " [14] quickstart-1594318864622\n",
            " [15] savvy-fountain-264606\n",
            " [16] secret-imprint-159309\n",
            " [17] Create a new project\n",
            "Please enter numeric choice or text value (must exactly match list \n",
            "item):  13\n",
            "\n",
            "Your current project has been set to: [prospera-datawarehouse-201014].\n",
            "\n",
            "Not setting default zone/region (this feature makes it easier to use\n",
            "[gcloud compute] by setting an appropriate default value for the\n",
            "--zone and --region flag).\n",
            "See https://cloud.google.com/compute/docs/gcloud-compute section on how to set\n",
            "default compute region and zone manually. If you would like [gcloud init] to be\n",
            "able to do this for you the next time you run it, make sure the\n",
            "Compute Engine API is enabled for your project on the\n",
            "https://console.developers.google.com/apis page.\n",
            "\n",
            "Your Google Cloud SDK is configured and ready to use!\n",
            "\n",
            "* Commands that require authentication will use y.gautomo@gmail.com by default\n",
            "* Commands will reference project `prospera-datawarehouse-201014` by default\n",
            "Run `gcloud help config` to learn how to change individual settings\n",
            "\n",
            "This gcloud configuration is called [default]. You can create additional configurations if you work with multiple accounts and/or projects.\n",
            "Run `gcloud topic configurations` to learn more.\n",
            "\n",
            "Some things to try next:\n",
            "\n",
            "* Run `gcloud --help` to see the Cloud Platform services you can interact with. And run `gcloud help COMMAND` to get help on any gcloud command.\n",
            "* Run `gcloud topic --help` to learn about advanced features of the SDK like arg files and output formatting\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyqoOQ03Nvgd",
        "outputId": "2513acae-75eb-4e0d-ced9-345f3b2cbaec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Mount AWS S3\n",
        "\n",
        "# reference https://realpython.com/python-boto3-aws-s3/\n",
        "# Add Credentials within, .boto files\n",
        "# [Credentials]\n",
        "# aws_access_key_id = AKIAJ23E6HNMRJM3UVPA\n",
        "# aws_secret_access_key = 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\n",
        "# [s3]\n",
        "# use-sigv4=True\n",
        "# host=s3.us-east-2.amazonaws.com\n",
        "\n",
        "# !cat '/content/drive/My Drive/07 Google Colab/.boto'\n",
        "\n",
        "# !gcloud init\n",
        "# !gsutil config\n",
        "# !gsutil version -l\n",
        "\n",
        "# !gsutil cp '/content/drive/My Drive/07 Google Colab/.boto' '/content/.config/legacy_credentials/data@prospera.or.id/.boto'\n",
        "\n",
        "# update .boto files\n",
        "# !cd '/content/.config/legacy_credentials/data@prospera.or.id'\n",
        "# !cd '/content/.config/legacy_credentials/y.gautomo@gmail.com/'\n",
        "!cat '/root/.boto'\n",
        "# !gsutil rm '/root/.boto'\n",
        "# !cd /root/\n",
        "# !ls\n",
        "# !pwd\n",
        "\n",
        "# !cat '/content/.config/legacy_credentials/data@prospera.or.id/.boto'\n",
        "# print('\\n')\n",
        "\n",
        "# !gsutil cp '/content/drive/My Drive/Database/.boto' '/root/.boto'\n",
        "\n",
        "# bucketID = 'bucket-prospera-01'\n",
        "# !gsutil ls s3://{bucketID}"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# This file contains credentials and other configuration information needed\n",
            "# by the boto library, used by gsutil. You can edit this file (e.g., to add\n",
            "# credentials) but be careful not to mis-edit any of the variable names (like\n",
            "# \"gs_access_key_id\") or remove important markers (like the \"[Credentials]\" and\n",
            "# \"[Boto]\" section delimiters).\n",
            "#\n",
            "# This file was created by gsutil version 4.53 at 2020-10-25 16:27:23.\n",
            "#\n",
            "# You can create additional configuration files by running\n",
            "# gsutil config [options] [-o <config-file>]\n",
            "\n",
            "\n",
            "[Credentials]\n",
            "\n",
            "# To add Google OAuth2 credentials (\"gs://\" URIs), edit and uncomment the\n",
            "# following line:\n",
            "#gs_oauth2_refresh_token = <your OAuth2 refresh token>\n",
            "\n",
            "# To add HMAC aws credentials for \"s3://\" URIs, edit and uncomment the\n",
            "# following two lines:\n",
            "#aws_access_key_id = <your aws access key ID>\n",
            "#aws_secret_access_key = <your aws secret access key>\n",
            "# The ability to specify an alternate storage host and port\n",
            "# is primarily for cloud storage service developers.\n",
            "# Setting a non-default gs_host only works if prefer_api=xml.\n",
            "#s3_host = <alternate storage host address>\n",
            "#s3_port = <alternate storage host port>\n",
            "# In some cases, (e.g. VPC requests) the \"host\" HTTP header should\n",
            "# be different than the host used in the request URL.\n",
            "#s3_host_header = <alternate storage host header>\n",
            "\n",
            "# google credentials (\"gs://\" URIs):\n",
            "gs_access_key_id = AKIAJ5K7HVWRGLVIVATA\n",
            "gs_secret_access_key = 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\n",
            "# The ability to specify an alternate storage host and port\n",
            "# is primarily for cloud storage service developers.\n",
            "# Setting a non-default gs_host only works if prefer_api=xml.\n",
            "#gs_host = <alternate storage host address>\n",
            "#gs_port = <alternate storage host port>\n",
            "# In some cases, (e.g. VPC requests) the \"host\" HTTP header should\n",
            "# be different than the host used in the request URL.\n",
            "#gs_host_header = <alternate storage host header>\n",
            "#gs_json_host = <alternate JSON API storage host address>\n",
            "#gs_json_port = <alternate JSON API storage host port>\n",
            "#gs_json_host_header = <alternate JSON API storage host header>\n",
            "\n",
            "# To impersonate a service account for \"%s://\" URIs over\n",
            "# JSON API, edit and uncomment the following line:\n",
            "#%s_impersonate_service_account = <service account email>\n",
            "\n",
            "\n",
            "\n",
            "[Boto]\n",
            "\n",
            "# http_socket_timeout specifies the timeout (in seconds) used to tell httplib\n",
            "# how long to wait for socket timeouts. The default is 70 seconds. Note that\n",
            "# this timeout only applies to httplib, not to httplib2 (which is used for\n",
            "# OAuth2 refresh/access token exchanges).\n",
            "#http_socket_timeout = 70\n",
            "\n",
            "# The following two options control the use of a secure transport for requests\n",
            "# to S3 and Google Cloud Storage. It is highly recommended to set both options\n",
            "# to True in production environments, especially when using OAuth2 bearer token\n",
            "# authentication with Google Cloud Storage.\n",
            "\n",
            "# Set 'https_validate_certificates' to False to disable server certificate\n",
            "# checking. The default for this option in the boto library is currently\n",
            "# 'False' (to avoid breaking apps that depend on invalid certificates); it is\n",
            "# therefore strongly recommended to always set this option explicitly to True\n",
            "# in configuration files, to protect against \"man-in-the-middle\" attacks.\n",
            "https_validate_certificates = True\n",
            "\n",
            "# 'debug' controls the level of debug messages printed for the XML API only:\n",
            "# 0 for none, 1 for basic boto debug, 2 for all boto debug plus HTTP\n",
            "# requests/responses.\n",
            "#debug = <0, 1, or 2>\n",
            "\n",
            "# 'num_retries' controls the number of retry attempts made when errors occur\n",
            "# during data transfers. The default is 6.\n",
            "# Note 1: You can cause gsutil to retry failures effectively infinitely by\n",
            "# setting this value to a large number (like 10000). Doing that could be useful\n",
            "# in cases where your network connection occasionally fails and is down for an\n",
            "# extended period of time, because when it comes back up gsutil will continue\n",
            "# retrying.  However, in general we recommend not setting the value above 10,\n",
            "# because otherwise gsutil could appear to \"hang\" due to excessive retries\n",
            "# (since unless you run gsutil -D you won't see any logged evidence that gsutil\n",
            "# is retrying).\n",
            "# Note 2: Don't set this value to 0, as it will cause boto to fail when reusing\n",
            "# HTTP connections.\n",
            "#num_retries = <integer value>\n",
            "\n",
            "# 'max_retry_delay' controls the max delay (in seconds) between retries. The\n",
            "# default value is 60, so the backoff sequence will be 1 seconds, 2 seconds, 4,\n",
            "# 8, 16, 32, and then 60 for all subsequent retries for a given HTTP request.\n",
            "# Note: At present this value only impacts the XML API and the JSON API uses a\n",
            "# fixed value of 60.\n",
            "#max_retry_delay = <integer value>\n",
            "\n",
            "# To use a proxy, edit and uncomment the proxy and proxy_port lines.\n",
            "# If you need a user/password with this proxy, edit and uncomment\n",
            "# those lines as well. If your organization also disallows DNS\n",
            "# lookups by client machines, set proxy_rdns to True (the default).\n",
            "# If you have installed gsutil through the Cloud SDK and have \n",
            "# configured proxy settings in gcloud, those proxy settings will \n",
            "# override any other options (including those set here, along with \n",
            "# any settings in proxy-related environment variables). Otherwise, \n",
            "# if proxy_host and proxy_port are not specified in this file and\n",
            "# one of the OS environment variables http_proxy, https_proxy, or\n",
            "# HTTPS_PROXY is defined, gsutil will use the proxy server specified\n",
            "# in these environment variables, in order of precedence according\n",
            "# to how they are listed above.\n",
            "#proxy = <proxy host>\n",
            "#proxy_type = <proxy type (socks4, socks5, http) | Defaults to http>\n",
            "#proxy_port = <proxy port>\n",
            "#proxy_user = <proxy user>\n",
            "#proxy_pass = <proxy password>\n",
            "#proxy_rdns = <let proxy server perform DNS lookups (True,False); socks proxy not supported>\n",
            "\n",
            "[GoogleCompute]\n",
            "\n",
            "# 'service_account' specifies the a Google Compute Engine service account to\n",
            "# use for credentials. This value is intended for use only on Google Compute\n",
            "# Engine virtual machines and usually lives in /etc/boto.cfg. Most users\n",
            "# shouldn't need to edit this part of the config.\n",
            "#service_account = default\n",
            "\n",
            "[GSUtil]\n",
            "\n",
            "# 'resumable_threshold' specifies the smallest file size [bytes] for which\n",
            "# resumable Google Cloud Storage uploads are attempted. The default is 8388608\n",
            "# (8 MiB).\n",
            "#resumable_threshold = 8388608\n",
            "\n",
            "# 'rsync_buffer_lines' specifies the number of lines of bucket or directory\n",
            "# listings saved in each temp file during sorting. (The complete set is\n",
            "# split across temp files and separately sorted/merged, to avoid needing to\n",
            "# fit everything in memory at once.) If you are trying to synchronize very\n",
            "# large directories/buckets (e.g., containing millions or more objects),\n",
            "# having too small a value here can cause gsutil to run out of open file\n",
            "# handles. If that happens, you can try to increase the number of open file\n",
            "# handles your system allows (e.g., see 'man ulimit' on Linux; see also\n",
            "# http://docs.python.org/2/library/resource.html). If you can't do that (or\n",
            "# if you're already at the upper limit), increasing rsync_buffer_lines will\n",
            "# cause gsutil to use fewer file handles, but at the cost of more memory. With\n",
            "# rsync_buffer_lines set to 32000 and assuming a typical URL is 100 bytes\n",
            "# long, gsutil will require approximately 10 MiB of memory while building\n",
            "# the synchronization state, and will require approximately 60 open file\n",
            "# descriptors to build the synchronization state over all 1M source and 1M\n",
            "# destination URLs. Memory and file descriptors are only consumed while\n",
            "# building the state; once the state is built, it resides in two temp files that\n",
            "# are read and processed incrementally during the actual copy/delete\n",
            "# operations.\n",
            "#rsync_buffer_lines = 32000\n",
            "\n",
            "# 'state_dir' specifies the base location where files that\n",
            "# need a static location are stored, such as pointers to credentials,\n",
            "# resumable transfer tracker files, and the last software update check.\n",
            "# By default these files are stored in ~/.gsutil\n",
            "#state_dir = <file_path>\n",
            "# gsutil periodically checks whether a new version of the gsutil software is\n",
            "# available. 'software_update_check_period' specifies the number of days\n",
            "# between such checks. The default is 30. Setting the value to 0 disables\n",
            "# periodic software update checks.\n",
            "#software_update_check_period = 30\n",
            "\n",
            "# 'tab_completion_timeout' controls the timeout (in seconds) for tab\n",
            "# completions that involve remote requests (such as bucket or object names).\n",
            "# If tab completion does not succeed within this timeout, no tab completion\n",
            "# suggestions will be returned.\n",
            "# A value of 0 will disable completions that involve remote requests.\n",
            "#tab_completion_timeout = 5\n",
            "\n",
            "# 'parallel_process_count' and 'parallel_thread_count' specify the number\n",
            "# of OS processes and Python threads, respectively, to use when executing\n",
            "# operations in parallel. The default settings should work well as configured,\n",
            "# however, to enhance performance for transfers involving large numbers of\n",
            "# files, you may experiment with hand tuning these values to optimize\n",
            "# performance for your particular system configuration.\n",
            "#parallel_process_count = 2\n",
            "#parallel_thread_count = 5\n",
            "\n",
            "# 'parallel_composite_upload_threshold' specifies the maximum size of a file to\n",
            "# upload in a single stream. Files larger than this threshold will be\n",
            "# partitioned into component parts and uploaded in parallel and then composed\n",
            "# into a single object.\n",
            "# The number of components will be the smaller of\n",
            "# ceil(file_size / parallel_composite_upload_component_size) and\n",
            "# MAX_COMPONENT_COUNT. The current value of MAX_COMPONENT_COUNT is\n",
            "# 1024.\n",
            "# If 'parallel_composite_upload_threshold' is set to 0, then automatic parallel\n",
            "# uploads will never occur.\n",
            "# Setting an extremely low threshold is unadvisable. The vast majority of\n",
            "# environments will see degraded performance for thresholds below 80M, and it\n",
            "# is almost never advantageous to have a threshold below 20M.\n",
            "# 'parallel_composite_upload_component_size' specifies the ideal size of a\n",
            "# component in bytes, which will act as an upper bound to the size of the\n",
            "# components if ceil(file_size / parallel_composite_upload_component_size) is\n",
            "# less than MAX_COMPONENT_COUNT.\n",
            "# Values can be provided either in bytes or as human-readable values\n",
            "# (e.g., \"150M\" to represent 150 mebibytes)\n",
            "#\n",
            "# Note: At present parallel composite uploads are disabled by default, because\n",
            "# using composite objects requires a compiled crcmod (see \"gsutil help crcmod\"),\n",
            "# and for operating systems that don't already have this package installed this\n",
            "# makes gsutil harder to use. Google is actively working with a number of the\n",
            "# Linux distributions to get crcmod included with the stock distribution. Once\n",
            "# that is done we will re-enable parallel composite uploads by default in\n",
            "# gsutil.\n",
            "#\n",
            "# Note: Parallel composite uploads should not be used with NEARLINE, COLDLINE,\n",
            "# or ARCHIVE storage class buckets, as doing this incurs an early deletion\n",
            "# charge for each component object.\n",
            "#\n",
            "# Note: Parallel composite uploads are not enabled with Cloud KMS encrypted\n",
            "# objects as a source or destination, as composition with KMS objects is not yet\n",
            "# supported.\n",
            "\n",
            "#parallel_composite_upload_threshold = 0\n",
            "#parallel_composite_upload_component_size = 50M\n",
            "\n",
            "# 'sliced_object_download_threshold' and\n",
            "# 'sliced_object_download_component_size' have analogous functionality to\n",
            "# their respective parallel_composite_upload config values.\n",
            "# 'sliced_object_download_max_components' specifies the maximum number of\n",
            "# slices to be used when performing a sliced object download. It is not\n",
            "# restricted by MAX_COMPONENT_COUNT.\n",
            "#sliced_object_download_threshold = 0\n",
            "#sliced_object_download_component_size = 50M\n",
            "#sliced_object_download_max_components = 4\n",
            "\n",
            "# Compressed transport encoded uploads buffer chunks of compressed data. When\n",
            "# running a composite upload and/or many uploads in parallel, compression may\n",
            "# consume more memory than available. This setting restricts the number of\n",
            "# compressed transport encoded uploads running in parallel such that they\n",
            "# don't consume more memory than set here. This is 2GiB by default.\n",
            "# Values can be provided either in bytes or as human-readable values\n",
            "# (e.g., \"2G\" to represent 2 gibibytes)\n",
            "#max_upload_compression_buffer_size = 2G\n",
            "\n",
            "# GZIP compression level, if using compression. Reducing this can have\n",
            "# a dramatic impact on compression speed with minor size increases.\n",
            "# This is a value from 0-9, with 9 being max compression.\n",
            "# A good level to try is 6, which is the default used by the gzip tool.\n",
            "#gzip_compression_level = 9\n",
            "\n",
            "# 'task_estimation_threshold' controls how many files or objects gsutil\n",
            "# processes before it attempts to estimate the total work that will be\n",
            "# performed by the command. Estimation makes extra directory listing or API\n",
            "# list calls and is performed only if multiple processes and/or threads are\n",
            "# used. Estimation can slightly increase cost due to extra\n",
            "# listing calls; to disable it entirely, set this value to 0.\n",
            "#task_estimation_threshold=30000\n",
            "\n",
            "# 'use_magicfile' specifies if the 'file --mime <filename>' command should be\n",
            "# used to guess content types instead of the default filename extension-based\n",
            "# mechanism. Available on UNIX and macOS (and possibly on Windows, if you're\n",
            "# running Cygwin or some other package that provides implementations of\n",
            "# UNIX-like commands). When available and enabled use_magicfile should be more\n",
            "# robust because it analyzes file contents in addition to extensions.\n",
            "#use_magicfile = False\n",
            "\n",
            "# Service account emails for testing the hmac command. If these fields are not\n",
            "# populated with distinct service accounts the tests for the hmac command will\n",
            "# not be run.  Primarily useful for tool developers.\n",
            "#test_hmac_service_account =\n",
            "#test_hmac_alt_service_account =\n",
            "#test_hmac_list_service_account =\n",
            "\n",
            "# Service account emails for testing impersonation credentials. If this field is\n",
            "# not populated with a service account the tests for service account\n",
            "# impersonation will not run.  Primarily useful for tool developers.\n",
            "#test_impersonate_service_account =\n",
            "\n",
            "# 'content_language' specifies the ISO 639-1 language code of the content, to be\n",
            "# passed in the Content-Language header. By default no Content-Language is sent.\n",
            "# See the ISO 639-1 column of\n",
            "# http://www.loc.gov/standards/iso639-2/php/code_list.php for a list of\n",
            "# language codes.\n",
            "content_language = en\n",
            "\n",
            "# 'check_hashes' specifies how strictly to require integrity checking for\n",
            "# downloaded data. Legal values are:\n",
            "#   'if_fast_else_fail' - (default) Only integrity check if the digest\n",
            "#       will run efficiently (using compiled code), else fail the download.\n",
            "#   'if_fast_else_skip' - Only integrity check if the server supplies a\n",
            "#       hash and the local digest computation will run quickly, else skip the\n",
            "#       check.\n",
            "#   'always' - Always check download integrity regardless of possible\n",
            "#       performance costs.\n",
            "#   'never' - Don't perform download integrity checks. This setting is\n",
            "#       not recommended except for special cases such as measuring download\n",
            "#       performance excluding time for integrity checking.\n",
            "# This option exists to assist users who wish to download a GCS composite object\n",
            "# and are unable to install crcmod with the C-extension. CRC32c is the only\n",
            "# available integrity check for composite objects, and without the C-extension,\n",
            "# download performance can be significantly degraded by the digest computation.\n",
            "# This option is ignored for daisy-chain copies, which don't compute hashes but\n",
            "# instead (inexpensively) compare the cloud source and destination hashes.\n",
            "#check_hashes = if_fast_else_fail\n",
            "\n",
            "# 'encryption_key' specifies a single customer-supplied encryption key that\n",
            "# will be used for all data written to Google Cloud Storage. See\n",
            "# \"gsutil help encryption\" for more information\n",
            "# Encryption key: RFC 4648 section 4 base64-encoded AES256 string\n",
            "# Warning: If decrypt_key is specified without an encrypt_key, objects will be\n",
            "# decrypted when copied in the cloud.\n",
            "#encryption_key=\n",
            "\n",
            "# Each 'decryption_key' entry specifies a customer-supplied decryption key that\n",
            "# will be used to access and Google Cloud Storage objects encrypted with\n",
            "# the corresponding key.\n",
            "# Decryption keys: Up to 100 RFC 4648 section 4 base64-encoded AES256 strings\n",
            "# in ascending numerical order, starting with 1.\n",
            "#decryption_key1=\n",
            "#decryption_key2=\n",
            "#decryption_key3=\n",
            "\n",
            "# The ability to specify an alternative JSON API version is primarily for cloud\n",
            "# storage service developers.\n",
            "#json_api_version = v1\n",
            "\n",
            "# Specifies the API to use when interacting with cloud storage providers. If the\n",
            "# gsutil command supports this API for the provider, it will be used instead of\n",
            "# the default API. Commands typically default to XML for S3 and JSON for GCS.\n",
            "# Note that if any encryption configuration options are set (see above), the\n",
            "# JSON API will be used for interacting with Google Cloud Storage buckets even\n",
            "# if XML is preferred, as gsutil does not currently support this functionality\n",
            "# when using the XML API.\n",
            "#prefer_api = json\n",
            "#prefer_api = xml\n",
            "\n",
            "# Disables the prompt asking for opt-in to data collection for analytics.\n",
            "#disable_analytics_prompt = True\n",
            "\n",
            "# The \"test\" command runs tests against regional buckets (unless you supply the\n",
            "# `-b` option). By default, the region used is us-central1, but you can change\n",
            "# the default region using this option.\n",
            "#test_cmd_regional_bucket_location = us-central1\n",
            "\n",
            "# Tests for the \"notification watchbucket\" command require a notification URL.\n",
            "# If this option is not supplied, those tests will be skipped.\n",
            "#test_notification_url = https://yourdomain.url/notification-endpoint\n",
            "\n",
            "\n",
            "# 'default_api_version' specifies the default Google Cloud Storage XML API\n",
            "# version to use. If not set below gsutil defaults to API version 1.\n",
            "default_api_version = 1\n",
            "\n",
            "[OAuth2]\n",
            "# This section specifies options used with OAuth2 authentication.\n",
            "\n",
            "# 'token_cache' specifies how the OAuth2 client should cache access tokens.\n",
            "# Valid values are:\n",
            "#  'in_memory': an in-memory cache is used. This is only useful if the boto\n",
            "#      client instance (and with it the OAuth2 plugin instance) persists\n",
            "#      across multiple requests.\n",
            "#  'file_system' : access tokens will be cached in the file system, in files\n",
            "#      whose names include a key derived from the refresh token the access token\n",
            "#      based on.\n",
            "# The default is 'file_system'.\n",
            "#token_cache = file_system\n",
            "#token_cache = in_memory\n",
            "\n",
            "# 'token_cache_path_pattern' specifies a path pattern for token cache files.\n",
            "# This option is only relevant if token_cache = file_system.\n",
            "# The value of this option should be a path, with place-holders '%(key)s' (which\n",
            "# will be replaced with a key derived from the refresh token the cached access\n",
            "# token was based on), and (optionally), %(uid)s (which will be replaced with\n",
            "# the UID of the current user, if available via os.getuid()).\n",
            "# Note that the config parser itself interpolates '%' placeholders, and hence\n",
            "# the above placeholders need to be escaped as '%%(key)s'.\n",
            "# The default value of this option is\n",
            "#  token_cache_path_pattern = <tmpdir>/oauth2client-tokencache.%%(uid)s.%%(key)s\n",
            "# where <tmpdir> is the system-dependent default temp directory.\n",
            "\n",
            "# The following options specify the label and endpoint URIs for the OAUth2\n",
            "# authorization provider being used. Primarily useful for tool developers.\n",
            "#provider_label = Google\n",
            "#provider_authorization_uri = https://accounts.google.com/o/oauth2/auth\n",
            "#provider_token_uri = https://oauth2.googleapis.com/token\n",
            "\n",
            "# 'oauth2_refresh_retries' controls the number of retry attempts made when\n",
            "# rate limiting errors occur for OAuth2 requests to retrieve an access token.\n",
            "# The default value is 6.\n",
            "#oauth2_refresh_retries = <integer value>\n",
            "\n",
            "# The following options specify the OAuth2 client identity and secret that is\n",
            "# used when requesting and using OAuth2 tokens. If not specified, a default\n",
            "# OAuth2 client for the gsutil tool is used; for uses of the boto library (with\n",
            "# OAuth2 authentication plugin) in other client software, it is recommended to\n",
            "# use a tool/client-specific OAuth2 client. For more information on OAuth2, see\n",
            "# http://code.google.com/apis/accounts/docs/OAuth2.html\n",
            "#client_id = <OAuth2 client id>\n",
            "#client_secret = <OAuth2 client secret>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPRDCgHSA7Pq"
      },
      "source": [
        "## Step 0202 Unzip Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-oCvtIW5L2A"
      },
      "source": [
        "%%time\n",
        "s3_resource = boto3.resource('s3')\n",
        "zip_obj = s3_resource.Object(bucket_name=\"bucket_name_here\", key=zip_key)\n",
        "buffer = BytesIO(zip_obj.get()[\"Body\"].read())\n",
        "\n",
        "z = zipfile.ZipFile(buffer)\n",
        "for filename in z.namelist():\n",
        "    file_info = z.getinfo(filename)\n",
        "    s3_resource.meta.client.upload_fileobj(\n",
        "        z.open(filename),\n",
        "        Bucket=bucket,\n",
        "        Key=f'{filename}'\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xbxq6SOqa86u"
      },
      "source": [
        "### List File within working Directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4v-wmgalSZC"
      },
      "source": [
        "%%time\n",
        "# Set working directory 01\n",
        "workingDirectory = dictDirectory['sakernas']\n",
        "# workingDirectory = dictDirectory['sakernas'] + '/data'\n",
        "os.chdir(workingDirectory)\n",
        "path = !pwd\n",
        "print(path)\n",
        "\n",
        "# List file on working directory 02\n",
        "listFile = [f for f in glob.glob('*.csv')]\n",
        "# listFile = [f for f in glob.glob('*.dta')]\n",
        "# listFile = [f for f in glob.glob('*.*')]\n",
        "\n",
        "# Prints all files within directory 03\n",
        "loop = 0\n",
        "for e in listFile:\n",
        "  print(e)\n",
        "  loop += 1\n",
        "\n",
        "print(loop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAFdTPM7nXzG"
      },
      "source": [
        "# Copy data from Google Cloud Storage into AWS S3\n",
        "bucketName = 'bucket-prospera-01'\n",
        "bucketDirectory = dictGCSDirectory['se-2016-direktori']\n",
        "\n",
        "# !gsutil ls s3://{bucketDirectory}\n",
        "\n",
        "# Delete data using gsutil cp command\n",
        "# !gsutil rm gs://{bucketName}/01-rawdata/01-bps/04-sensus-ekonomi/se-2016/se-2016-umb-jk/**\n",
        "\n",
        "# Copy data using gsutil cp command\n",
        "# !gsutil -m cp -r /content/drive/My\\ Drive/Database/se-2016-umb-keuangan/data/* gs://{bucketName}/01-rawdata/01-bps/04-sensus-ekonomi/se-2016/se-2016-umb-jk/data\n",
        "\n",
        "# !gsutil cp /content/drive/My\\ Drive/Database/se-2016-listing/data/se2016-listing-merge.csv gs://{bucketName}/01-rawdata/01-bps/04-sensus-ekonomi/se-2016/se-2016-listing/data\n",
        "# !gsutil cp /content/drive/My\\ Drive/Database/se-2016-listing/data/se2016-listing-33-convert.csv gs://{bucketName}/01-rawdata/01-bps/04-sensus-ekonomi/se-2016/se-2016-listing/data\n",
        "# !gsutil -m cp -r /content/drive/My\\ Drive/Data/* gs://bucket-prospera-01/01-rawdata/01-bps/04-sensus-ekonomi/se-2016/se-2016-listing/data\n",
        "\n",
        "# Copy data using gsutil rsync command exclude directories\n",
        "# !gsutil rsync -d /content/drive/My\\ Drive/Database/se-2016-umb-nonkeuangan/ gs://{bucketName}/01-rawdata/01-bps/04-sensus-ekonomi/se-2016/se-2016-umb-nonkeuangan/\n",
        "# !gsutil rsync -d /content/drive/My\\ Drive/Database/se-2016-umb-produksi/ gs://{bucketDirectory}\n",
        "\n",
        "# Copy data using gsutil rsync command include directories\n",
        "!gsutil rsync -d -r gs://{bucketDirectory} s3://{bucketDirectory}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2MrpDWrCXeN"
      },
      "source": [
        "### Google Big Query Command"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sbtq21f9cg8"
      },
      "source": [
        "# Set working directory on Google Big Query 01\n",
        "projectId = 'datawarehouse-001'\n",
        "directoryBQ = ['datawarehouse-001:04_sensus_ekonomi', 'datawarehouse-001:04_sensus_ekonomi']\n",
        "\n",
        "!gcloud config set project {projectID}\n",
        "# List file on Google Big Query working directory 02\n",
        "# !bq show datawarehouse-001:04_sensus_ekonomi.se_2016_listing_merge\n",
        "!bq ls  {directoryBQ[0]}\n",
        "#  !bq ls --max_results=1000 {directoryBQ[0]}\n",
        "# !bq rm --help"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DJ701ULOU8m"
      },
      "source": [
        "Big Query Delete Table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QywdOf7L7QJ"
      },
      "source": [
        "%%time\n",
        "# Big Query delete table se2016-listing\n",
        "listBQFile = [\n",
        "  'se_2016_listing_11', 'se_2016_listing_12', 'se_2016_listing_13', 'se_2016_listing_14', 'se_2016_listing_15', \n",
        "  'se_2016_listing_16', 'se_2016_listing_17', 'se_2016_listing_18', 'se_2016_listing_19', 'se_2016_listing_21', \n",
        "  'se_2016_listing_31', 'se_2016_listing_32', 'se_2016_listing_33', 'se_2016_listing_34', 'se_2016_listing_35', \n",
        "  'se_2016_listing_36', 'se_2016_listing_51', 'se_2016_listing_52', 'se_2016_listing_53', 'se_2016_listing_61', \n",
        "  'se_2016_listing_62', 'se_2016_listing_63', 'se_2016_listing_64', 'se_2016_listing_65', 'se_2016_listing_71', \n",
        "  'se_2016_listing_72', 'se_2016_listing_73', 'se_2016_listing_74', 'se_2016_listing_75', 'se_2016_listing_76', \n",
        "  'se_2016_listing_81', 'se_2016_listing_82', 'se_2016_listing_91', 'se_2016_listing_94', 'se_2016_listing_merge' \n",
        "]\n",
        "\n",
        "# Big Query delete table se2016-direktori\n",
        "listBQFile = [\n",
        "]\n",
        "\n",
        "# Big Query delete table se2016-umk\n",
        "listBQFile = [\n",
        "]\n",
        "\n",
        "# Big Query delete table se2016-umb-jk\n",
        "listBQFile = [\n",
        "]\n",
        "\n",
        "# Big Query delete table se2016-umb-jnk\n",
        "listBQFile = [\n",
        "]\n",
        "\n",
        "# Big Query delete table se2016-umb-sp\n",
        "listBQFile = [\n",
        "]\n",
        "\n",
        "# Set working directory on Google Big Query 01\n",
        "projectId = 'datawarehouse-001'\n",
        "directoryBQ = ['datawarehouse-001:03_sakernas']\n",
        "\n",
        "# List file on Google Big Query working directory 02\n",
        "# !bq ls --max_results=1000 {directoryBQ[0]}\n",
        "\n",
        "# !bq rm --help\n",
        "\n",
        "loop = 0\n",
        "for e in listBQFile:\n",
        "  bqFileName = directoryBQ[0] + \".\" + e\n",
        "  !bq rm -f -t {bqFileName}\n",
        "  # print(\"delete\", e)\n",
        "  print(\"delete\", bqFileName)\n",
        "  loop += 1\n",
        "\n",
        "print(loop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4bk_H_tPCaN"
      },
      "source": [
        "Big Query Create Table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T__9kwZ-5c7Z"
      },
      "source": [
        "# Upload file on working directory to Google Big Query\n",
        "\n",
        "# Set Working Diretory 01\n",
        "workingDirectory = dictDirectory['sakernas']\n",
        "\n",
        "!bq load \\\n",
        "    --source_format=CSV \\\n",
        "    --skip_leading_rows=1 \\\n",
        "    datawarehouse-001:04_sensus_ekonomi.se_2016_umb_jk_02_21 \\\n",
        "    gs://bucket-prospera-01/01-rawdata/01-bps/04-sensus-ekonomi/se-2016/se-2016-umb-jk/data/data-01/se2016-umb-jk-01-21.csv \\\n",
        "    ./se2016-umb-jka-layout.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8yDeMxHPN_D"
      },
      "source": [
        "%%time\n",
        "# Big Query create table susenas\n",
        "listBQFile = [\n",
        "  'susenas00_ki', 'susenas00-ki.csv', 'susenas00-ki-layout-prospera.json', 'susenas00_kr', 'susenas00-kr.csv', 'susenas00-kr-layout-prospera.json', 'susenas00_kna', 'susenas00-kna.csv', 'susenas00-kna-layout-prospera.json'\n",
        "]\n",
        "\n",
        "# Set working directory 01\n",
        "workingDirectory = dictDirectory['susenas-2000']\n",
        "os.chdir(workingDirectory)\n",
        "path = !pwd\n",
        "\n",
        "pathData = 'gs://bucket-prospera-01/01-rawdata/01-bps/05-susenas/susenas-2000/data/'\n",
        "fileLayout = ''\n",
        "\n",
        "loop = 0\n",
        "for i in range (0, len(listBQFile), 3):\n",
        "  pathSource = pathData + listBQFile[i+1]\n",
        "  fileLayout = listBQFile[i+2]\n",
        "  pathDestination = 'datawarehouse-001:05_susenas.' + listBQFile[i]\n",
        "\n",
        "  # Upload file on working directory to Google Big Query\n",
        "  !bq load \\\n",
        "    --source_format=CSV \\\n",
        "    --skip_leading_rows=1 \\\n",
        "    --replace=True \\\n",
        "    {pathDestination} \\\n",
        "    {pathSource} \\\n",
        "    ./{fileLayout}\n",
        "\n",
        "  print(pathDestination, pathSource, fileLayout)\n",
        "  loop += 1\n",
        "\n",
        "print(loop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCsx8TVEB4Ln"
      },
      "source": [
        "%%time\n",
        "# Big Query create table\n",
        "listBQFile = [\n",
        "\t'sakernas_1994', 'sakernas_1994.csv',\n",
        "\t'sakernas_1995', 'sakernas_1995.csv',\n",
        "\t'sakernas_1996', 'sakernas_1996.csv',\n",
        "\t'sakernas_1997', 'sakernas_1997.csv',\n",
        "\t'sakernas_1998', 'sakernas_1998.csv',\n",
        "\t'sakernas_1999', 'sakernas_1999.csv',\n",
        "\t'sakernas_2000', 'sakernas_2000.csv',\n",
        "\t'sakernas_2001', 'sakernas_2001.csv',\n",
        "\t'sakernas_2002', 'sakernas_2002.csv',\n",
        "\t'sakernas_2003', 'sakernas_2003.csv',\n",
        "\t'sakernas_2004', 'sakernas_2004.csv',\n",
        "\t'sakernas_2005nov', 'sakernas_2005nov.csv',\n",
        "\t'sakernas_2006aug', 'sakernas_2006aug.csv',\n",
        "\t'sakernas_2007aug', 'sakernas_2007aug.csv',\n",
        "\t'sakernas_2007feb', 'sakernas_2007feb.csv',\n",
        "\t'sakernas_2008aug', 'sakernas_2008aug.csv',\n",
        "\t'sakernas_2008feb', 'sakernas_2008feb.csv',\n",
        "\t'sakernas_2009aug', 'sakernas_2009aug.csv',\n",
        "\t'sakernas_2009feb', 'sakernas_2009feb.csv',\n",
        "\t'sakernas_2010aug', 'sakernas_2010aug.csv',\n",
        "\t'sakernas_2010feb', 'sakernas_2010feb.csv',\n",
        "\t'sakernas_2011aug_rev', 'sakernas_2011aug_rev.csv',\n",
        "\t'sakernas_2011feb', 'sakernas_2011feb.csv',\n",
        "\t'sakernas_2012aug_rev', 'sakernas_2012aug_rev.csv',\n",
        "\t'sakernas_2012feb', 'sakernas_2012feb.csv',\n",
        "\t'sakernas_2013aug_rev', 'sakernas_2013aug_rev.csv',\n",
        "\t'sakernas_2013feb', 'sakernas_2013feb.csv',\n",
        "\t'sakernas_2014aug', 'sakernas_2014aug.csv',\n",
        "\t'sakernas_2014feb', 'sakernas_2014feb.csv',\n",
        "\t'sakernas_2015aug', 'sakernas_2015aug.csv',\n",
        "\t'sakernas_2015feb', 'sakernas_2015feb.csv',\n",
        "\t'sakernas_2016aug', 'sakernas_2016aug.csv',\n",
        "\t'sakernas_2016feb', 'sakernas_2016feb.csv',\n",
        "\t'sakernas_2017aug', 'sakernas_2017aug.csv',\n",
        "\t'sakernas_2017feb', 'sakernas_2017feb.csv',\n",
        "\t'sakernas_2018aug', 'sakernas_2018aug.csv',\n",
        "\t'sakernas_2018feb', 'sakernas_2018feb.csv',\n",
        "\t'sakernas_2019aug', 'sakernas_2019aug.csv'\n",
        "]\n",
        "\n",
        "# Set working directory 01\n",
        "os.chdir(workingDirectory)\n",
        "path = !pwd\n",
        "\n",
        "loop = 0\n",
        "for i in range (0, 76, 2):\n",
        "  pathData = 'gs://bucket-prospera-01/01-rawdata/01-bps/03-sakernas/'\n",
        "\n",
        "  if listBQFile[i] == 'se2016_umb_jk_01_11':\n",
        "    pathData = 'gs://bucket-prospera-01/01-rawdata/01-bps/04-sensus-ekonomi/se-2016/se-2016-umb-jk/data/data-01/'\n",
        "    # fileLayout = 'se2016-umb-jka-layout.json'\n",
        "  elif listBQFile[i] == 'se2016_umb_jk_02_11':\n",
        "    pathData = 'gs://bucket-prospera-01/01-rawdata/01-bps/04-sensus-ekonomi/se-2016/se-2016-umb-jk/data/data-02/'\n",
        "    # fileLayout = 'se2016-umb-jkb-layout.json'\n",
        "  elif listBQFile[i] == 'se2016_umb_jk_03_11':\n",
        "    pathData = 'gs://bucket-prospera-01/01-rawdata/01-bps/04-sensus-ekonomi/se-2016/se-2016-umb-jk/data/data-03/'\n",
        "    # fileLayout = 'se2016-umb-jkc-layout.json'\n",
        "  elif listBQFile[i] == 'se2016_umb_jk_11':\n",
        "    pathData = 'gs://bucket-prospera-01/01-rawdata/01-bps/04-sensus-ekonomi/se-2016/se-2016-umb-jk/data/'\n",
        "    # fileLayout = 'se2016-umb-jk-layout.json'\n",
        "\n",
        "  pathSource = pathData + listBQFile[i+1]\n",
        "  pathDestination = 'datawarehouse-001:03_sakernas.' + listBQFile[i]\n",
        "\n",
        "  # Upload file on working directory to Google Big Query\n",
        "  !bq load \\\n",
        "      --autodetect \\\n",
        "      --source_format=CSV \\\n",
        "      --skip_leading_rows=1 \\\n",
        "      {pathDestination} \\\n",
        "      {pathSource}      # \\\n",
        "      #./{fileLayout}\n",
        "\n",
        "  print(listBQFile[i])\n",
        "  loop += 1\n",
        "\n",
        "print(loop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1py-B7WEAnm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ha6JKL57cCs8"
      },
      "source": [
        "## Step 0202 Standardize File Names"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuNnCdMDADXc"
      },
      "source": [
        "%%time\n",
        "dictFile = {\n",
        "  'susenas-2000': {'source': 'susenas-2000-', 'dest': 'susenas00-'},\n",
        "  'susenas-2001': {'source': 'susenas-2001-', 'dest': 'susenas01-'},\n",
        "  'susenas-2002': {'source': 'susenas-2002-', 'dest': 'susenas02-'},\n",
        "  'susenas-2003': {'source': 'se-2016-umk-', 'dest': 'susenas-umk-'},\n",
        "  'susenas-2004': {'source': '_data1_umk_v1', 'dest': 'susenas-umk-01-'},\n",
        "  'susenas-2005': {'source': '_data2_umk_v1', 'dest': 'susenas-umk-02-'},\n",
        "  'susenas-2006': {'source': 'se-2016-umb-jk', 'dest': 'susenas-umb-jk'},\n",
        "  'susenas-2007': {'source': '_data1_umb-jk_v1', 'dest': 'susenas-umb-jk-01-'},\n",
        "  'susenas-2008': {'source': '_data2_umb-jk_v1', 'dest': 'susenas-umb-jk-02-'},\n",
        "  'susenas-2009': {'source': '_data3_umb-jk_v1', 'dest': 'susenas-umb-jk-03-'},\n",
        "  'susenas-2010': {'source': 'se-2016-umb-jnk', 'dest': 'susenas-umb-jnk'},\n",
        "  'susenas-2011': {'source': '_data1_umb-jnk_v1', 'dest': 'susenas-umb-jnk-01-'},\n",
        "  'susenas-2012': {'source': '_data2_umb-jnk_v1', 'dest': 'susenas-umb-jnk-02-'},\n",
        "  'susenas-2013': {'source': '_data3_umb-jnk_v1', 'dest': 'susenas-umb-jnk-03-'},\n",
        "  'susenas-2014': {'source': 'se-2016-umb-sp', 'dest': 'susenas-umb-sp'},\n",
        "  'susenas-2015': {'source': '_data1_umb-sp_v1', 'dest': 'susenas-umb-sp-01-'},\n",
        "  'susenas-2016': {'source': '_data2_umb-sp_v1', 'dest': 'susenas-umb-sp-02-'},\n",
        "  'susenas-2017': {'source': '_data3_umb-sp_v1', 'dest': 'susenas-umb-sp-03-'},\n",
        "  'susenas-2018': {'source': '_data3_umb-sp_v1', 'dest': 'susenas-umb-sp-03-'}\n",
        "}\n",
        "\n",
        "# Set working directory 01\n",
        "workingDirectory = dictDirectory['susenas-2002']\n",
        "os.chdir(workingDirectory)\n",
        "path = !pwd\n",
        "\n",
        "# List dbf file within directory 02\n",
        "listFile = [f for f in glob.glob('*.*')]\n",
        "fileSource = dictFile['susenas-2002']['source']\n",
        "fileDestination = dictFile['susenas-2002']['dest']\n",
        "\n",
        "# Rename dbf file within directory 03\n",
        "loop = 0\n",
        "for e in listFile:\n",
        "  pathSource = path[0] + '/' + e\n",
        "  pathDestination = path[0] + '/' + e\n",
        "  # pathDestination = path[0] + '/' + fileDestination + e\n",
        "  # print('renaming ' + pathSource)\n",
        "  fname,ext = os.path.splitext(pathDestination)\n",
        "  fname = fname.replace(fileSource,fileDestination)\n",
        "\n",
        "  os.rename(pathSource, fname + ext)\n",
        "  print(e, fname)\n",
        "  loop += 1\n",
        "\n",
        "print(loop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_cUm_tdcRdb"
      },
      "source": [
        "## Step 0203A Convert File from stata into csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRd5IauKsBwG"
      },
      "source": [
        "%%time\n",
        "listFile = [f for f in glob.glob('*.dta')]\n",
        "# listFile = ['ind96a.dta', 'ind96b.dta']\n",
        "\n",
        "path = !pwd\n",
        "loop = 0\n",
        "for e in listFile:\n",
        "  pathSource = path[0] + '/' + e\n",
        "  fname,ext = os.path.splitext(pathSource)\n",
        "  pathDestination = fname + '.csv'\n",
        "\n",
        "  # Convert file from stata into csv\n",
        "  dfStata = pd.io.stata.read_stata(pathSource, convert_categoricals=False)\n",
        "  dfStata.to_csv(pathDestination, encoding='utf-8', index=False)\n",
        "\n",
        "  # Read and import csv file dataset into pandas data frame, change paths if needed\n",
        "  dfBPSData = pd.read_csv(pathDestination)\n",
        "\n",
        "  # print(\"dfBPSData.shape :\", dfBPSData.shape)\n",
        "  # print(\"type(dfBPSData) :\", type(dfBPSData))\n",
        "  print(\"converting\", e, \"to\", pathDestination)\n",
        "  loop += 1\n",
        "\n",
        "print(loop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnxjYhKyPR88"
      },
      "source": [
        "## Step 0203B Convert File from dbf into csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boCXFJ8aPLbE"
      },
      "source": [
        "%%time\n",
        "# Set working directory 01\n",
        "os.chdir(dictDirectory['susenas-2002-data'])\n",
        "path = !pwd\n",
        "\n",
        "# List dbf file within directory 02\n",
        "listFile = [f for f in glob.glob('*.dbf')]\n",
        "# listFile = ['ssn01kr.dbf']\n",
        "\n",
        "# Convert dbf file into csv 03\n",
        "loop = 0\n",
        "for e in listFile:\n",
        "  pathSource = path[0] + '/' + e\n",
        "  fname,ext = os.path.splitext(pathSource)\n",
        "  pathDestination = fname + '.csv'\n",
        "\n",
        "  # Convert file from dbf into csv (using Dbf5)\n",
        "  dbfFile = Dbf5(pathSource)\n",
        "  dfDbf = dbfFile.to_dataframe()\n",
        "  dfDbf.to_csv(pathDestination, encoding='utf-8', index=False)\n",
        "\n",
        "  # Convert file from dbf into csv (using dbf)\n",
        "  # with dbf.Table(pathSource) as table:\n",
        "  #  dbf.export(table, pathDestination)\n",
        "\n",
        "  # Read and import csv file dataset into pandas data frame, change paths if needed\n",
        "  # dfBPSData = pd.read_csv(pathDestination)\n",
        "\n",
        "  # print(\"dfBPSData.shape :\", dfBPSData.shape)\n",
        "  # print(\"type(dfBPSData) :\", type(dfBPSData))\n",
        "  print(\"converting\", e, \"to\", pathDestination)\n",
        "  loop += 1\n",
        "\n",
        "print(loop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYWlW7Y0cdvZ"
      },
      "source": [
        "## Step 0204 Check & Review Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJIWFRwY4Po9"
      },
      "source": [
        "# Read and import csv file dataset into pandas data frame, change paths if needed\n",
        "dictFileReview = {\n",
        "  'susenas00-ki': 'susenas00-ki.csv', 'susenas00-kr': 'susenas00-kr.csv', 'susenas00-kna': 'susenas00-kna.csv', 'susenas00-mod-ki': 'susenas00-mod-ki.csv', 'susenas00-mod-kr': 'susenas00-mod-kr.csv',\n",
        "  'susenas01-ki': 'susenas01-ki.csv', 'susenas01-kr': 'susenas01-kr.csv', 'susenas01-ind-km': 'susenas01-ind-km.csv', 'susenas01-rt-km': 'susenas01-rt-km.csv',\n",
        "  'susenas02-ki': 'susenas02jul-ki.csv', 'susenas02-kr': 'susenas02jul-kr.csv'\n",
        "  \n",
        "}\n",
        "\n",
        "# Set working directory 01\n",
        "workingDirectory = dictDirectory['sakernas']\n",
        "# workingDirectory = dictDirectory['sakernas'] + '/data'\n",
        "os.chdir(workingDirectory)\n",
        "path = !pwd\n",
        "\n",
        "# Review dataset 02\n",
        "# fname = dictFileReview['susenas00-kna']\n",
        "# fname = 'susenas02jul-module-consumption.csv'\n",
        "\n",
        "pathSource = path[0] + '/' + fname\n",
        "# print(pathSource)\n",
        "dfBPSData = pd.read_csv(pathSource)\n",
        "\n",
        "print(fname)\n",
        "print(\"dfBPSData.shape  :\", dfBPSData.shape)\n",
        "# print(\"type(dfBPSData)  :\", type(dfBPSData))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJULAHsasrAb"
      },
      "source": [
        "%%time\n",
        "# Set working directory 01\n",
        "workingDirectory = dictDirectory['sakernas']\n",
        "os.chdir(workingDirectory)\n",
        "path = !pwd\n",
        "\n",
        "# List csv file within directory 02\n",
        "listFile = [f for f in glob.glob('*.csv')]\n",
        "\n",
        "# Prints all files within directory 03\n",
        "loop = 0\n",
        "for fname in listFile:\n",
        "  pathSource = path[0] + '/' + fname\n",
        "  # pathDestination = pathSource\n",
        "  dfBPSData = pd.read_csv(pathSource)\n",
        "\n",
        "  print(fname, \".shape  :\", dfBPSData.shape)\n",
        "  loop += 1\n",
        "\n",
        "print(loop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqeuGrJj-RFA"
      },
      "source": [
        "# Examine dataset, see data type\n",
        "print(pathSource)\n",
        "dfBPSData.info(verbose=True, null_counts=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUYzQXRCqpVf"
      },
      "source": [
        "# Examine dataset, see data type\n",
        "print(dfBPSData.describe(percentiles=[], include='all').transpose().to_string())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4HNyNlp5783"
      },
      "source": [
        "# Examine dataset, see data values\n",
        "dfBPSData.head()\n",
        "# dfBPSData.tail()\n",
        "# dfBPSData.sort_values(by=['psid'], ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGloACERum6K"
      },
      "source": [
        "# Examine dataset, see data values\n",
        "dfBPSData.head()\n",
        "# dfBPSData.tail()\n",
        "# dfBPSData.sort_values(by=['psid'], ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nyd6i3s6Ykb"
      },
      "source": [
        "# basic info about columns in each dataset\n",
        "for name, df in dfs.items():\n",
        "    print(\"df: %s\\n\" %name)\n",
        "    print(\"df:\", name, \"type:\", type(df), \"\\n\")\n",
        "    print(\"shape: %d rows, %d cols\\n\" %df.shape)\n",
        "    \n",
        "    print(\"column info:\")\n",
        "    for col in df.columns:\n",
        "        print(\"* %s: %d nulls, %d nans, %d unique vals, most common: %s\" % (\n",
        "            col, \n",
        "            df[col].isnull().sum(),\n",
        "            df[col].isna().sum(),\n",
        "            df[col].nunique(),\n",
        "            df[col].value_counts().head(2).to_dict()\n",
        "        ))\n",
        "    print(\"\\n------\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q362ArQ_HXIn"
      },
      "source": [
        "# Examine dataset\n",
        "# print(dfBPSData.describe(percentiles=[], include='all').transpose().to_string())\n",
        "print(dfBPSData.count().transpose().to_string())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pneHPWaGBjsR"
      },
      "source": [
        "pd.reset_option('display.show_dimensions')\n",
        "pd.set_option('display.show_dimensions', False)\n",
        "print(pd.options.display.max_rows, pd.options.display.show_dimensions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v78u6YBW86_c"
      },
      "source": [
        "# Examine dataset, first 5 rows\n",
        "# dfBPSData['DDESA94'].isna().any()\n",
        "# dfBPSData.sort_values(by='psid')\n",
        "# dfBPSData.tail(10)\n",
        "dfBPSData.isna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjXJiFPxgu-Y"
      },
      "source": [
        "## Step 0205 Convert Data Type\n",
        "Convert data type float into integer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eH1j5M24iv4F"
      },
      "source": [
        "%%time\n",
        "# Read and import csv file dataset into pandas data frame, change paths if needed\n",
        "fname = 'se2016-listing-11.csv'\n",
        "\n",
        "path = !pwd\n",
        "# print(path)\n",
        "\n",
        "pathSource = path[0] + '/' + fname\n",
        "pathDestination = pathSource\n",
        "# print(pathSource)\n",
        "dfBPSData = pd.read_csv(pathSource)\n",
        "\n",
        "print(fname)\n",
        "print(\"dfBPSData.shape  :\", dfBPSData.shape)\n",
        "print(\"type(dfBPSData)  :\", type(dfBPSData))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DusZEXqGjAY7"
      },
      "source": [
        "# Examine dataset, found isna & maximum values\n",
        "# dfBPSData.columns.isna().any()\n",
        "# dfBPSData['psid'].max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48fPO0PYgsPF"
      },
      "source": [
        "# Examine dataset, create data type dictionary\n",
        "dfBPSTypeSeries  = dict(dfBPSData.dtypes)\n",
        "print(dfBPSTypeSeries)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7Rk2oELhMXJ"
      },
      "source": [
        "# Convert data type float into integer\n",
        "for (key, values) in dfBPSTypeSeries.items():\n",
        "  if values=='float64':\n",
        "    print(key, values)\n",
        "    dfBPSData[key] = dfBPSData[key].astype('Int64')\n",
        "\n",
        "    # Special case on certain field\n",
        "    # if key!='D94_VNOB':\n",
        "      # dfBPSData[key] = dfBPSData[key].astype('Int64')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eFX_WNOyRRs"
      },
      "source": [
        "# Save data from convert data type operation\n",
        "print(pathDestination)\n",
        "dfBPSData.to_csv(pathDestination, encoding='utf-8', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hfng_tp1uJhE"
      },
      "source": [
        "Convert data type float into integer (Loop)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aY5RjFlpuHJW"
      },
      "source": [
        "%%time\n",
        "# list csv file within directory\n",
        "listFile = [f for f in glob.glob('*.csv')]\n",
        "\n",
        "# data type to convert\n",
        "dDataType = {\n",
        "  'provinsi':'object',\n",
        "  'nama_prov':'object',\n",
        "  'kabupaten':'object',\n",
        "  'nama_kab':'object',\n",
        "  'idperkab':'Int64',\n",
        "  'b1r11d':'object',\n",
        "  'b1r13':'object',\n",
        "  'b1r14a':'object',\n",
        "  'b1r14b':'object',\n",
        "  'kategori':'object',\n",
        "  'b1r15c':'object',\n",
        "  'b1r15d':'object',\n",
        "  'b1r16':'object',\n",
        "  'b1r19a':'Int64',\n",
        "  'b1r21':'object',\n",
        "  'b1r22a':'object',\n",
        "  'b1r22b':'object',\n",
        "  'kat_omset':'Int64',\n",
        "  'skalausaha':'object',\n",
        "  'penimbang':'object',\n",
        "  'renum':'Int64'\n",
        "    }\n",
        "\n",
        "path = !pwd\n",
        "print(path)\n",
        "loop = 0\n",
        "\n",
        "for e in listFile:\n",
        "  pathSource = path[0] + '/' + e\n",
        "  fname,ext = os.path.splitext(pathSource)\n",
        "  pathDestination = fname + \"-convert\" + ext\n",
        "  dfBPSData = pd.read_csv(pathSource)\n",
        "\n",
        "  # print(fname, \".shape  :\", dfBPSData.shape)\n",
        "  # print(pathSource)\n",
        "  print(pathDestination)\n",
        "  \n",
        "  # Examine dataset, create data type dictionary\n",
        "  dfBPSTypeSeries  = dict(dfBPSData.dtypes)\n",
        "\n",
        "  # Convert data type float into integer\n",
        "  for (key, values) in dfBPSTypeSeries.items():\n",
        "    dfBPSData[key] = dfBPSData[key].astype(dDataType[key.lower()])\n",
        "    # print(key, values, \"convert to\", dDataType[key.lower()])\n",
        "\n",
        "  # Save data from convert data type operation\n",
        "  # print(pathDestination)\n",
        "  dfBPSData.to_csv(pathDestination, encoding='utf-8', index=False)\n",
        "  loop += 1\n",
        "\n",
        "print(loop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uX3tNt4zFM4O"
      },
      "source": [
        "# save data into google cloud storage\n",
        "bucket_name = 'bucket-prospera-01'\n",
        "!gsutil cp /content/drive/My\\ Drive/Database/se-2016-listing/data/se2016-listing-33.csv gs://{bucket_name}/01-rawdata/01-bps/04-sensus-ekonomi/se-2016/se-2016-listing/data\n",
        "!gsutil cp /content/drive/My\\ Drive/Database/se-2016-listing/data/se-2016-listing-33-convert.csv gs://{bucket_name}/01-rawdata/01-bps/04-sensus-ekonomi/se-2016/se-2016-listing/data\n",
        "# !gsutil -m cp -r /content/drive/My\\ Drive/Data/* gs://bucket-prospera-01/01-rawdata/01-bps/04-sensus-ekonomi/se-2016/se-2016-listing/data\n",
        "/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAoEpo1PjNO4"
      },
      "source": [
        "## Step 0206 Merge Dataset\n",
        "Merge Dataset if required"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ndyaSVwzo7o"
      },
      "source": [
        "%%time\n",
        "# Read and import csv file dataset into pandas data frame, change paths if needed\n",
        "fname  = 'ind95.csv' # Merge data files\n",
        "fnameA = 'ind95a.csv'\n",
        "fnameB = 'ind95b.csv'\n",
        "\n",
        "path = !pwd\n",
        "pathSourceA = path[0] + '/' + fnameA\n",
        "pathSourceB = path[0] + '/' + fnameB\n",
        "pathDestination = path[0] + '/' + fname\n",
        "print(pathSourceA)\n",
        "print(pathSourceB)\n",
        "dfBPSDataA = pd.read_csv(pathSourceA)\n",
        "dfBPSDataB = pd.read_csv(pathSourceB)\n",
        "\n",
        "print(\"dfBPSDataA.shape  :\", dfBPSDataA.shape)\n",
        "print(\"type(dfBPSDataA)  :\", type(dfBPSDataA))\n",
        "print(\"dfBPSDataB.shape  :\", dfBPSDataB.shape)\n",
        "print(\"type(dfBPSDataB)  :\", type(dfBPSDataB))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YAl2E_m2RjP"
      },
      "source": [
        "# Rename joining keys\n",
        "dfBPSDataA.rename({'nomor': 'NOMOR_A'}, axis='columns', inplace=True)\n",
        "dfBPSDataB.rename({'nomor': 'NOMOR_B'}, axis='columns', inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksYR16Zzj7g_"
      },
      "source": [
        "# Merge data files\n",
        "dfBPSData = dfBPSDataA.merge(dfBPSDataB, left_on='NOMOR_A', right_on='NOMOR_B')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhHx-SgAkXmB"
      },
      "source": [
        "dfBPSData[['NOMOR_A','NOMOR_B']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_NEl1xr_3mu"
      },
      "source": [
        "# Save data from convert data type operation\n",
        "print(pathDestination)\n",
        "dfBPSData.to_csv(pathDestination, encoding='utf-8', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bz91TM1WofAg"
      },
      "source": [
        "### List Files within Working Directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhsjEz6jwMVW"
      },
      "source": [
        "%%time\n",
        "# Set working directory 01\n",
        "workingDirectory = dictDirectory['se-2016-direktori-data']\n",
        "os.chdir(workingDirectory)\n",
        "path = !pwd\n",
        "print(path)\n",
        "\n",
        "# List file on working directory 02\n",
        "listFile = [f for f in glob.glob('*.csv')]\n",
        "# listFile = [f for f in glob.glob('*.dbf')]\n",
        "# listFile = [f for f in glob.glob('*.*')]\n",
        "\n",
        "# Prints all files within directory 03\n",
        "loop = 0\n",
        "for e in listFile:\n",
        "  print(e)\n",
        "  loop += 1\n",
        "\n",
        "print(loop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iov9X9Hx9Skg"
      },
      "source": [
        "### Merge File tableA + tableB -> tableMerge"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_gQnmQAodv0"
      },
      "source": [
        "%%time\n",
        "# Merge table se2016-umk\n",
        "listFile = [\n",
        "  'se2016-umk-11.csv', 'se2016-umk-01-11.csv', 'se2016-umk-02-11.csv', 'se2016-umk-12.csv', 'se2016-umk-01-12.csv', 'se2016-umk-02-12.csv', 'se2016-umk-13.csv', 'se2016-umk-01-13.csv', 'se2016-umk-02-13.csv', 'se2016-umk-14.csv', 'se2016-umk-01-14.csv', 'se2016-umk-02-14.csv', 'se2016-umk-15.csv', 'se2016-umk-01-15.csv', 'se2016-umk-02-15.csv', \n",
        "  'se2016-umk-16.csv', 'se2016-umk-01-16.csv', 'se2016-umk-02-16.csv', 'se2016-umk-17.csv', 'se2016-umk-01-17.csv', 'se2016-umk-02-17.csv', 'se2016-umk-18.csv', 'se2016-umk-01-18.csv', 'se2016-umk-02-18.csv', 'se2016-umk-19.csv', 'se2016-umk-01-19.csv', 'se2016-umk-02-19.csv', 'se2016-umk-21.csv', 'se2016-umk-01-21.csv', 'se2016-umk-02-21.csv', \n",
        "  'se2016-umk-31.csv', 'se2016-umk-01-31.csv', 'se2016-umk-02-31.csv', 'se2016-umk-32.csv', 'se2016-umk-01-32.csv', 'se2016-umk-02-32.csv', 'se2016-umk-33.csv', 'se2016-umk-01-33.csv', 'se2016-umk-02-33.csv', 'se2016-umk-34.csv', 'se2016-umk-01-34.csv', 'se2016-umk-02-34.csv', 'se2016-umk-35.csv', 'se2016-umk-01-35.csv', 'se2016-umk-02-35.csv', \n",
        "  'se2016-umk-36.csv', 'se2016-umk-01-36.csv', 'se2016-umk-02-36.csv', 'se2016-umk-51.csv', 'se2016-umk-01-51.csv', 'se2016-umk-02-51.csv', 'se2016-umk-52.csv', 'se2016-umk-01-52.csv', 'se2016-umk-02-52.csv', 'se2016-umk-53.csv', 'se2016-umk-01-53.csv', 'se2016-umk-02-53.csv', 'se2016-umk-61.csv', 'se2016-umk-01-61.csv', 'se2016-umk-02-61.csv', \n",
        "  'se2016-umk-62.csv', 'se2016-umk-01-62.csv', 'se2016-umk-02-62.csv', 'se2016-umk-63.csv', 'se2016-umk-01-63.csv', 'se2016-umk-02-63.csv', 'se2016-umk-64.csv', 'se2016-umk-01-64.csv', 'se2016-umk-02-64.csv', 'se2016-umk-65.csv', 'se2016-umk-01-65.csv', 'se2016-umk-02-65.csv', 'se2016-umk-71.csv', 'se2016-umk-01-71.csv', 'se2016-umk-02-71.csv', \n",
        "  'se2016-umk-72.csv', 'se2016-umk-01-72.csv', 'se2016-umk-02-72.csv', 'se2016-umk-73.csv', 'se2016-umk-01-73.csv', 'se2016-umk-02-73.csv', 'se2016-umk-74.csv', 'se2016-umk-01-74.csv', 'se2016-umk-02-74.csv', 'se2016-umk-75.csv', 'se2016-umk-01-75.csv', 'se2016-umk-02-75.csv', 'se2016-umk-76.csv', 'se2016-umk-01-76.csv', 'se2016-umk-02-76.csv', \n",
        "  'se2016-umk-81.csv', 'se2016-umk-01-81.csv', 'se2016-umk-02-81.csv', 'se2016-umk-82.csv', 'se2016-umk-01-82.csv', 'se2016-umk-02-82.csv', 'se2016-umk-91.csv', 'se2016-umk-01-91.csv', 'se2016-umk-02-91.csv', 'se2016-umk-94.csv', 'se2016-umk-01-94.csv', 'se2016-umk-02-94.csv'\n",
        "]\n",
        "\n",
        "# Set working directory 01\n",
        "workingDirectory = dictDirectory['se-2016-umk-data']\n",
        "os.chdir(workingDirectory)\n",
        "path = !pwd\n",
        "# print(path)\n",
        "\n",
        "loop = 0\n",
        "for i in range (0, len(listFile), 3):\n",
        "  fname = listFile[i]     # Merge data files\n",
        "  fnameA = listFile[i+1]\n",
        "  fnameB  = listFile[i+2]\n",
        "  pathSourceA = path[0] + '/data-01/' + fnameA\n",
        "  pathSourceB = path[0] + '/data-02/' + fnameB\n",
        "  pathDestination = path[0] + '/' + fname\n",
        "\n",
        "  # print(pathSourceA, pathSourceB, pathDestination)\n",
        "  dfBPSDataA = pd.read_csv(pathSourceA)\n",
        "  dfBPSDataB = pd.read_csv(pathSourceB)\n",
        "\n",
        "  # Rename joining keys\n",
        "  dfBPSDataA.rename({'IDPERUSAHA': 'PERUSAHAAN_ID', 'JENISKUESI': 'JENISKUESIONER_A', 'PROV': 'PROVINSI_IDA', 'SKALAUSAHA': 'SKALAUSAHA_A', 'WEIGHT': 'WEIGHT_A'}, axis='columns', inplace=True)\n",
        "  dfBPSDataB.rename({'IDPERUSAHA': 'PERUSAHAAN_ID', 'JENISKUESI': 'JENISKUESIONER_B', 'PROV': 'PROVINSI_IDB', 'SKALAUSAHA': 'SKALAUSAHA_B', 'WEIGHT': 'WEIGHT_B'}, axis='columns', inplace=True)\n",
        "\n",
        "  dfBPSData = [dfBPSDataA, dfBPSDataB]\n",
        "\n",
        "  # Merge data files\n",
        "  # dfBPSData = dfBPSDataA.merge(dfBPSDataB, left_on='IDPERUSAHAAN_A', right_on='IDPERUSAHAAN_B')\n",
        "  dfBPSDataMerge = reduce(lambda left,right: pd.merge(left,right,on='PERUSAHAAN_ID'), dfBPSData)\n",
        "  \n",
        "  # print(pathDestination, \"dfBPSDataA.shape:\", dfBPSDataA.shape, \"dfBPSDataB.shape:\", dfBPSDataB.shape)\n",
        "  # print(pathDestination, \"dfBPSDataA.shape:\", dfBPSDataA.shape, \"dfBPSDataB.shape:\", dfBPSDataB.shape, \"dfBPSDataMerge.shape:\", dfBPSData.shape)\n",
        "  # print(\"dfBPSDataA.shape:\", dfBPSDataA.shape[0], \"dfBPSDataB.shape:\", dfBPSDataB.shape[0], \"dfBPSDataMerge.shape:\", dfBPSData.shape[0])\n",
        "\n",
        "  # Save data from merge data type operation\n",
        "  print(pathDestination, dfBPSDataMerge.shape)\n",
        "  # dfBPSDataMerge.to_csv(pathDestination, encoding='utf-8', index=False)\n",
        "\n",
        "  loop += 1\n",
        "\n",
        "print(loop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3Pezw3vtu9-"
      },
      "source": [
        "%%time\n",
        "# Merge all table\n",
        "# Set working directory 01\n",
        "workingDirectory = dictDirectory['se-2016-direktori-data']\n",
        "os.chdir(workingDirectory)\n",
        "path = !pwd\n",
        "# print(path)\n",
        "\n",
        "# List file on working directory 02\n",
        "listFile = [f for f in glob.glob('*.csv')]\n",
        "\n",
        "pathDestination = path[0] + '/' + 'se2016-direktori-merge.csv'\n",
        "dfMerges = []\n",
        "totalRows = 0\n",
        "loop = 0\n",
        "for e in listFile:\n",
        "  pathSource = path[0] + '/' + e\n",
        "  # print('merge ' + pathSource)\n",
        "\n",
        "  # Read and import csv file dataset into pandas data frame, change paths if needed\n",
        "  dfBPSData = pd.read_csv(pathSource)\n",
        "\n",
        "  dfMerges.append(dfBPSData)\n",
        "  print(\"dfBPSData.shape :\", e, dfBPSData.shape)\n",
        "  totalRows += dfBPSData.shape[0]\n",
        "  loop += 1\n",
        "\n",
        "print(loop)\n",
        "\n",
        "\n",
        "dfBPSDataMerge = pd.concat(dfMerges)\n",
        "print(\"dfBPSDataMerge.shape :\", dfBPSDataMerge.shape, totalRows)\n",
        "\n",
        "print(pathDestination)\n",
        "dfBPSDataMerge.to_csv(pathDestination, encoding='utf-8', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqhUYaU_9smX"
      },
      "source": [
        "### Merge File tableA + tableB + tableC -> tableMerge"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lfsv8YVMZyPf"
      },
      "source": [
        "%%time\n",
        "# Merge table se2016-umb-jk\n",
        "listFile = [\n",
        "  'se2016-umb-jk-11.csv', 'se2016-umb-jk-01-11.csv', 'se2016-umb-jk-02-11.csv', 'se2016-umb-jk-03-11.csv', 'se2016-umb-jk-12.csv', 'se2016-umb-jk-01-12.csv', 'se2016-umb-jk-02-12.csv', 'se2016-umb-jk-03-12.csv', 'se2016-umb-jk-13.csv', 'se2016-umb-jk-01-13.csv', 'se2016-umb-jk-02-13.csv', 'se2016-umb-jk-03-13.csv', 'se2016-umb-jk-14.csv', 'se2016-umb-jk-01-14.csv', 'se2016-umb-jk-02-14.csv', 'se2016-umb-jk-03-14.csv', 'se2016-umb-jk-15.csv', 'se2016-umb-jk-01-15.csv', 'se2016-umb-jk-02-15.csv', 'se2016-umb-jk-03-15.csv', \n",
        "  'se2016-umb-jk-16.csv', 'se2016-umb-jk-01-16.csv', 'se2016-umb-jk-02-16.csv', 'se2016-umb-jk-03-16.csv', 'se2016-umb-jk-17.csv', 'se2016-umb-jk-01-17.csv', 'se2016-umb-jk-02-17.csv', 'se2016-umb-jk-03-17.csv', 'se2016-umb-jk-18.csv', 'se2016-umb-jk-01-18.csv', 'se2016-umb-jk-02-18.csv', 'se2016-umb-jk-03-18.csv', 'se2016-umb-jk-19.csv', 'se2016-umb-jk-01-19.csv', 'se2016-umb-jk-02-19.csv', 'se2016-umb-jk-03-19.csv', 'se2016-umb-jk-21.csv', 'se2016-umb-jk-01-21.csv', 'se2016-umb-jk-02-21.csv', 'se2016-umb-jk-03-21.csv', \n",
        "  'se2016-umb-jk-31.csv', 'se2016-umb-jk-01-31.csv', 'se2016-umb-jk-02-31.csv', 'se2016-umb-jk-03-31.csv', 'se2016-umb-jk-32.csv', 'se2016-umb-jk-01-32.csv', 'se2016-umb-jk-02-32.csv', 'se2016-umb-jk-03-32.csv', 'se2016-umb-jk-33.csv', 'se2016-umb-jk-01-33.csv', 'se2016-umb-jk-02-33.csv', 'se2016-umb-jk-03-33.csv', 'se2016-umb-jk-34.csv', 'se2016-umb-jk-01-34.csv', 'se2016-umb-jk-02-34.csv', 'se2016-umb-jk-03-34.csv', 'se2016-umb-jk-35.csv', 'se2016-umb-jk-01-35.csv', 'se2016-umb-jk-02-35.csv', 'se2016-umb-jk-03-35.csv', \n",
        "  'se2016-umb-jk-36.csv', 'se2016-umb-jk-01-36.csv', 'se2016-umb-jk-02-36.csv', 'se2016-umb-jk-03-36.csv', 'se2016-umb-jk-51.csv', 'se2016-umb-jk-01-51.csv', 'se2016-umb-jk-02-51.csv', 'se2016-umb-jk-03-51.csv', 'se2016-umb-jk-52.csv', 'se2016-umb-jk-01-52.csv', 'se2016-umb-jk-02-52.csv', 'se2016-umb-jk-03-52.csv', 'se2016-umb-jk-53.csv', 'se2016-umb-jk-01-53.csv', 'se2016-umb-jk-02-53.csv', 'se2016-umb-jk-03-53.csv', 'se2016-umb-jk-61.csv', 'se2016-umb-jk-01-61.csv', 'se2016-umb-jk-02-61.csv', 'se2016-umb-jk-03-61.csv', \n",
        "  'se2016-umb-jk-62.csv', 'se2016-umb-jk-01-62.csv', 'se2016-umb-jk-02-62.csv', 'se2016-umb-jk-03-62.csv', 'se2016-umb-jk-63.csv', 'se2016-umb-jk-01-63.csv', 'se2016-umb-jk-02-63.csv', 'se2016-umb-jk-03-63.csv', 'se2016-umb-jk-64.csv', 'se2016-umb-jk-01-64.csv', 'se2016-umb-jk-02-64.csv', 'se2016-umb-jk-03-64.csv', 'se2016-umb-jk-65.csv', 'se2016-umb-jk-01-65.csv', 'se2016-umb-jk-02-65.csv', 'se2016-umb-jk-03-65.csv', 'se2016-umb-jk-71.csv', 'se2016-umb-jk-01-71.csv', 'se2016-umb-jk-02-71.csv', 'se2016-umb-jk-03-71.csv', \n",
        "  'se2016-umb-jk-72.csv', 'se2016-umb-jk-01-72.csv', 'se2016-umb-jk-02-72.csv', 'se2016-umb-jk-03-72.csv', 'se2016-umb-jk-73.csv', 'se2016-umb-jk-01-73.csv', 'se2016-umb-jk-02-73.csv', 'se2016-umb-jk-03-73.csv', 'se2016-umb-jk-74.csv', 'se2016-umb-jk-01-74.csv', 'se2016-umb-jk-02-74.csv', 'se2016-umb-jk-03-74.csv', 'se2016-umb-jk-75.csv', 'se2016-umb-jk-01-75.csv', 'se2016-umb-jk-02-75.csv', 'se2016-umb-jk-03-75.csv', 'se2016-umb-jk-76.csv', 'se2016-umb-jk-01-76.csv', 'se2016-umb-jk-02-76.csv', 'se2016-umb-jk-03-76.csv', \n",
        "  'se2016-umb-jk-81.csv', 'se2016-umb-jk-01-81.csv', 'se2016-umb-jk-02-81.csv', 'se2016-umb-jk-03-81.csv', 'se2016-umb-jk-82.csv', 'se2016-umb-jk-01-82.csv', 'se2016-umb-jk-02-82.csv', 'se2016-umb-jk-03-82.csv', 'se2016-umb-jk-91.csv', 'se2016-umb-jk-01-91.csv', 'se2016-umb-jk-02-91.csv', 'se2016-umb-jk-03-91.csv', 'se2016-umb-jk-94.csv', 'se2016-umb-jk-01-94.csv', 'se2016-umb-jk-02-94.csv', 'se2016-umb-jk-03-94.csv'\n",
        "]\n",
        "\n",
        "# Merge table se2016-umb-jnk\n",
        "listFile = [\n",
        "  'se2016-umb-jnk-11.csv', 'se2016-umb-jnk-01-11.csv', 'se2016-umb-jnk-02-11.csv', 'se2016-umb-jnk-03-11.csv', 'se2016-umb-jnk-12.csv', 'se2016-umb-jnk-01-12.csv', 'se2016-umb-jnk-02-12.csv', 'se2016-umb-jnk-03-12.csv', 'se2016-umb-jnk-13.csv', 'se2016-umb-jnk-01-13.csv', 'se2016-umb-jnk-02-13.csv', 'se2016-umb-jnk-03-13.csv', 'se2016-umb-jnk-14.csv', 'se2016-umb-jnk-01-14.csv', 'se2016-umb-jnk-02-14.csv', 'se2016-umb-jnk-03-14.csv', 'se2016-umb-jnk-15.csv', 'se2016-umb-jnk-01-15.csv', 'se2016-umb-jnk-02-15.csv', 'se2016-umb-jnk-03-15.csv', \n",
        "  'se2016-umb-jnk-16.csv', 'se2016-umb-jnk-01-16.csv', 'se2016-umb-jnk-02-16.csv', 'se2016-umb-jnk-03-16.csv', 'se2016-umb-jnk-17.csv', 'se2016-umb-jnk-01-17.csv', 'se2016-umb-jnk-02-17.csv', 'se2016-umb-jnk-03-17.csv', 'se2016-umb-jnk-18.csv', 'se2016-umb-jnk-01-18.csv', 'se2016-umb-jnk-02-18.csv', 'se2016-umb-jnk-03-18.csv', 'se2016-umb-jnk-19.csv', 'se2016-umb-jnk-01-19.csv', 'se2016-umb-jnk-02-19.csv', 'se2016-umb-jnk-03-19.csv', 'se2016-umb-jnk-21.csv', 'se2016-umb-jnk-01-21.csv', 'se2016-umb-jnk-02-21.csv', 'se2016-umb-jnk-03-21.csv', \n",
        "  'se2016-umb-jnk-31.csv', 'se2016-umb-jnk-01-31.csv', 'se2016-umb-jnk-02-31.csv', 'se2016-umb-jnk-03-31.csv', 'se2016-umb-jnk-32.csv', 'se2016-umb-jnk-01-32.csv', 'se2016-umb-jnk-02-32.csv', 'se2016-umb-jnk-03-32.csv', 'se2016-umb-jnk-33.csv', 'se2016-umb-jnk-01-33.csv', 'se2016-umb-jnk-02-33.csv', 'se2016-umb-jnk-03-33.csv', 'se2016-umb-jnk-34.csv', 'se2016-umb-jnk-01-34.csv', 'se2016-umb-jnk-02-34.csv', 'se2016-umb-jnk-03-34.csv', 'se2016-umb-jnk-35.csv', 'se2016-umb-jnk-01-35.csv', 'se2016-umb-jnk-02-35.csv', 'se2016-umb-jnk-03-35.csv', \n",
        "  'se2016-umb-jnk-36.csv', 'se2016-umb-jnk-01-36.csv', 'se2016-umb-jnk-02-36.csv', 'se2016-umb-jnk-03-36.csv', 'se2016-umb-jnk-51.csv', 'se2016-umb-jnk-01-51.csv', 'se2016-umb-jnk-02-51.csv', 'se2016-umb-jnk-03-51.csv', 'se2016-umb-jnk-52.csv', 'se2016-umb-jnk-01-52.csv', 'se2016-umb-jnk-02-52.csv', 'se2016-umb-jnk-03-52.csv', 'se2016-umb-jnk-53.csv', 'se2016-umb-jnk-01-53.csv', 'se2016-umb-jnk-02-53.csv', 'se2016-umb-jnk-03-53.csv', 'se2016-umb-jnk-61.csv', 'se2016-umb-jnk-01-61.csv', 'se2016-umb-jnk-02-61.csv', 'se2016-umb-jnk-03-61.csv', \n",
        "  'se2016-umb-jnk-62.csv', 'se2016-umb-jnk-01-62.csv', 'se2016-umb-jnk-02-62.csv', 'se2016-umb-jnk-03-62.csv', 'se2016-umb-jnk-63.csv', 'se2016-umb-jnk-01-63.csv', 'se2016-umb-jnk-02-63.csv', 'se2016-umb-jnk-03-63.csv', 'se2016-umb-jnk-64.csv', 'se2016-umb-jnk-01-64.csv', 'se2016-umb-jnk-02-64.csv', 'se2016-umb-jnk-03-64.csv', 'se2016-umb-jnk-65.csv', 'se2016-umb-jnk-01-65.csv', 'se2016-umb-jnk-02-65.csv', 'se2016-umb-jnk-03-65.csv', 'se2016-umb-jnk-71.csv', 'se2016-umb-jnk-01-71.csv', 'se2016-umb-jnk-02-71.csv', 'se2016-umb-jnk-03-71.csv', \n",
        "  'se2016-umb-jnk-72.csv', 'se2016-umb-jnk-01-72.csv', 'se2016-umb-jnk-02-72.csv', 'se2016-umb-jnk-03-72.csv', 'se2016-umb-jnk-73.csv', 'se2016-umb-jnk-01-73.csv', 'se2016-umb-jnk-02-73.csv', 'se2016-umb-jnk-03-73.csv', 'se2016-umb-jnk-74.csv', 'se2016-umb-jnk-01-74.csv', 'se2016-umb-jnk-02-74.csv', 'se2016-umb-jnk-03-74.csv', 'se2016-umb-jnk-75.csv', 'se2016-umb-jnk-01-75.csv', 'se2016-umb-jnk-02-75.csv', 'se2016-umb-jnk-03-75.csv', 'se2016-umb-jnk-76.csv', 'se2016-umb-jnk-01-76.csv', 'se2016-umb-jnk-02-76.csv', 'se2016-umb-jnk-03-76.csv', \n",
        "  'se2016-umb-jnk-81.csv', 'se2016-umb-jnk-01-81.csv', 'se2016-umb-jnk-02-81.csv', 'se2016-umb-jnk-03-81.csv', 'se2016-umb-jnk-82.csv', 'se2016-umb-jnk-01-82.csv', 'se2016-umb-jnk-02-82.csv', 'se2016-umb-jnk-03-82.csv', 'se2016-umb-jnk-91.csv', 'se2016-umb-jnk-01-91.csv', 'se2016-umb-jnk-02-91.csv', 'se2016-umb-jnk-03-91.csv', 'se2016-umb-jnk-94.csv', 'se2016-umb-jnk-01-94.csv', 'se2016-umb-jnk-02-94.csv', 'se2016-umb-jnk-03-94.csv'\n",
        "]\n",
        "\n",
        "# Merge table se2016-umb-sp\n",
        "listFile = [\n",
        "  'se2016-umb-sp-11.csv', 'se2016-umb-sp-01-11.csv', 'se2016-umb-sp-02-11.csv', 'se2016-umb-sp-03-11.csv', 'se2016-umb-sp-12.csv', 'se2016-umb-sp-01-12.csv', 'se2016-umb-sp-02-12.csv', 'se2016-umb-sp-03-12.csv', 'se2016-umb-sp-13.csv', 'se2016-umb-sp-01-13.csv', 'se2016-umb-sp-02-13.csv', 'se2016-umb-sp-03-13.csv', 'se2016-umb-sp-14.csv', 'se2016-umb-sp-01-14.csv', 'se2016-umb-sp-02-14.csv', 'se2016-umb-sp-03-14.csv', 'se2016-umb-sp-15.csv', 'se2016-umb-sp-01-15.csv', 'se2016-umb-sp-02-15.csv', 'se2016-umb-sp-03-15.csv', \n",
        "  'se2016-umb-sp-16.csv', 'se2016-umb-sp-01-16.csv', 'se2016-umb-sp-02-16.csv', 'se2016-umb-sp-03-16.csv', 'se2016-umb-sp-17.csv', 'se2016-umb-sp-01-17.csv', 'se2016-umb-sp-02-17.csv', 'se2016-umb-sp-03-17.csv', 'se2016-umb-sp-18.csv', 'se2016-umb-sp-01-18.csv', 'se2016-umb-sp-02-18.csv', 'se2016-umb-sp-03-18.csv', 'se2016-umb-sp-19.csv', 'se2016-umb-sp-01-19.csv', 'se2016-umb-sp-02-19.csv', 'se2016-umb-sp-03-19.csv', 'se2016-umb-sp-21.csv', 'se2016-umb-sp-01-21.csv', 'se2016-umb-sp-02-21.csv', 'se2016-umb-sp-03-21.csv', \n",
        "  'se2016-umb-sp-31.csv', 'se2016-umb-sp-01-31.csv', 'se2016-umb-sp-02-31.csv', 'se2016-umb-sp-03-31.csv', 'se2016-umb-sp-32.csv', 'se2016-umb-sp-01-32.csv', 'se2016-umb-sp-02-32.csv', 'se2016-umb-sp-03-32.csv', 'se2016-umb-sp-33.csv', 'se2016-umb-sp-01-33.csv', 'se2016-umb-sp-02-33.csv', 'se2016-umb-sp-03-33.csv', 'se2016-umb-sp-34.csv', 'se2016-umb-sp-01-34.csv', 'se2016-umb-sp-02-34.csv', 'se2016-umb-sp-03-34.csv', 'se2016-umb-sp-35.csv', 'se2016-umb-sp-01-35.csv', 'se2016-umb-sp-02-35.csv', 'se2016-umb-sp-03-35.csv', \n",
        "  'se2016-umb-sp-36.csv', 'se2016-umb-sp-01-36.csv', 'se2016-umb-sp-02-36.csv', 'se2016-umb-sp-03-36.csv', 'se2016-umb-sp-51.csv', 'se2016-umb-sp-01-51.csv', 'se2016-umb-sp-02-51.csv', 'se2016-umb-sp-03-51.csv', 'se2016-umb-sp-52.csv', 'se2016-umb-sp-01-52.csv', 'se2016-umb-sp-02-52.csv', 'se2016-umb-sp-03-52.csv', 'se2016-umb-sp-53.csv', 'se2016-umb-sp-01-53.csv', 'se2016-umb-sp-02-53.csv', 'se2016-umb-sp-03-53.csv', 'se2016-umb-sp-61.csv', 'se2016-umb-sp-01-61.csv', 'se2016-umb-sp-02-61.csv', 'se2016-umb-sp-03-61.csv', \n",
        "  'se2016-umb-sp-62.csv', 'se2016-umb-sp-01-62.csv', 'se2016-umb-sp-02-62.csv', 'se2016-umb-sp-03-62.csv', 'se2016-umb-sp-63.csv', 'se2016-umb-sp-01-63.csv', 'se2016-umb-sp-02-63.csv', 'se2016-umb-sp-03-63.csv', 'se2016-umb-sp-64.csv', 'se2016-umb-sp-01-64.csv', 'se2016-umb-sp-02-64.csv', 'se2016-umb-sp-03-64.csv', 'se2016-umb-sp-65.csv', 'se2016-umb-sp-01-65.csv', 'se2016-umb-sp-02-65.csv', 'se2016-umb-sp-03-65.csv', 'se2016-umb-sp-71.csv', 'se2016-umb-sp-01-71.csv', 'se2016-umb-sp-02-71.csv', 'se2016-umb-sp-03-71.csv', \n",
        "  'se2016-umb-sp-72.csv', 'se2016-umb-sp-01-72.csv', 'se2016-umb-sp-02-72.csv', 'se2016-umb-sp-03-72.csv', 'se2016-umb-sp-73.csv', 'se2016-umb-sp-01-73.csv', 'se2016-umb-sp-02-73.csv', 'se2016-umb-sp-03-73.csv', 'se2016-umb-sp-74.csv', 'se2016-umb-sp-01-74.csv', 'se2016-umb-sp-02-74.csv', 'se2016-umb-sp-03-74.csv', 'se2016-umb-sp-75.csv', 'se2016-umb-sp-01-75.csv', 'se2016-umb-sp-02-75.csv', 'se2016-umb-sp-03-75.csv', 'se2016-umb-sp-76.csv', 'se2016-umb-sp-01-76.csv', 'se2016-umb-sp-02-76.csv', 'se2016-umb-sp-03-76.csv', \n",
        "  'se2016-umb-sp-81.csv', 'se2016-umb-sp-01-81.csv', 'se2016-umb-sp-02-81.csv', 'se2016-umb-sp-03-81.csv', 'se2016-umb-sp-82.csv', 'se2016-umb-sp-01-82.csv', 'se2016-umb-sp-02-82.csv', 'se2016-umb-sp-03-82.csv', 'se2016-umb-sp-91.csv', 'se2016-umb-sp-01-91.csv', 'se2016-umb-sp-02-91.csv', 'se2016-umb-sp-03-91.csv', 'se2016-umb-sp-94.csv', 'se2016-umb-sp-01-94.csv', 'se2016-umb-sp-02-94.csv', 'se2016-umb-sp-03-94.csv'\n",
        "]\n",
        "\n",
        "# Set working directory 01\n",
        "workingDirectory = dictDirectory['se-2016-umb-sp-data']\n",
        "os.chdir(workingDirectory)\n",
        "path = !pwd\n",
        "# print(path)\n",
        "\n",
        "loop = 0\n",
        "for i in range (0, 136, 4):\n",
        "  # print(listFile[i], \"merge with\", listFile[i+1], \"into\", listFile[i+2])\n",
        "  fname = listFile[i]     # Merge data files\n",
        "  fnameA = listFile[i+1]\n",
        "  fnameB  = listFile[i+2]\n",
        "  fnameC  = listFile[i+3]\n",
        "  pathSourceA = path[0] + '/data-01/' + fnameA\n",
        "  pathSourceB = path[0] + '/data-02/' + fnameB\n",
        "  pathSourceC = path[0] + '/data-03/' + fnameC\n",
        "  pathDestination = path[0] + '/' + fname\n",
        "  \n",
        "  # print(pathSourceA, pathSourceB, pathSourceC, pathDestination)\n",
        "  dfBPSDataA = pd.read_csv(pathSourceA)\n",
        "  dfBPSDataB = pd.read_csv(pathSourceB)\n",
        "  dfBPSDataC = pd.read_csv(pathSourceC)\n",
        "\n",
        "  # Rename joining keys\n",
        "  dfBPSDataA.rename({'IDPERUSAHA': 'PERUSAHAAN_ID', 'PROV': 'PROVINSI_IDA', 'SKALAUSAHA': 'SKALAUSAHA_A', 'WEIGHT': 'WEIGHT_A'}, axis='columns', inplace=True)\n",
        "  dfBPSDataB.rename({'IDPERUSAHA': 'PERUSAHAAN_ID', 'JENISKUESI': 'JENISKUESIONER_B', 'PROV': 'PROVINSI_IDB', 'SKALAUSAHA': 'SKALAUSAHA_B', 'WEIGHT': 'WEIGHT_B'}, axis='columns', inplace=True)\n",
        "  dfBPSDataC.rename({'IDPERUSAHA': 'PERUSAHAAN_ID', 'JENISKUESI': 'JENISKUESIONER_C', 'PROV': 'PROVINSI_IDC', 'SKALAUSAHA': 'SKALAUSAHA_C', 'WEIGHT': 'WEIGHT_C'}, axis='columns', inplace=True)\n",
        "  \n",
        "  dfBPSData = [dfBPSDataA, dfBPSDataB, dfBPSDataC]\n",
        "\n",
        "  # Merge data files\n",
        "  # dfBPSData = dfBPSDataA.merge(dfBPSDataB, left_on='IDPERUSAHAAN_A', right_on='IDPERUSAHAAN_B')\n",
        "  dfBPSDataMerge = reduce(lambda left,right: pd.merge(left,right,on='PERUSAHAAN_ID'), dfBPSData)\n",
        "  \n",
        "  # print(pathDestination, \"dfBPSDataA.shape:\", dfBPSDataA.shape, \"dfBPSDataB.shape:\", dfBPSDataB.shape)\n",
        "  # print(pathDestination, \"dfBPSDataA.shape:\", dfBPSDataA.shape, \"dfBPSDataB.shape:\", dfBPSDataB.shape, \"dfBPSDataMerge.shape:\", dfBPSData.shape)\n",
        "  # print(\"dfBPSDataA.shape:\", dfBPSDataA.shape[0], \"dfBPSDataB.shape:\", dfBPSDataB.shape[0], \"dfBPSDataMerge.shape:\", dfBPSData.shape[0])\n",
        "\n",
        "  # Save data from merge data type operation\n",
        "  print(pathDestination, dfBPSDataA.shape, dfBPSDataMerge.shape)\n",
        "  # print(fname, dfBPSDataA.shape, dfBPSDataMerge.shape)\n",
        "  dfBPSDataMerge.to_csv(pathDestination, encoding='utf-8', index=False)\n",
        "\n",
        "  loop += 1\n",
        "\n",
        "print(loop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZnUlRjp_hK-"
      },
      "source": [
        "%%time\n",
        "# Merge table se2016-umk\n",
        "# Set working directory 01\n",
        "workingDirectory = dictDirectory['se-2016-umb-sp-data']\n",
        "os.chdir(workingDirectory)\n",
        "path = !pwd\n",
        "# print(path)\n",
        "\n",
        "# List file on working directory 02\n",
        "listFile = [f for f in glob.glob('*.csv')]\n",
        "\n",
        "pathDestination = path[0] + '/' + 'se2016-umb-sp-merge.csv'\n",
        "dfMerges = []\n",
        "totalRows = 0\n",
        "loop = 0\n",
        "for e in listFile:\n",
        "  pathSource = path[0] + '/' + e\n",
        "  # print('merge ' + pathSource)\n",
        "\n",
        "  # Read and import csv file dataset into pandas data frame, change paths if needed\n",
        "  dfBPSData = pd.read_csv(pathSource)\n",
        "\n",
        "  dfMerges.append(dfBPSData)\n",
        "  print(\"dfBPSData.shape :\", e, dfBPSData.shape)\n",
        "  totalRows += dfBPSData.shape[0]\n",
        "  loop += 1\n",
        "\n",
        "print(loop)\n",
        "\n",
        "\n",
        "dfBPSDataMerge = pd.concat(dfMerges)\n",
        "print(\"dfBPSDataMerge.shape :\", dfBPSDataMerge.shape, totalRows)\n",
        "\n",
        "print(pathDestination)\n",
        "dfBPSDataMerge.to_csv(pathDestination, encoding='utf-8', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fjcS1i_x2zm"
      },
      "source": [
        "# Examine dataset, see data type\n",
        "dfBPSDataMerge.info(verbose=True, null_counts=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TDt3_F-1TEC"
      },
      "source": [
        "pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foPWry8Evs0o"
      },
      "source": [
        "%%time\n",
        "# Read and import csv file dataset into pandas data frame, change paths if needed\n",
        "\n",
        "# Set working directory 01\n",
        "os.chdir(dictDirectory['se-2016-umb-jk'])\n",
        "path = !pwd\n",
        "print(path)\n",
        "\n",
        "# List file on working directory 02\n",
        "listFile = [f for f in glob.glob('*.csv')]\n",
        "\n",
        "pathDestination = path[0] + '/' + 'se-2016-umb-jk-merge.csv'\n",
        "dfMerges = []\n",
        "totalRows = 0\n",
        "loop = 0\n",
        "for e in listFile:\n",
        "  pathSource = path[0] + '/' + e\n",
        "  # print('merge ' + pathSource)\n",
        "\n",
        "  # Read and import csv file dataset into pandas data frame, change paths if needed\n",
        "  dfBPSData = pd.read_csv(pathSource)\n",
        "\n",
        "  dfMerges.append(dfBPSData)\n",
        "  print(\"dfBPSData.shape :\", dfBPSData.shape)\n",
        "  totalRows += dfBPSData.shape[0]\n",
        "  # print(\"type(dfBPSData) :\", type(dfBPSData))\n",
        "  loop += 1\n",
        "\n",
        "print(loop)\n",
        "\n",
        "\n",
        "dfBPSDataMerge = pd.concat(dfMerges)\n",
        "print(\"dfBPSDataMerge.shape :\", dfBPSDataMerge.shape, totalRows)\n",
        "\n",
        "print(pathDestination)\n",
        "dfBPSDataMerge.to_csv(pathDestination, encoding='utf-8', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aI4eQISe648Z"
      },
      "source": [
        "## Step 0207 Create Data Description"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUewNpHp7Djt"
      },
      "source": [
        "# Sample json file for rawdata IBS 1993\n",
        "[\n",
        "\t{\n",
        "\t\t\"name\": \"DSTATS93\",\n",
        "\t\t\"type\": \"String\",\n",
        "\t\t\"description\": \"Status Permodalan\"\n",
        "\t},\n",
        "\t{\n",
        "\t\t\"name\": \"DETYPE93\",\n",
        "\t\t\"type\": \"String\",\n",
        "\t\t\"description\": \"Bentuk Badan Hukum\"\n",
        "\t},\n",
        "\t{\n",
        "\t\t\"name\": \"DPROVI93\",\n",
        "\t\t\"type\": \"String\",\n",
        "\t\t\"description\": \"Propinsi\"\n",
        "\t},\n",
        "\t{\n",
        "\t\t\"name\": \"DKABUP93\",\n",
        "\t\t\"type\": \"String\",\n",
        "\t\t\"description\": \"Kabupaten/Kotamadya\"\n",
        "\t},\n",
        "\t{\n",
        "\t\t\"name\": \"DSRVYR93\",\n",
        "\t\t\"type\": \"String\",\n",
        "\t\t\"description\": \"Tahun Survei\"\n",
        "\t},\n",
        "\t{\n",
        "\t\t\"name\": \"DYRSTR93\",\n",
        "\t\t\"type\": \"String\",\n",
        "\t\t\"description\": \"Tahun Mulai Produksi Komersial di Propinsi ini\"\n",
        "\t},\n",
        " \n",
        "...\n",
        "\n",
        "\t{\n",
        "\t\t\"name\": \"LPDNOU93\",\n",
        "\t\t\"type\": \"Integer\",\n",
        "\t\t\"description\": \"Jumlah Banyaknya Pekerja/Karyawan Pekerja (Produksi + Lainnya) (Laki-laki + Perempuan) dibayar rata-rata setiap bulan\"\n",
        "\t},\n",
        "\t{\n",
        "\t\t\"name\": \"LTLNOU93\",\n",
        "\t\t\"type\": \"Integer\",\n",
        "\t\t\"description\": \"Jumlah Banyaknya Pekerja/Karyawan Pekerja (Produksi + Lainnya) (dibayar + tidak dibayar) (Laki-laki + Perempuan) rata-rata setiap bulan\"\n",
        "\t},\n",
        "\n",
        " ...\n",
        "\n",
        "\t{\n",
        "\t\t\"name\": \"EWOVCE93\",\n",
        "\t\t\"type\": \"Integer\",\n",
        "\t\t\"description\": \"Nilai Kayu Bakar dipakai selama tahun 1993 (Pembangkit Listrik)\"\n",
        "\t},\n",
        "\t{\n",
        "\t\t\"name\": \"ENCVCE93\",\n",
        "\t\t\"type\": \"Integer\",\n",
        "\t\t\"description\": \"Nilai Bahan Bakar Lainnya dipakai selama tahun 1993 (Pembangkit Listrik)\"\n",
        "\t},\n",
        "\t{\n",
        "\t\t\"name\": \"ETLQUE93\",\n",
        "\t\t\"type\": \"Integer\",\n",
        "\t\t\"description\": \"Banyaknya Bahan Bakar Lainnya dipakai selama tahun 1993 (Pembangkit Listrik)\"\n",
        "\t},\n",
        "\t{\n",
        "\t\t\"name\": \"NST93\",\n",
        "\t\t\"type\": \"String\",\n",
        "\t\t\"description\": \"NST93 Variabel tidak digunakan\"\n",
        "\t},\n",
        "\t{\n",
        "\t\t\"name\": \"PSID\",\n",
        "\t\t\"type\": \"Integer\",\n",
        "\t\t\"description\": \"PSID Variabel\"\n",
        "\t}\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaN2SrfQsBwO"
      },
      "source": [
        "# Step 03 - Data Preparation\n",
        "In this step, we pre-process the data, clean it, wrangle it, and\n",
        "manipulate it as needed. Initial exploratory data analysis is also carried out.\n",
        "* **Data Processing & Wrangling**: \n",
        "  Mainly concerned with data processing, cleaning, munging, wrangling and performing initial descriptive and exploratory data analysis\n",
        "* **Feature Extraction & Engineering**: Here, we extract important features or attributes from the raw data and even create or engineer new features from existing features.\n",
        "* **Feature Scaling & Selection**: Data features often need to be normalized and scaled to prevent Machine Learning algorithms from getting biased. Besides this, often we need to select a subset of all available features based on feature importance and quality.\n",
        "\n",
        "Final Update 20200315"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gLlMZJdsBwP"
      },
      "source": [
        "## Step 0301 Dataset Summary Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qb8GwcDVsBwQ"
      },
      "source": [
        "# Examine dataset, shape, rows and columns\n",
        "print(\"dfTrain shape   :\", dfTrain.shape)\n",
        "print(\"type(dfTrain)   :\", type(dfTrain))\n",
        "print(\"dfTrain.index   :\", dfTrain.index)\n",
        "print(\"dfTrain.columns :\", dfTrain.columns, \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7YKu4bCsBwU"
      },
      "source": [
        "# Examine dataset, first 5 rows\n",
        "# dfTrain.head()\n",
        "dfTrain.head().T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snS2w77YsBwX"
      },
      "source": [
        "# Examine dataset, types of all features and total dataframe size in memory\n",
        "dfTrain.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxoJLmrksBwb"
      },
      "source": [
        "# Examine dataset, types of all features and total dataframe size in memory\n",
        "dfTrain.describe().T\n",
        "# dfTrain.describe(include='all').T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UkNenLVsBwf"
      },
      "source": [
        "dfTrain.columns.isna().any()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8PxB-22sBw9"
      },
      "source": [
        "\n",
        "# Step 04 - Deployment and Monitoring\n",
        "Datawarehouse are deployed in production and are constantly monitored based on their performance and transformation.\n",
        "\n",
        "Final Update 20200315"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-WBgjeTnOT8"
      },
      "source": [
        "%%time\n",
        "# Big Query delete table se2016-listing\n",
        "listBQFile = [\n",
        "  'se_2016_listing_11', 'se_2016_listing_12', 'se_2016_listing_13', 'se_2016_listing_14', 'se_2016_listing_15', \n",
        "  'se_2016_listing_16', 'se_2016_listing_17', 'se_2016_listing_18', 'se_2016_listing_19', 'se_2016_listing_21', \n",
        "  'se_2016_listing_31', 'se_2016_listing_32', 'se_2016_listing_33', 'se_2016_listing_34', 'se_2016_listing_35', \n",
        "  'se_2016_listing_36', 'se_2016_listing_51', 'se_2016_listing_52', 'se_2016_listing_53', 'se_2016_listing_61', \n",
        "  'se_2016_listing_62', 'se_2016_listing_63', 'se_2016_listing_64', 'se_2016_listing_65', 'se_2016_listing_71', \n",
        "  'se_2016_listing_72', 'se_2016_listing_73', 'se_2016_listing_74', 'se_2016_listing_75', 'se_2016_listing_76', \n",
        "  'se_2016_listing_81', 'se_2016_listing_82', 'se_2016_listing_91', 'se_2016_listing_94', 'se_2016_listing_merge' \n",
        "]\n",
        "\n",
        "# Set working directory on Google Big Query 01\n",
        "projectId = 'datawarehouse-001'\n",
        "directoryBQ = ['datawarehouse-001:04_sensus_ekonomi', 'datawarehouse-001:04_sensus_ekonomi_rawdata']\n",
        "\n",
        "# List file on Google Big Query working directory 02\n",
        "# !bq ls --max_results=1000 {directoryBQ[0]}\n",
        "\n",
        "# !bq rm --help\n",
        "\n",
        "loop = 0\n",
        "for e in listBQFile:\n",
        "  bqFileName = directoryBQ[0] + \".\" + e\n",
        "  # !bq rm -f -t {bqFileName}\n",
        "  print(bqFileName)\n",
        "  loop += 1\n",
        "\n",
        "print(loop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oa_1isjNsBw-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}